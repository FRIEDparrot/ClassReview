#### 一、输出是连续的-> 回归问题
	输出由

#### 二、输出是离散的 -> 分类问题



对于一般的线性模型: 
1) 线性回归的一般形式：

$$f(x) = \omega_1 x_1 + \omega_2 x_2 + .....+ \omega_n x_n$$
$$f(x) = \omega^Tx+b$$
1) 损失函数:

	损失函数主要包括均方差损失函数和交叉熵损失函数
	定义损失函数$L$ ，描述预测值和真实值的偏差
	此时， 当损失函数$L$取得最小值时，$\omega_1,\omega_2....$等取得最优解
	常见的损失函数有: 0-1损失函数，绝对值损失函数，平方损失函数，Log对数损失函数，指数损失函数，交叉熵损失函数，Hinge损失函数等等

2) 常用的损失函数 
	1) 均方误差损失函数（MSE，mean square error）
	$$L = \frac{1}{n} \sum^{n}_{i=1} (\hat{y}_i -y_i)^2$$
	2) 交叉熵损失函数

$$H(y^{(i)}, \hat{y}^{(i)}) = -\overset{q}{\underset{j = 1}{\sum}}y_j^{(i)}\log(y_j^{(i)})$$
其中, $\hat{y}_j^{(i)}$非0即1

#### 三、线型回归模型的构建

最小二乘法（generalized least squares）通过最小化误差的平方和，寻求最佳函数

对于线型回归$\hat{y} = \omega x+b$ ,采用均方误差损失函数（最小二乘法的思想）
（采用$y= \omega x+b$来模拟数据，则寻求一组$\omega$和$b$使得结果和预测值的偏差平方和最小）
对原始的模型构建损失函数之后，接下来是通过优化算法，使得损失函数$L$最小
$$(\omega^*, b^*) = arg\text{ min } (\sum^{n}_{i=1} \omega x_i + b - y_i)^2$$

参数的更新方法：对$\omega$和$b$求解梯度，每一次参数分别减去梯度 x 学习率(事先指定)

$$\large\begin{matrix}
\omega_1 = \omega_1 - \frac{\eta}{\beta}(\sum_{i} \frac{\partial L^{(i)}}{\partial \omega_1})
\end{matrix}$$
其中，$\beta$ 为批量的大小,$\eta$ 为学习率，需要事先指定

#### 四、逻辑回归模型
Sigmoid函数
$$Sigmoid(x) = h_\theta(x) = g(\theta^T x) = \frac{1}{1+e^{-\theta^T x}}$$
其中，sigmoid函数满足: 
$$y' = y(1-y)$$
则概率综合起来写成：
$$P(y|x; \theta) = (h(\theta))^y (1-h_\theta (x))^{1-y}$$
其中，$$\begin{cases}
P(y = 1|x, \theta) = h(\theta) \\
P(y = 0|x, \theta) = 1 - h(\theta)
\end{cases}$$
所以上面的y只能取1或者0
取似然函数:
$$L(\theta) = \underset{i = 1}{\overset{m}{\prod}}(h_\theta(x))^{y_i}(1-h_\theta(x_i))^{1-y_i}$$
对数似然函数
$$ l(\theta) = \log L(\theta) = \sum^{m}_{i=1}(y_i \log h_\theta( x_i) + (1-y_i)\log (1-h_\theta(x_i)))$$
取
$$J(\theta) = -\frac{1}{m} l(\theta)$$
为损失函数

在更新中，每次取其梯度$\frac{\partial l(\theta)}{\partial \theta}$即可:
在迭代中，取
$$\theta_j = \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta}$$

#### 五、正则化方法
引入正则化项的目的是防止模型过拟合，函数对样本的拟合有三种结果
 
欠拟合：直观的理解就是在训练集上的误差比较大，拟合出来的函数应该是曲线，结果拟合成了一条直线
 
过拟合：在训练集上的误差很小甚至为0，追求经验风险最小化，模型拟合的很复杂，往往在未知的样本集上表现的不够好

合适的拟合

-  L1规则化：

$$f(W) = f(W) + \lambda \overset{N}{\underset{i=1}{\sum}}|w_i|$$
![[Pasted image 20221121194008.png|500]]
- L2规则化：

$$f(W) = f(W)+ \lambda \overset{N}{\underset{i=1}{\sum}}w_i^2$$
![[Pasted image 20221121194026.png|500]]
