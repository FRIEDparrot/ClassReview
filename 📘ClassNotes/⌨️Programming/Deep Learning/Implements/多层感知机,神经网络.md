
```python
# 多层感知机 ---> 本部分由p51开始  
# Multilayer Perceptron/ MLP --> 多层感知机  
#  
# 目标是全部分对给定的点
# 3.8.1 隐藏层  
# 输入-> 隐藏层 -> 输出, 其中包含隐藏单元  
# 多层感知机的计数: 输入层不涉及计算 -> 给定一个2层的神经网络  
# 记隐藏层的输出为H, H = XW_h + b_h     O = HW_o + b_o  
  
# 经过联立，可得到:  
# O = (XW_h + b_h)W_o + b_o = XW_hW_o + b_hW_o + b_o  
# 即使再添加更多的隐藏层，以上设计仍然只能与仅含输出层的单层神经网络等价  
# 其中，输出层权重参数为W_hW_o,偏差参数为b_hW_o+b_o  
# 
# 其中，使用 y*(wx + b) 来进行分类错误的点的判断
# 如果有错分类的点， 则寻找相应的梯度，并进行输出

# 3.8.3 多层感知机  
# H = phi(XW_h + b_h) # 注意其中要乘以一个激活函数
# 

# O = HW_o + b_o  
# 其中，phi为激活函数，在分类问题中，可以对O做softmax运算,并使用softmax中的交叉熵损失函数  
# 将输出层的个数设为1

# 将输出O直接提供给线型回归中的平方损失函数, 使用softmax回归的交叉熵损失函数  
# 常用的激活函数是ReLU函数 ,sigmoid函数和 tanh函数
```

![[多层感知机 2022-11-21 21.00.28.excalidraw|300]]
```python
from mxnet import nd
# 定义激活函数为relu函数 --> 对矩阵实行激活函数运算(映射到0-> +inf)  
def relu(X):  
    return nd.maximum(X,0)

# 2. sigmoid函数
# 定义: sigmoid(x) = 1/(1 + exp(-x))

# 3.tanh函数
# tanh = (1-exp(-2x))/(1+exp(-2x))
```

```python
# 对多层感知机的实现方法  
from mxnet import nd, autograd  
import d2lzh  
from mxnet.gluon import loss as gloss  
  
# 读取数据集  
batch_size = 256  
train_iter, test_iter = d2lzh.load_data_fashion_mnist(batch_size)  # 读取数据集  
  
num_inputs, num_outputs, num_hiddens = 784, 10,256 # 使用长度为784的向量代表每一张图像  
# 输入的个数为784，输出个数为10，隐藏单元个数(超参数)为256  
W1 = nd.random.normal(scale=0.01,shape=(num_inputs,num_hiddens))  
b1 = nd.zeros(num_hiddens)  
# 由于这个是中间有一个隐藏层，所以输出是输出到隐藏层中  
W2 = nd.random.normal(scale=0.01,shape=(num_hiddens,num_outputs))  
b2 = nd.zeros(num_outputs)  
# 随机生成正态分布的w,b的初始值，并使用双线性来进行运算  
  
params = [W1,b1,W2,b2]  
for param in params:   # 必须事先在这里求导  
    param.attach_grad()  
  
# 定义激活函数为relu函数 --> 对矩阵实行激活函数运算(映射到0-> +inf)  
def relu(X):  
    return nd.maximum(X,0)  
  
# 定义整个神经网络，这是有一个隐藏层的神经网络  
def net(X):  # 注意，不用加位置参数  
    X = X.reshape((-1, num_inputs))  # [1,784]的向量  
    H = relu(nd.dot(X,W1) + b1)    # 使用多层线性模型作为回归网络  
    # 注意: 此处H为Hiddens层，即隐藏层，所以需要指定相应的尺寸大小  
    return nd.dot(H,W2) + b2  
  
loss = gloss.SoftmaxCrossEntropyLoss()    # 定义损失函数为softmax归一化运算的交叉熵损失函数  
num_epochs ,learning_rate = 5, 0.5  
  
d2lzh.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, learning_rate)
```

```python
from mxnet.gluon import loss as gloss  
from mxnet.gluon import nn  
from mxnet import gluon  
from mxnet import init   # 调用函数，自动初始化相关参数  
import d2lzh  
  
net = nn.Sequential()   # 建立神经网络作为训练模型  
net.add(nn.Dense(256, activation='relu'), nn.Dense(10))  
# 添加256，激活函数为relu函数的隐藏层和输出数量为10的输出层  
net.initialize(init.Normal(sigma = 0.01))  # 使用正态分布，并指定标准差为0.01  
  
batch_size = 256  
train_iter, test_iter = d2lzh.load_data_fashion_mnist(batch_size)  
loss = gloss.SoftmaxCrossEntropyLoss()  # 定义gloss中的交叉熵损失函数  
trainer = gluon.Trainer(net.collect_params(),'sgd',{"learning_rate": 0.05})  
num_epochs = 5  
# sgd是使用小批量梯度下降算法  
d2lzh.train_ch3(net,train_iter,test_iter, loss, num_epochs, batch_size, None, None, trainer)
```

### 神经网络的反向传播算法

- 不同的模型具有不同的模型参数，神经网络的连接方式，网络层数等等，人为设置的参数称之为超参数
- 反向传播算法实质是使用链式求导法则
- 先确定神经网络的目标函数，再使用随机梯度下降算法求解目标参数最小值是的参数值
- 使用所有输出层的节点的误差的平方和为目标函数

$$E_d \equiv \frac{1}{2} \sum_{outputs} (t_i - y_i)^2$$
$$net_j = \vec{w}_j \vec{x_j} $$
![[Pasted image 20221124200223.png|380]]
$y_j$对net求导是sigmoid函数的求导
![[Pasted image 20221124200512.png]]
$$\frac{\partial E_d }{\partial net_j} = -(t_j -y_j)y_j(1-y_j)$$
![[Pasted image 20221124200738.png]]

