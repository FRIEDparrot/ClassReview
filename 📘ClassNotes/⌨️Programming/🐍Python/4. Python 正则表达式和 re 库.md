```python
re.sub
re.match
re.findall 
```

参考文章 [Python正则表达式详解](https://blog.csdn.net/qq_44159028/article/details/120575621)  和 [正则表达式案例](https://blog.csdn.net/Java_ZZZZZ/article/details/130862224), 一般所有的作为参数都需要加上 r 表示 raw 传入原始字符串, 否则会匹配错误。

![[attachments/Pasted image 20240910170843.png|700]]
`\-` 匹配 - 本身

将一个带有特殊符号的句子转换为词汇: ^ 表示
```python
cleaned_sentence = re.sub(r'[^\w\s]', '', sentence)
text_content = re.sub(r'[^\w\s-]', '', text_content)   # eliminate the special characters 
```
原理是匹配除了字母，数字和下划线和空白符以外的所有字符, 将其替换为 '', 第二个示例考虑了带有 - 的词汇

[] : 只允许出现[ ]中列举的字符, ^ 一般用于匹配开头字符，但是在 `[]` 中表示不匹配任何一个字符。
```python
import re
print(re.match('12[^234]','232s12testasdtest'))  #因为开头的12没匹配上，所以直接返回none
print(re.match('12[^234]','1232s12testasdtest')) #返回none
print(re.match('12[^234]','1252s12testasdtest')) #返回125
```

() 用于定义捕获组, 用于统计多次进行捕获，例如:
```python
stri = "StrInt"
res = re.match(r'([A-Z]+)([a-z]+)([A-Z]+[a-z]*)', stri)
```
以使用非捕获组 `(?:...)` 和捕获组 `(...)` 的组合来实现任意一个组的捕获。另外, re.findall采用捕获组时, 会返回多个元组, 分别表示每个组中的内容。

`*` 出现0次或无数次 
`+` 出现1次或者多次


```python
words = re.split(r'\w+', sentence)
words_in_sentence = re.split(r'\s+',sentence)
```

大写字母更改为小写: 
```python
sentence = "This Is A Test Sentence."
lowercase_sentence = sentence.lower()

print(lowercase_sentence)
```

`````ad-note
title: 检查一个单词是否为完整单词
collapse: open
完整单词有 AZURE, basis2, Theory3 等多种形式，具体可以如下匹配: (注意, 也会匹配纯数字), 其中 {2,}是至少出现2次
另外, 纯数字和开头为数字的单词,均不需要匹配 
因此, 对于 soilder2Adam，会返回两个单词, 即返回 Flase
```python
def valid_word(word:str):  
    if len(word) < 2 or len(word) > 20:  
        return False    
    elif '_' in word or '-' in word:  
        return False  
    elif word.startswith('[0-9]'):   # filter the pure number and the 131a  
        return False  
    word_pattern = r'[A-Z0-9]{2,}|[a-z0-9]+|[A-Z][a-z0-9]+'  
    match_res = re.findall(word_pattern, word)  
    if match_res is [] or len(match_res) >= 2:  
        # print("unmatched item : ", word)  
        return False  
    return True
```
`````

`````ad-note
title:提取html文件内容并进行分词，计算每个词的词频
collapse: close
```python
import re  
import os  
import sklearn  
from sklearn import feature_extraction  
from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import TfidfVectorizer  
from lxml import html, etree  
from bs4 import BeautifulSoup  
  
# from sklearn.datasets import load_digits  
  
def valid_word(word:str):  
    if len(word) < 2 or len(word) > 20:  
        return False    elif '_' in word or '-' in word:  
        return False    elif re.match(r'^[0-9]', word): #  word.startswith('[0-9]'):   # filter the pure number and the 131a  
        return False  
    word_pattern = r'[A-Z0-9]{2,}|[a-z0-9]+|[A-Z][a-z0-9]+'  
    match_res = re.findall(word_pattern, word)  
    if match_res is [] or len(match_res) >= 2:  
        # print("unmatched item : ", word)  
        return False  
    return True  
def get_words_in_html(html_content):  
    words_list = []  
    html_content = BeautifulSoup(contents, parser="html5lib", features='lxml').get_text()  
    html_content = re.sub(r'[^a-zA-Z0-9\s\-_]', '', html_content)  # eliminate the special characters  
    sentences = list(filter(None, html_content.split('\n')))  
    for sentence in sentences:  
        words_in_sentence = re.split(r'\s+',sentence)  
        for word_item in words_in_sentence:  
            if not valid_word(word_item):   # eliminate the words that contain '_' or '-'  
                pass  
            else:                word = word_item.lower()  # get the lower case of the word  
                words_list.append(word)  
    return words_list  
  
if __name__ ==  "__main__" :  
    words_dict = {}  
    files = os.listdir("./html")  
    for file in files:  
        with open("./html/" + file, "rb") as f:  
            contents = f.read()  
            word_list = get_words_in_html(contents)  
            for word in word_list:  
                if word not in words_dict:  
                    words_dict[word] = 1  
                else:  
                    words_dict[word] += 1  
  
    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)  
    transformer = TfidfTransformer()  
  
    print(words_dict)
```

`````

