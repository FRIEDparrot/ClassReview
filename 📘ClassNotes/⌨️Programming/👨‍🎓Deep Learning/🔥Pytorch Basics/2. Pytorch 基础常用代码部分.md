对于 Pytorch, 有两个文档， 其中传统的是 pytorch,  另外还有[Quantized Documentation](https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.modules) 对各个维度做了注解， 常会使用。 

## 一、基本数据操作
### (1) 常用变形函数 
对应于 tensor 支持的函数，具体参考 https://pytorch.org/docs/stable/tensors.html 
#### 1. unsqueeze,squeeze和reshape使用
增加维度或者减少维度: 
`torch.squeeze(input,dim=0)` **移除某个维度**
`torch.unsqueeze(input,dim=0)` **在某个维度上增加维度**  
reshape 的使用: 在 pytorch 等中, 我们常常使用 reshape 将数据集转换为指定的形状, 例如对于 (60000, 28,28) 的数据集, 我们会使用 `x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)` 得到 (60000,28,28,1) 的4d数据集, 即将最后的28个中, 每个均作为输入拆分开;

```python title:squeeze使用示例
train[1].unsqueeze(0).shape
Out[12]: torch.Size([1, 28, 28])
A = train[1].unsqueeze(0).unsqueeze(0)
A.shape
Out[13]: torch.Size([1, 1, 28, 28]) 
A.squeeze(0).shape
Out[17]: torch.Size([1, 28, 28])
A.squeeze(0).squeeze(0).shape
Out[19]: torch.Size([28, 28])
A.squeeze([0,1]).shape
Out[22]: torch.Size([28, 28])
B.unsqueeze(dim=2).shape
Out[28]: torch.Size([28, 28, 1])
```

#### 2. View Copy 和 Clone
1. view 返回本身的对象， 只是 "观察维度" 不同， 得到尺寸不同。相当于 reshape 但保留原来对象 
```python fold title:
a.view()
```

2. **`tensor.clone()`** 为深拷贝  
- **保留计算图**（保留梯度传播路径）： 
    - 如果对 `clone()` 的结果进行操作，梯度会反向传播到原始张量。
    - 适用于需要保留自动微分（Autograd）的场景。  

2. **`tensor.copy_(src)`**
原地操作：将 src 的数据复制到目标张量中（覆盖目标张量的值），不创建新张量。
不保留计算图 : `copy_()` 是一个就地操作（in-place），会中断梯度传播。
适用于不需要梯度更新的场景（如模型权重初始化）。
需要目标张量和源张量形状一致。
```python fold title:
a = torch.tensor([1., 2.])
b = torch.tensor([3., 4.], requires_grad=True)
a.copy_(b)  # 将 b 的值复制到 a，a 变为 tensor([3., 4.])

print(a)  # 输出: tensor([3., 4.])
b.sum().backward()  # a 的修改不会影响 b 的梯度
```

#### 3.  narrow, expand, contiguous  
There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is organized. These operations include:

> `narrow()`, `view()`, `expand()` and `transpose()` 

1. `narrow(dim, start, length)`  : Returns a narrowed version of the tensor along a specified dimension.
for  example : 
```python fold title:narrow-usage
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
y = x.narrow(1, 1, 2)  # Takes columns 1 to 2 (0-based)
# y = [[2, 3], [5, 6]]
```

2. `expand(*sizes)`: Expands a tensor to a larger size by repeating elements (<b><mark style="background: transparent; color: orange">only works for size-1 dimensions</mark></b>). 
```python fold title:expand-usage 
x = torch.tensor([[1], [2]])  # shape (2, 1)
y = x.expand(2, 3)  # [[1, 1, 1], [2, 2, 2]]
``` 

3. `contiguous` :  **Ensures the tensor is stored in contiguous memory (row-major order)**.    
```python
x = torch.randn(3,2)
y = torch.transpose(x, 0, 1)
```
in that case x is `contiguous` but y not.  

> [!warning] Why need contiguous memory
> Some operations (e.g., `view()`) <b><mark style="background: transparent; color: orange">require contiguous memory</mark></b>. Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get a `RuntimeError: input is not contiguous` where PyTorch expects a contiguous tensor to add a call to `contiguous()`.

#### 4. repeat_interleave 
基本函数`torch.repeat_interleave (_input_, _repeats_, _dim=None_, _*_, _output_size=None_)` 
对于 `repeat_interleave` 部分,  

注意 : By default, if dim not specified, use the flattened input array, and return a flat output array. 

```python fold title:examples 
y = torch.tensor([[1, 2], [3, 4]])
torch.repeat_interleave(y, 3, dim=1)
tensor([[1, 1, 1, 2, 2, 2],
        [3, 3, 3, 4, 4, 4]]) 
torch.repeat_interleave(y, 2)

torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)
tensor([[1, 2],
        [3, 4],  # 第二个 repeat 数为 2 
        [3, 4]])
```

#### 5.  transpose,  permute ,  reshape 
对于 transpose,  `torch.transpose(_input_, _dim0_, _dim1_)` 非常容易， 不多讲

permute 用法如下 : 
```python fold title:
torch.permute(input, dims) → Tensor
``` 

其中有 : 
- input (Tensor) – the input tensor. 
- dims (tuple of int) – The desired ordering of dimensions  

实际上 dims 是 `(0,1,2,3)` 顺序的随便排列组合, 例如 : 
```python fold title:example-of-permute 
>>> x = torch.randn(2, 3, 5)
>>> x.size()
torch.Size([2, 3, 5])
>>> torch.permute(x, (2, 0, 1)).size()
torch.Size([5, 2, 3]) 
```

对于 torch.reshape ， 接受一个元组， 转换形状  
- data 
- **shape** ([_tuple_](https://docs.python.org/3/library/stdtypes.html#tuple "(in Python v3.13)") _of_ [_int_](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) – the new shape 

### (2) 数据类型转换
#### 1. 转换值  
取 tensor 值方法 : 采用  item() 获取 Tensor 的值: `label_item = torch.Tensor(label).item()`  

对于带有自动求导部分， 采用 `x.detach.numpy()` 
```python
# detach 取消求导连接,  而 numpy 转换为 numpy 数组
y.detach.numpy(); 
```

#### 2. GPU 设备数据传输 
对于所有的 Tensor 和模型 (nn.module) 都提供了`to` 方法,   一般采用 `.cuda()` , `.cpu` , `.to(self.device)`  进行 gpu 数据移动。 

对于多 GPU 训练的模型， 可能涉及数据不同 GPU 之间的传递 
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  
print(torch.cuda.device_count())

num = 1
data.to(torch.device(f'cuda:{num}'))   # 将数据放在某个gpu 上
model.to(torch.device('cuda'))
```

另外，pytorch中大多数的操作都支持 `inplace` 操作，也就是可以直接对 tensor 进行操作而不需要另外开辟内存空间，方式非常简单，一般都是在操作的符号后面加`_`  

### (3) 常用内置函数 
`torch.topk`  也称为 `tensor.topk`  
```python fold title:topk-usage
x = torch.arange(1., 6.)
val, idx = torch.topk(x,3) 
val, idx 
Out[6]: (tensor([5., 4., 3.]), tensor([4, 3, 2]))
```

## 二、nn.utils 包常用组件 

### (2) RNN 包
-  `torch.nn.utils.rnn.pack_padded_sequence ` 和  [`pad_packed_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence "torch.nn.utils.rnn.pad_packed_sequence")  

note [`pad_packed_sequence()`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence "torch.nn.utils.rnn.pad_packed_sequence") can be used to recover the underlying tensor packed in 

不使用 `pack_padded_sequence` 
```python fold title:
padded_input = torch.tensor([[1, 2, 0], [3, 0, 0], [4, 5, 6]])  # 长度 [2, 1, 3]
output, hidden = rnn(padded_input)  # RNN 会计算所有位置（包括 0） 

# 下面是使用 pack_padded_sequence  优化效率之后的计算方法 :   
lengths = torch.tensor([2, 1, 3])
packed = pack_padded_sequence(padded_input, lengths, batch_first=True)
output, hidden = rnn(packed)  # RNN 仅计算有效部分
```

需要是说明的是， 当 `pack_padded_sequence` 的 `enforce_sorted=False`（默认 `True`），PyTorch 会 **自动按序列长度降序排序**，而， 这样做的主要目的是

**(1) 提高计算效率**
- RNN 在处理变长序列时，是按时间步（`time step`）依次计算的。
- **如果批次内序列长度降序排列**，短序列会提前终止计算，避免无效的 padding 计算。
  - **示例**：
    - 假设 `batch_size=3`，序列长度分别为 `[5, 3, 1]`。
    - 在 `time_step=1`，所有 3 个序列都计算。
    - 在 `time_step=4`，只有第 1 个序列（长度 5）仍在计算，其余 2 个序列已终止。
    - **减少不必要的计算**，提升 GPU 利用率。

**(2) 避免 `CUDA` 计算时的同步问题**
- GPU 并行计算时，如果某些序列已经结束（如 `[5, 3, 1]` 的 `time_step=2` 只有前 2 个序列需要计算），PyTorch 可以提前释放已完成序列的计算资源。
- 如果序列未排序（如 `[3, 5, 1]`），GPU 可能被迫在所有时间步上同步计算，降低效率。 

**(3) 更高效的内存访问**
- 降序排列后，RNN 的内存访问模式更连续（`memory-coalesced`），减少 GPU 访存延迟。  


#### 2. 常用函数集和运算 
**torch.nn.functional** 包含了**绝大多数的常用函数**: 例如 relu， softmax, sigmoid 等等。

```python
def SoftmaxRegression(nn.Module):
	self.w = torch.normal(0,simga, size =(num_inputs, num_outpus), requires_grad=True)
	self.b = torch.zeros(num_outputs, requires_grad = True)
	self.optim = torch.optim.SGD(self.parameters(), lr = 0.01)
	
	 def parameters(self) : 
		return [self.w, self.b]
	 def forward(self):
		 return softmax(torch.matmul(X.reshape((-1,  self.W.shape[0])),  self.W) + self.b)
```

对于 minist 数据集, 由于x的维度为 784, num_inputs = 784, num_outputs = 10. 即 w 为一个 784 x 10 的权重矩阵。

torch.gather(): 取某个tensor中指定下标的元素, 但是需要使用如下方法指定:
```Python
For a 3-D tensor the output is specified by: 
out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
```
#### 3. 取某些行或者某些列的元素(torch.gather)
例如gather(dim = 0) 是按行取元素, 
```python
a = tensor([[ 1,  2,  3,  4,  5], [ 6,  7,  8,  9, 10]])
a.gather(0, torch.tensor([[0, 1],[0,1]]))   # 第一列进行gather, 
# tensor([[1, 7], [1, 7]]) 
a.gather(1, torch.tensor([[0, 1],[0,1]]))
```
当 dim = 0 时, 取法为取指定行相应列的元素
$$\left[\begin{matrix}
0  &|&1 \\  0&  |  & 1
\end{matrix}\right]$$
dim = 0 时, 取法为取指定列相应行的元素。第一列取 0 行, 0行, 第二列取1行, 1行; 而
$$\left[\begin{matrix}
0  &1   \\   -  &  -  \\  0  & 1
\end{matrix}\right]$$
此时得到的是`[[1,2],[6,7]]`。 
**说明: 最常见的应用是在交叉熵的计算中, 获取到的概率矩阵 y_pred(例如784* 10), 正确目标是离散的点 y(0,1,2,3..9), 此时需要y_pred中,y所对应列的部分**, 即可采用:
```python 
y_pred.gather(1, y.view(-1, 1)) 
```

但实际上用的多的还是如下的方法: 
```python
criterion =  nn.CrossEntropyLoss()
y_pred = model(X)
loss = criterion(y_pred, y)
```


## 3. 基本回归模型和训练步骤
### (1) 基本模型训练步骤 
在 Pytorch 中, 最基本的部分包括如下几步(参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/🔥Pytorch Basics/Implements/简单 SGD 训练 MINIST 数据集示例|简单 SGD 训练 MINIST 数据集示例]]): 

对于训练数据集, 一般需要转换为 4 维张量， 如 (5000, 3, 32, 32),  分别对应 (N, C, W, H)  
N：这一维通常代表批量大小 (batch size)，即一次性输入到网络中的样本数量。在这个例子中，一次输入 5000 个样本。
C：这一维表示输入通道数。在图像处理中，输入通道通常对应于颜色通道，`3` 表示 RGB 图像（红、绿、蓝三种颜色)
W, H：这两个维度分别表示图像的高度和宽度。 
#### 1. 定义网络模型 
1. 继承继承 `nn.Module`, 并调用 `super().init()` 
2. **定义变换 (forward) 函数**
一般在 forward 函数中定义每一层变换,  而 
self.net 等一般也在   forward 中被调用, 即可以不使用 net, 而采用 forward 得到结果, 实际上都是一个输入 X 
```python
class CIFIAR_CNN(nn.Module):  
    def __init__(self, learning_rate = 0.02):  
        super(CIFIAR_CNN, self).__init__()  
  
        self.net = nn.Sequential(  
            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # 首先, 由于是 RGB 数据, 输入通道数为 3, 尺寸为 (batch_size, 3, 32, 32)            nn.Conv2d( 32, 64, kernel_size=3, padding=1), # 形成 64 *32 * 32 的输出  
            # nn.MaxPool2d(kernel_size=(2,2), stride=2),  # n * 64 * 16 * 16, or use the following:  
            nn.AdaptiveMaxPool2d((16, 16)) ,                # n * 64 * 16 * 16 target size  
            nn.AvgPool2d(kernel_size=(2,2), stride=(2,2)),  # n * 64 * 8 * 8  
            nn.Flatten(1, -1),    # 将 64 * 8 * 8 的张量展平  
            nn.Linear(64 * 8 * 8, 512),  
            nn.ReLU(),         # 放在 Linear 之后, 激活函数, 用于实现非线性学习, 得到更复杂的模型  
            nn.Linear(512, 10),  # CIFAR-10 has 10 classes, so output is 10 dimensions  
            nn.Softmax(dim=1)  # Softmax 激活函数：将输出转化为概率分布  
        )  
  
    def forward(self,X):  
        return self.net(X)
```

#### 2. 加载数据和训练
1. **初始变换和加载数据集** 
方法是定义一个函数或者 `torchvision.Compose()` 容器, 然后在 transform 中指定参数， 然后
> [!NOTE] 说明
> 在加载数据和训练的类中,  我们直接将相应的 `self.model` 指定为刚刚定义的类

**需要说明**:  `transform` 只会影响在 `DataLoader` 中批量加载数据时的图像。如果您想**查看经过缩放处理后的图像，您应该查看 `train_loader` 中的数据，而不是 `train_dataset.data`**。`train_loader` 批量加载数据时，会应用 `transform`。

```python
t = v2.Compose([  
    v2.ToDtype(torch.float32, scale=True),  
    v2.ToTensor(),  
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  
])  
train_dataset = CIFAR10( root='./cifar10', train=True, download=True, transform=t)
```

2. **定义一个 DataLoader 迭代器**
```python
self.train_loader = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=True)
```

3. **定义损失函数 (criterion 或者 loss) 和优化器(optim)** 
```python
def criterion(y_true, y_pred):
	return nn.CrossEntropyLoss(y_true, y_pred)
```

如果分类方法依赖于梯度, 则需要定义 optim 优化器: 
`optim` 的作用包括:
- **梯度更新**：基于模型参数的当前梯度调整参数值。
- **控制学习率**：学习率（`learning_rate`）是优化器的关键超参数，决定更新步伐大小。
```python
self.optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # 常用: SGD, Adam, RMSprop, AdaGrad 等等, 
# 其中 momentum 为用于加速收敛的动量设置项
```
需要说明,  如果不使用 optim 优化器, 可以手动进行参数更新:
```python
with torch.no_grad():  # 禁止计算图构建，直接更新参数
    for param in model.parameters():
        param -= learning_rate * param.grad  # 手动更新参数
```

4. 进行训练:  
其中， 调用 self.model.train 获取到
```python
def train_model(self, max_epoch = 100):  
    self.model.train()    # 只需要调用 train() 函数即可  
    # 初始时模型是未训练的模型  
    for epoch in range(max_epoch):  
        train_loss = 0.0  
        for data, labels in self.train_loader:  
            self.optim.zero_grad()   # 在每次小批量训练时, 都需要清零梯度  
            loss = self.criterion(self.model(data), labels)  
            loss.backward()         # 反向传播  
            self.optim.step()       # 利用反向传播得到的梯度, 进行参数更新  
            train_loss += loss.item() * data.size(0)    # 计算总和的误差
```

MaxPool 池化的参数指定方法: 
```python
# target output size of 5x7
m = nn.AdaptiveMaxPool2d((5, 7))
input = torch.randn(1, 64, 8, 9)
output = m(input)
# target output size of 7x7 (square)
m = nn.AdaptiveMaxPool2d(7)
input = torch.randn(1, 64, 10, 9)
output = m(input)
# target output size of 10x7
m = nn.AdaptiveMaxPool2d((None, 7))
input = torch.randn(1, 64, 10, 9)
output = m(input)
```


### (1)多项式回归模型
下面更进一步尝试一下多项式回归，下面是关于 x 的多项式：
$$
\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 
$$
这样就能够拟合更加复杂的模型，这里使用了 $x$ 的更高次，同理还有多元回归模型，形式也是一样的，只是除了使用 $x$，还是更多的变量，比如 $y$、$z$ 等等，同时他们的 $loss$ 函数和简单的线性回归模型是一致的。 
```python
import numpy as np  
import torch
import matplotlib.pyplot as plt
# 定义一个多变量函数
w_target = np.array([0.5, 3, 2.4]) # 定义参数
b_target = np.array([0.9]) # 定义参数

f_des = 'y = {:.2f} + {:.2f} * x + {:.2f} * x^2 + {:.2f} * x^3'.format(
    b_target[0], w_target[0], w_target[1], w_target[2]) # 打印出函数的式子

print(f_des)
```

我们下面以 `w0 + w1 + w2 + b = y_sample` 的式子, 以 randn 作为 w 初始值, 0 作为b 初始值进行小批量梯度算法求解获取结果(定义的两个函数中, 一个是模型, 一个是损失函数): 
```python
import numpy as np  
import matplotlib.pyplot as plt  
import torch  
  
fig = plt.figure()  
epochNum = 80  
lr = 0.001  

x_sample = np.arange(-3, 3.1, 0.1)    # 定义绘画数组区间  
y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample ** 2 + w_target[2] * x_sample ** 3  
# plt.plot(x_sample,y_sample)  
# ----------------------------------------------------------  
# 构建数据 x 和 y# x 是一个如下矩阵 [x, x^2, x^3]# y 是函数的结果 [y]x_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)  # 构建从相应的数据取得的自变量范围矩阵  
x_train = torch.from_numpy(x_train).float() # 转换成 float tensory_train = torch.from_numpy(y_sample).float().unsqueeze(1) # 转化成 float tensor  
# 定义参数 w和b  
w = torch.randn((3, 1), dtype=torch.float, requires_grad=True)  # 每一次run得到的初始fit曲线不同  
b = torch.zeros((1), dtype=torch.float, requires_grad=True)  

# 定义模型
def multi_linear(x):  
    return torch.mm(x, w) + b   # 注: mm可以使用matmul来进行代替  
  
def loss_func(y_, y):  # 使用平方函数作为损失函数  
    return torch.mean((y_ - y) ** 2)  
y_pred = multi_linear(x_train) # 构建多项式函数  
  
ax1 = fig.add_subplot(121)  
ax1.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='curve_prefit', color='r')  
ax1.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b') 

# 画出更新之前的模型  
# ============ 训练数据集 ====================
for epoch in range (epochNum):  
    loss = loss_func(y_pred, y_train)  
    try:  
        w.grad.zero_()  
        b.grad.zero_()  
    except:  
        pass  
    loss.backward()  
    w.data = w.data - lr * w.grad.data  
    b.data = b.data - lr * b.grad.data  
    y_pred = multi_linear(x_train)  
  
ax2 = fig.add_subplot(122)  
ax2.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve-', color='r')  
ax2.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b')  
  
plt.legend()  
plt.show()
```

### (2) Logistic 回归模型
对于经典的$(0,1)$模型，使用交叉熵损失函数
$$loss = -[y * log(\hat{y}) + (1-y) *log(1-y)]$$
```python
def logistic_regression(x):
    return torch.sigmoid(torch.mm(x, w) + b)
## mm: matmul

y_pred = logistic_regression(x_data)
# 计算loss, 使用clamp的目的是防止数据过小而对结果产生较大影响。
def binary_loss(y_pred, y):
    logits = ( y * y_pred.clamp(1e-12).log() + \
              (1 - y) * (1 - y_pred).clamp(1e-12).log() ).mean()
    return -logits
```
只需要将loss改为这个即可。

其中，pytorch提供了相关的函数，重要的是 torch.optim.SGD 可以直接采用小批量梯度回归, 另外每一次需要调用 optimizer.zero_grad()清零优化器梯度即可。
```python title:最简单的小批量梯度下降回归代码(外部定义模型y_pred和损失binary_loss)
# 使用 torch.optim 更新参数
from torch import nn
import time
# use the neural network model for training
# 定义优化参数
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

# 优化器的设定以及相关的使用
optimizer = torch.optim.SGD([w, b], lr=0.1) # 使用SGD算法

for e in range(1000):
    # 前向传播
    y_pred = logistic_regression(x_data)  # 这个y_pred和loss是自己定义函数计算的
    loss = binary_loss(y_pred, y_data)      # 计算 loss
    # 反向传播
    optimizer.zero_grad() #-----使用优化器将梯度归 0
    loss.backward()
    optimizer.step() # *** 在这一步中，更新参数被封装，使用优化器来更新参数 **** 
    # 计算正确率
    mask = y_pred.ge(0.5).float()
    acc = (mask == y_data).sum().item() / y_data.shape[0]
```


## 三、PyTorch提供的损失函数 
前面使用了自己写的 loss函数，其实 PyTorch 已经提供了一些常见的 loss函数，比如线性回归里面的 loss 是 `nn.MSE()`，而 Logistic 回归的二分类 loss 在 PyTorch 中是 `nn.BCEWithLogitsLoss()`，关于更多的 loss，可以查看[文档](https://pytorch.org/docs/stable/nn.html#loss-functions)

PyTorch 实现的 loss函数有两个好处：第一是方便使用，不需要重复造轮子；第二就是其实现是在底层 C++ 语言上的，所以速度上和稳定性上都要比自己实现的要好。

另外，PyTorch 出于稳定性考虑，将模型的 Sigmoid 操作和最后的 loss 都合在了 `nn.BCEWithLogitsLoss()`，所以我们使用 PyTorch 自带的 loss 就不需要再加上 Sigmoid 操作了
```python
# 使用自带的loss
criterion = nn.BCEWithLogitsLoss() # 将 sigmoid 和 loss 写在一层，有更快的速度、更好的稳定性
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

def logistic_reg(x):
    return torch.mm(x, w) + b
    
optimizer = torch.optim.SGD([w, b], 1.)

y_pred = logistic_reg(x_data)
loss = criterion(y_pred, y_data)  # 这样定义损失函数
# 之后调用
for ......
	loss.backward()
	optimizer.step()_
print(loss.data)
```

常见的损失函数：
浅层： LR，SCM, ELm
深层：CNN ..




### (3) 损失函数专题 
损失函数实际上是一种经验风险最小化的思路， 但是需要注意[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/⚓Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#1. Basic Concept|经验风险和实际风险的区别]]. 常见的损失函数有: 0-1损失函数，绝对值损失函数，平方损失函数，Log对数损失函数，指数损失函数，交叉熵损失函数，Hinge损失函数等等;

#### 1. MSE, MAE 损失函数
MSE : 均方误差损失函数(MSE，mean square error), 一般用于回归问题(most commonly used loss function for regression tasks.) 
$$L = \frac{1}{n} \sum^{n}_{i=1} (\hat{y}_i -y_i)^2$$
```python
nn.MSELoss()
```

**Mean Absolute Error (MAE)** 
$$\text{L1Loss} = \frac{1}{n} \sum_{i=1}^n |\hat{y}_i - y_i|$$
其调用方法是
```python
nn.L1Loss() 
```

**Huber Loss** 
对 Huber Loss 实际上是 MSE 和 MAE 的组合,   具体公式如下 : 
$$L_{\delta}(a) =
\begin{cases} 
\frac{1}{2}a^2 & \text{if } |a| \leq \delta, \\
\delta (|a| - \frac{1}{2} \delta) & \text{otherwise.}
\end{cases}$$
```python
nn.HuberLoss
``` 

#### 2. 交叉熵损失函数 
交叉熵损失函数 (常用于分类问题)
$$H(y^{(i)}, \hat{y}^{(i)}) = -\overset{q}{\underset{j = 1}{\sum}}y_j^{(i)}\log(y_j^{(i)})$$
其中, $\hat{y}_j^{(i)}$非0即1  为分类标签;

一般地, 为了防止模型过拟合， 往往会加入<b><mark style="background: transparent; color: blue">正则化参数</mark></b>， 实际上是权重损失的概念, 参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/9. 正则化方法, 概率图模型和贝叶斯网络|正则化方法, 概率图模型和贝叶斯网络]] 部分;

## 四、神经网络池化层的概念简介
池化层也是一个类似二维的层;

池化层在CNN中,用于减少空间维数进行降维使用, 实际上是使用一个feature map, 其宽高分别为 nh 和 nw, 类似于卷积过程, 在 map 的指定范围内进行扫描; 然后过滤出符合要求的部分。
一般池化需要指定 feature map 大小和 stride, 示例如下: 
![[attachments/1_fhdK1xJND_m1Rdmr1m_9PQ.webp]]
例如16x16的输入在2x2窗, 步进为(2x2)池化后成为 8x8 的特征; 常见的池化有均值池化和最大值池化等等; 


# 图像处理专题
### (1) 图片缩放代码 (Pytorch, sklearn 和  cv2) 
#### 1.  通过 skimage 库进行图形缩放 
一般地,  对于 32 x 32 的图像数据集, 输入是一个 (50000, 32, 32, 3) 的向量 , 可以采用 `cv2.resize` 或者 skimage.transform.resize 进行批量调整
```sh
pip install scikit-image
```

以下是 skimage 的
```python
import numpy as np
from skimage.transform import resize

# 假设输入的图像数据是 (50000, 32, 32, 3)
images = np.random.rand(50000, 32, 32, 3)

# 使用 resize 批量调整大小
resized_images = np.zeros((images.shape[0], 28, 28, 3))  # 初始化一个新的数组，目标尺寸是 (50000, 28, 28, 3)
for i in range(images.shape[0]):
    resized_images[i] = resize(images[i], (28, 28), mode='reflect', preserve_range=True)

print(resized_images.shape)  # 输出 (50000, 28, 28, 3)
```
处理 大批量图像时, 也可以采用
```python
import numpy as np
import cv2

# 假设输入的图像数据是 (50000, 32, 32, 3)
images = np.random.rand(50000, 32, 32, 3)

# 使用 cv2.resize 批量调整大小
resized_images = np.zeros((images.shape[0], 28, 28, 3), dtype=np.float32)
for i in range(images.shape[0]):
    resized_images[i] = cv2.resize(images[i], (28, 28))
print(resized_images.shape)  # 输出 (50000, 28, 28, 3)
```

#### 2. 对于  Pytorch 小批量训练
首先我们通过 `train_dataset = CIFAR10( root='./cifar10', train=True, download=True, transform=t)  ` 得到了实际的数据集,  而实际上
`self.train_dataset.data.shape` 是 (50000, 32, 32, 3) 大小的数据集(包含 RGB 数据), 而将其转换为 28 * 28 图像只需 `transforms.Resize((28, 28)),  # 将图像缩放到 28x28`  


其中, 参考 [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#flatten) 部分, 如果得到的输出需要变换为一维输出，则可以在 net 中插入一个 nn.Flatten 层,  会将后面的三维展开成一维, 
```python
input = torch.randn(32, 1, 5, 5)
# With default parameters
m = nn.Flatten()
output = m(input)
output.size()  # torch.Size([32, 25])
# With non-default parameters
m = nn.Flatten(start_dim =0, end_dim=2) # 将 0-2 维展平得到 (32 * 1 * 5 , 5)的形状
output = m(input)
output.size() # (160, 5)
```


### (2) 激活函数专题
在深度学习中，激活函数（如 `ReLU`, `Softmax` 等）通常用于添加非线性变换，使得神经网络能够学习到更复杂的模式。你可以将激活函数插入到 `nn.Linear` 层之后，通常是在输出层之前或某些中间层之后进行非线性变换。

常用的激活函数如下:
#### 1. **ReLU 激活函数**
$$ \text{ReLU} (x) = \max(0,x)$$
![[Excalidraw/Pytorch & sklearn的使用基础和基本代码 2024-12-21 12.24.08|250]]
`ReLU` 是常用的激活函数，通常用于 **隐藏层**，它会对输入进行非线性转换，防止梯度消失问题。一般来说，`ReLU` 放在 `nn.Linear` 层之后。
例如，在 `nn.Linear` 之后添加 `ReLU` 激活函数的写法：

```python
self.net = nn.Sequential(
	nn.Conv2d(3, 32, kernel_size=3, padding=1),   # 输入：batch_size, 3, 32, 32
	nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 输出：batch_size, 64, 32, 32
	nn.AdaptiveMaxPool2d((16, 16)),               # 输出：batch_size, 64, 16, 16
	nn.AvgPool2d(kernel_size=(2, 2), stride=2),   # 输出：batch_size, 64, 8, 8
	nn.Flatten(),                                 # 展平：batch_size, 64 * 8 * 8
	nn.Linear(64 * 8 * 8, 512),                   # 输入：batch_size, 64 * 8 * 8，输出：batch_size, 512
	nn.ReLU(),                                   # ReLU 激活函数
	nn.Linear(512, 10)                            # 输出层：10个类别
)
```

- 在 `nn.Linear(64 * 8 * 8, 512)` 后插入 `nn.ReLU()`，帮助网络学习更复杂的表示。

#### 2. **Sigmoid 激活函数**
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/⚓Deep Learning Basic Concepts/Chapter3 Linear Neural  Networks for regression(back Propagation)#(3) Sigmoid function|Sigmoid function]], 有公式关系:
$$\text{Sigmoid}(x) =  \frac{1}{1 + e^{-x}}$$
用于将 wx + b 部分投影到 (0,1) 之间, 基本图像如下: 
![[Excalidraw/Pytorch & sklearn的使用基础和基本代码 2024-12-21 12.07.40|250]]
直接采用如下访问:
```python
torch.sigmoid(x);
```

**什么时候使用 Sigmoid**:

- **输出层**: 如果需要概率输出（如二分类问题），`Sigmoid` 通常用于最后一层。
- **特定任务**: 在某些需要严格约束输出范围为 `[0, 1]` 的任务中，比如生成器网络（GANs）或图像处理任务中像素值的归一化。

#### 3. tanh 激活函数
$$\tanh  (x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}  = 1 - \frac{2\exp (-2x)}{1+ \exp (-2x)}$$
其导数为:
$$\boxed{\tanh '(x) = 1 - \tanh^{2} x}$$

![[Excalidraw/Pytorch & sklearn的使用基础和基本代码 2024-12-21 12.12.08|250]]
#### 4. **Softmax 函数** 
softmax 实际上不是激活函数, 是一个归一化函数, 通常放在 **输出层**，用于多分类任务的最终输出层，以将网络的输出转换为概率分布。`Softmax` 将每个类的输出转换为 `[0, 1]` 之间的概率，并确保所有类的概率和为 1(参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/9. softmax函数和交叉熵损失CrossEntropy|softmax函数和交叉熵损失]]部分). 

在多分类问题中，`Softmax` 主要用于 **最后一层**，用于输出每个类别的概率。( 如果是二分类问题， 则采用 `Sigmoid`，或者多标签问题)  
示例 : 
但是需要注意, 当采用 CrossEntropyLoss 时,  我们不需要使用 Softmax, 因为实际上已经施加了 `log_softmax` 部分; 
```python
self.net = nn.Sequential(
	nn.Conv2d(3, 32, kernel_size=3, padding=1),   # 输入：batch_size, 3, 32, 32
	nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 输出：batch_size, 64, 32, 32
	nn.AdaptiveMaxPool2d((16, 16)),               # 输出：batch_size, 64, 16, 16
	nn.AvgPool2d(kernel_size=(2, 2), stride=2),   # 输出：batch_size, 64, 8, 8
	nn.Flatten(),                                 # 展平：batch_size, 64 * 8 * 8
	nn.Linear(64 * 8 * 8, 512),                   # 输入：batch_size, 64 * 8 * 8，输出：batch_size, 512
	nn.ReLU(),                                   # ReLU 激活函数
	nn.Linear(512, 10),                           # 输出层：10个类别
	nn.Softmax(dim=1)                             # Softmax 激活函数：将输出转化为概率分布
)
```
- `Softmax` 放在 `nn.Linear(512, 10)` 后，通常会在训练时使用交叉熵损失 (`nn.CrossEntropyLoss`)，这个损失函数会自动应用 `Softmax`。
- **ReLU** 通常应用于 **隐藏层**，位于每个 `nn.Linear` 后，用于引入非线性。
- **Softmax** 通常应用于 **输出层**，用于多分类任务，将网络的输出转化为概率分布。`Softmax` 应放在最后的输出层之后。

激活函数的一般规则:
- **隐藏层：** `ReLU`, `LeakyReLU`, `Tanh`, `Sigmoid` 等（根据任务和需求选择）。
- **输出层（分类任务）：** `Softmax`（多分类问题）或 `Sigmoid`（二分类问题，或者多标签问题）。

### 3.  Logarithmic MSE 误差 
对于房价等等较大的数据, 一般我们更加关心的是相对于原始价格的准确程度, 
$$\sqrt{\frac{1}{n} \sum_{i = 1}^{n} (\log_{} y_{i}  - \log_{} \hat{y}_{i})^{2}}$$
具体参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/⚓Deep Learning Basic Concepts/Chapter5 Essential Concepts & Heuristics#(1) Logarithmic MSE Loss|Logarithmic MSE Loss]] 部分

Log-Cosh 误差, 可以采用:
$$\text{Log-Cosh}(x) = \sum_{i=1}^{n} \log(\cosh(\hat{y}_{i} - y_{i}))$$

### 验证集和k折交叉验证
- 方法是将训练数据集分割成k个不同的子数据集，每一次使用1个数据集验证，k-1个训练 
- 做k次模型训练和验证，并将k次的训练误差和验证误差分别求平均值
参考  [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/⚓Deep Learning Basic Concepts/Chapter5 Essential Concepts & Heuristics#(2) K-fold Cross-Validation|K-fold Cross-Validation]] 部分.


### 定义训练的层 
需要说明,  Pytorch  中, 支持自定义不同的层, 仍然可以采用 nn.Module 进行定义; 仍然需要继承 nn.Module 类型,  并且重写 forward 方法; 

forward 必须采用 `def forward(self, X):`  为定义, 如果有 forward, 则可以视为一层;

```python
import torch
import torch.nn as nn

class EpochDropout(nn.Module):
    def __init__(self, p=0.5):
        super(EpochDropout, self).__init__()
        self.p = p
        self.mask = None

    def forward(self, x):
        if self.training:
            # 如果 mask 为空，则生成 mask
            if self.mask is None or self.mask.shape != x.shape:
                self.mask = torch.bernoulli(torch.full(x.shape, 1 - self.p)).to(x.device) / (1 - self.p)
            return x * self.mask
        else:
            return x  # 测试阶段不使用 Dropout

    def reset_mask(self):
        """在每个 epoch 开始时重置 mask"""
        self.mask = None
```

```python
self.dropout = EpochDropout(p=0.5)
# 或者直接添加到 net 中 
```

### (4) 平均值层简介 
1. Batch Norm  
Add `nn.BatchNorm2d` after convolutional layers to stabilize learning and improve convergence speed.

`nn.BatchNorm2d` 是一种常见的神经网络层normalization技术,主要用于卷积神经网络中。它通常被添加在卷积层之后,起到以下作用:

1. **加速训练收敛**:
   - 通过标准化输入数据,可以有效缓解内部协变移移(internal covariate shift)的问题,加快模型的训练收敛速度。

2. **提高泛化能力**:
   - BatchNorm能够使得模型对输入数据的分布变化更加鲁棒,从而提高模型的泛化性能。

3. **降低对初始化的依赖**:
   - 由于BatchNorm能够对数据进行动态标准化,因此模型的训练过程对初始化参数的依赖性大大降低。

4. **防止梯度消失/爆炸**:
   - BatchNorm能够有效地缓解梯度消失或爆炸的问题,为网络的训练提供更加稳定的环境。

**一般情况下, `nn.BatchNorm2d` 层会被添加在卷积层和激活函数层之间, 这样可以在特征提取和特征变换之后对特征进行归一化**,从而获得更加稳定和鲁棒的特征表示。


## 常用的层部分
nn.GroupNorm,  nn.BatchNorm1d, nn.BatchNorm2d  等等

**此外, `nn.BatchNorm2d` 还有一些其他的变体,如 `nn.BatchNorm1d` 用于全连接层, `nn.InstanceNorm2d` 用于图像生成等任务**。实际应用中需要根据具体问题和网络结构来选择合适的normalization技术。

 `@torch.no_grad()` 提供了将整个函数不进行梯度的控制修饰器。

ResNet 


有一些比 `AdaptiveMaxPool2d` 更加优越的池化方法,具体取决于实际问题和模型需求。以下是一些对比和建议:

1. **GlobalAveragePooling2d**:
   - 这种方法将特征图上每个通道的特征值取平均,得到一个全局特征向量。
   - 相比 `AdaptiveMaxPool2d`，GlobalAveragePooling2d 能够更好地保留特征信息,不会丢失重要的特征。
   - 在一些任务上,如图像分类,GlobalAveragePooling2d 表现更优。

2. **Spatial Pyramid Pooling (SPP)**:
   - SPP 利用多尺度的池化操作,能够更好地捕捉不同尺度的特征信息。
   - 相比固定大小的池化,SPP 能够适应不同大小的输入,提高模型的鲁棒性。
   - SPP 模块可以作为卷积网络的一部分,与其他层一起端到端训练。

3. **Dilated Convolution**:
   - Dilated Convolution 可以在不降低分辨率的情况下扩大感受野,捕获更丰富的上下文信息。
   - 相比池化,Dilated Convolution 不会丢失特征信息,能够更好地保留原始的语义特征。

关于您提到的GPU足够的情况下,是否应该使用池化层:

- 如果 GPU 资源足够充裕,一般来说不使用池化层训练效果会更好。这是因为池化会丢失一些有价值的特征信息。
- 但如果GPU资源受限,使用池化层可以有效降低计算量和内存占用,从而能够训练更深更大的模型。这种情况下,池化的优势可能会大于其带来的信息损失。

综合考虑,如果GPU资源充足,可以尝试使用 GlobalAveragePooling2d 或 Dilated Convolution 等更高级的池化/感受野扩张技术。同时也要根据具体任务和模型需求进行实验验证,找到最合适的方案。


### 保存和加载模型 
一般对于任意 nn.Module 模块， 可以通过存储 state_dict 获取其中所有参数信息。 
```python
torch.save(self.model.state_dict(), 'cnn_models.pth')
```

然后采用 load_state_dict 得到原始模型参数
```python
model = EasyModule()
model.load_state_dict(torch.load('cnn_models.pth')) 
```



##  模型显示方法: 
一般对于  nn.Module 类型继承的模型， 可以直接使用 
`print(model)` 打印模型输入和输出部分， 

或者 module.named_parameters() 打印
```python
for name, param in model.named_parameters():
    print(f"Layer: {name}, Size: {param.size()}")
```

此外可以采用 `torchsummary`  库获取更加详细的模型结构. 
```python
from torchsummary import summary

summary(model, input_size=(10,))
```

### Lazy 网络 

> [!HINT] 为何采用 Lazy 网络
> 常用的 Lazy 网络包括 LazyLinear,  LazyConv2d 等, 其输入数量是根据前一层的输出确定的，因此可以灵活处理不同形状的输入数据， 因此能够更好地迁移到不同的数据集上来  
