å¯¹äº Pytorch, æœ‰ä¸¤ä¸ªæ–‡æ¡£ï¼Œ å…¶ä¸­ä¼ ç»Ÿçš„æ˜¯ pytorch,  å¦å¤–è¿˜æœ‰[Quantized Documentation](https://pytorch.org/docs/stable/quantization-support.html#module-torch.ao.nn.quantized.modules) å¯¹å„ä¸ªç»´åº¦åšäº†æ³¨è§£ï¼Œ å¸¸ä¼šä½¿ç”¨ã€‚ 

## ä¸€ã€åŸºæœ¬æ•°æ®æ“ä½œ
### (1) å¸¸ç”¨å˜å½¢å‡½æ•° 
å¯¹åº”äº tensor æ”¯æŒçš„å‡½æ•°ï¼Œå…·ä½“å‚è€ƒ https://pytorch.org/docs/stable/tensors.html 
#### 1. unsqueeze,squeezeå’Œreshapeä½¿ç”¨
å¢åŠ ç»´åº¦æˆ–è€…å‡å°‘ç»´åº¦: 
`torch.squeeze(input,dim=0)` **ç§»é™¤æŸä¸ªç»´åº¦**
`torch.unsqueeze(input,dim=0)` **åœ¨æŸä¸ªç»´åº¦ä¸Šå¢åŠ ç»´åº¦**  
reshape çš„ä½¿ç”¨: åœ¨ pytorch ç­‰ä¸­, æˆ‘ä»¬å¸¸å¸¸ä½¿ç”¨ reshape å°†æ•°æ®é›†è½¬æ¢ä¸ºæŒ‡å®šçš„å½¢çŠ¶, ä¾‹å¦‚å¯¹äº (60000, 28,28) çš„æ•°æ®é›†, æˆ‘ä»¬ä¼šä½¿ç”¨ `x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)` å¾—åˆ° (60000,28,28,1) çš„4dæ•°æ®é›†, å³å°†æœ€åçš„28ä¸ªä¸­, æ¯ä¸ªå‡ä½œä¸ºè¾“å…¥æ‹†åˆ†å¼€;

```python title:squeezeä½¿ç”¨ç¤ºä¾‹
train[1].unsqueeze(0).shape
Out[12]: torch.Size([1, 28, 28])
A = train[1].unsqueeze(0).unsqueeze(0)
A.shape
Out[13]: torch.Size([1, 1, 28, 28]) 
A.squeeze(0).shape
Out[17]: torch.Size([1, 28, 28])
A.squeeze(0).squeeze(0).shape
Out[19]: torch.Size([28, 28])
A.squeeze([0,1]).shape
Out[22]: torch.Size([28, 28])
B.unsqueeze(dim=2).shape
Out[28]: torch.Size([28, 28, 1])
```

#### 2. View Copy å’Œ Clone
1. view è¿”å›æœ¬èº«çš„å¯¹è±¡ï¼Œ åªæ˜¯ "è§‚å¯Ÿç»´åº¦" ä¸åŒï¼Œ å¾—åˆ°å°ºå¯¸ä¸åŒã€‚ç›¸å½“äº reshape ä½†ä¿ç•™åŸæ¥å¯¹è±¡ 
```python fold title:
a.view()
```

2. **`tensor.clone()`** ä¸ºæ·±æ‹·è´  
- **ä¿ç•™è®¡ç®—å›¾**ï¼ˆä¿ç•™æ¢¯åº¦ä¼ æ’­è·¯å¾„ï¼‰ï¼š 
    - å¦‚æœå¯¹Â `clone()`Â çš„ç»“æœè¿›è¡Œæ“ä½œï¼Œæ¢¯åº¦ä¼šåå‘ä¼ æ’­åˆ°åŸå§‹å¼ é‡ã€‚
    - é€‚ç”¨äºéœ€è¦ä¿ç•™è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰çš„åœºæ™¯ã€‚  

2. **`tensor.copy_(src)`**
åŸåœ°æ“ä½œï¼šå°† src çš„æ•°æ®å¤åˆ¶åˆ°ç›®æ ‡å¼ é‡ä¸­ï¼ˆè¦†ç›–ç›®æ ‡å¼ é‡çš„å€¼ï¼‰ï¼Œä¸åˆ›å»ºæ–°å¼ é‡ã€‚
ä¸ä¿ç•™è®¡ç®—å›¾ : `copy_()` æ˜¯ä¸€ä¸ªå°±åœ°æ“ä½œï¼ˆin-placeï¼‰ï¼Œä¼šä¸­æ–­æ¢¯åº¦ä¼ æ’­ã€‚
é€‚ç”¨äºä¸éœ€è¦æ¢¯åº¦æ›´æ–°çš„åœºæ™¯ï¼ˆå¦‚æ¨¡å‹æƒé‡åˆå§‹åŒ–ï¼‰ã€‚
éœ€è¦ç›®æ ‡å¼ é‡å’Œæºå¼ é‡å½¢çŠ¶ä¸€è‡´ã€‚
```python fold title:
a = torch.tensor([1., 2.])
b = torch.tensor([3., 4.], requires_grad=True)
a.copy_(b)  # å°† b çš„å€¼å¤åˆ¶åˆ° aï¼Œa å˜ä¸º tensor([3., 4.])

print(a)  # è¾“å‡º: tensor([3., 4.])
b.sum().backward()  # a çš„ä¿®æ”¹ä¸ä¼šå½±å“ b çš„æ¢¯åº¦
```

#### 3.  narrow, expand, contiguous  
There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is organized. These operations include:

> `narrow()`,Â `view()`,Â `expand()`Â andÂ `transpose()` 

1. `narrow(dim, start, length)`  : Returns a narrowed version of the tensor along a specified dimension.
for  example : 
```python fold title:narrow-usage
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
y = x.narrow(1, 1, 2)  # Takes columns 1 to 2 (0-based)
# y = [[2, 3], [5, 6]]
```

2. `expand(*sizes)`: Expands a tensor to a larger size by repeating elements (<b><mark style="background: transparent; color: orange">only works for size-1 dimensions</mark></b>). 
```python fold title:expand-usage 
x = torch.tensor([[1], [2]])  # shape (2, 1)
y = x.expand(2, 3)  # [[1, 1, 1], [2, 2, 2]]
``` 

3. `contiguous` :  **Ensures the tensor is stored in contiguous memory (row-major order)**.    
```python
x = torch.randn(3,2)
y = torch.transpose(x, 0, 1)
```
in that case x is `contiguous` but y not.  

> [!warning] Why need contiguous memory
> Some operations (e.g.,Â `view()`) <b><mark style="background: transparent; color: orange">require contiguous memory</mark></b>. Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get aÂ `RuntimeError: input is not contiguous`Â where PyTorch expects a contiguous tensor to add a call toÂ `contiguous()`.

#### 4. repeat_interleave 
åŸºæœ¬å‡½æ•°`torch.repeat_interleave (_input_,Â _repeats_,Â _dim=None_,Â _*_,Â _output_size=None_)` 
å¯¹äº `repeat_interleave` éƒ¨åˆ†,  

æ³¨æ„ : By default, if dim not specified, use the flattened input array, and return a flat output array. 

```python fold title:examples 
y = torch.tensor([[1, 2], [3, 4]])
torch.repeat_interleave(y, 3, dim=1)
tensor([[1, 1, 1, 2, 2, 2],
        [3, 3, 3, 4, 4, 4]]) 
torch.repeat_interleave(y, 2)

torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)
tensor([[1, 2],
        [3, 4],  # ç¬¬äºŒä¸ª repeat æ•°ä¸º 2 
        [3, 4]])
```

#### 5.  transpose,  permute ,  reshape 
å¯¹äº transpose,  `torch.transpose(_input_,Â _dim0_,Â _dim1_)` éå¸¸å®¹æ˜“ï¼Œ ä¸å¤šè®²

permute ç”¨æ³•å¦‚ä¸‹ : 
```python fold title:
torch.permute(input, dims) â†’ Tensor
``` 

å…¶ä¸­æœ‰ : 
- input (Tensor) â€“ the input tensor. 
- dims (tuple of int) â€“ The desired ordering of dimensions  

å®é™…ä¸Š dims æ˜¯ `(0,1,2,3)` é¡ºåºçš„éšä¾¿æ’åˆ—ç»„åˆ, ä¾‹å¦‚ : 
```python fold title:example-of-permute 
>>> x = torch.randn(2, 3, 5)
>>> x.size()
torch.Size([2, 3, 5])
>>> torch.permute(x, (2, 0, 1)).size()
torch.Size([5, 2, 3]) 
```

å¯¹äº torch.reshape ï¼Œ æ¥å—ä¸€ä¸ªå…ƒç»„ï¼Œ è½¬æ¢å½¢çŠ¶  
- data 
- **shape**Â ([_tuple_](https://docs.python.org/3/library/stdtypes.html#tuple "(in Python v3.13)")Â _of_Â [_int_](https://docs.python.org/3/library/functions.html#int "(in Python v3.13)")) â€“ the new shape 

### (2) æ•°æ®ç±»å‹è½¬æ¢
#### 1. è½¬æ¢å€¼  
å– tensor å€¼æ–¹æ³• : é‡‡ç”¨  item() è·å– Tensor çš„å€¼: `label_item = torch.Tensor(label).item()`  

å¯¹äºå¸¦æœ‰è‡ªåŠ¨æ±‚å¯¼éƒ¨åˆ†ï¼Œ é‡‡ç”¨ `x.detach.numpy()` 
```python
# detach å–æ¶ˆæ±‚å¯¼è¿æ¥,  è€Œ numpy è½¬æ¢ä¸º numpy æ•°ç»„
y.detach.numpy(); 
```

#### 2. GPU è®¾å¤‡æ•°æ®ä¼ è¾“ 
å¯¹äºæ‰€æœ‰çš„ Tensor å’Œæ¨¡å‹ (nn.module) éƒ½æä¾›äº†`to` æ–¹æ³•,   ä¸€èˆ¬é‡‡ç”¨ `.cuda()` , `.cpu` , `.to(self.device)`  è¿›è¡Œ gpu æ•°æ®ç§»åŠ¨ã€‚ 

å¯¹äºå¤š GPU è®­ç»ƒçš„æ¨¡å‹ï¼Œ å¯èƒ½æ¶‰åŠæ•°æ®ä¸åŒ GPU ä¹‹é—´çš„ä¼ é€’ 
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  
print(torch.cuda.device_count())

num = 1
data.to(torch.device(f'cuda:{num}'))   # å°†æ•°æ®æ”¾åœ¨æŸä¸ªgpu ä¸Š
model.to(torch.device('cuda'))
```

å¦å¤–ï¼Œpytorchä¸­å¤§å¤šæ•°çš„æ“ä½œéƒ½æ”¯æŒÂ `inplace`Â æ“ä½œï¼Œä¹Ÿå°±æ˜¯å¯ä»¥ç›´æ¥å¯¹ tensor è¿›è¡Œæ“ä½œè€Œä¸éœ€è¦å¦å¤–å¼€è¾Ÿå†…å­˜ç©ºé—´ï¼Œæ–¹å¼éå¸¸ç®€å•ï¼Œä¸€èˆ¬éƒ½æ˜¯åœ¨æ“ä½œçš„ç¬¦å·åé¢åŠ `_`  

### (3) å¸¸ç”¨å†…ç½®å‡½æ•° 
`torch.topk`  ä¹Ÿç§°ä¸º `tensor.topk`  
```python fold title:topk-usage
x = torch.arange(1., 6.)
val, idx = torch.topk(x,3) 
val, idx 
Out[6]: (tensor([5., 4., 3.]), tensor([4, 3, 2]))
```

## äºŒã€nn.utils åŒ…å¸¸ç”¨ç»„ä»¶ 

### (2) RNN åŒ…
-  `torch.nn.utils.rnn.pack_padded_sequence ` å’Œ Â [`pad_packed_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence "torch.nn.utils.rnn.pad_packed_sequence")  

note [`pad_packed_sequence()`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence "torch.nn.utils.rnn.pad_packed_sequence")Â can be used to recover the underlying tensor packed in 

ä¸ä½¿ç”¨ `pack_padded_sequence` 
```python fold title:
padded_input = torch.tensor([[1, 2, 0], [3, 0, 0], [4, 5, 6]])  # é•¿åº¦ [2, 1, 3]
output, hidden = rnn(padded_input)  # RNN ä¼šè®¡ç®—æ‰€æœ‰ä½ç½®ï¼ˆåŒ…æ‹¬ 0ï¼‰ 

# ä¸‹é¢æ˜¯ä½¿ç”¨ pack_padded_sequence  ä¼˜åŒ–æ•ˆç‡ä¹‹åçš„è®¡ç®—æ–¹æ³• :   
lengths = torch.tensor([2, 1, 3])
packed = pack_padded_sequence(padded_input, lengths, batch_first=True)
output, hidden = rnn(packed)  # RNN ä»…è®¡ç®—æœ‰æ•ˆéƒ¨åˆ†
```

éœ€è¦æ˜¯è¯´æ˜çš„æ˜¯ï¼Œ å½“Â `pack_padded_sequence`Â çš„Â `enforce_sorted=False`ï¼ˆé»˜è®¤Â `True`ï¼‰ï¼ŒPyTorch ä¼šÂ **è‡ªåŠ¨æŒ‰åºåˆ—é•¿åº¦é™åºæ’åº**ï¼Œè€Œï¼Œ è¿™æ ·åšçš„ä¸»è¦ç›®çš„æ˜¯

**(1) æé«˜è®¡ç®—æ•ˆç‡**
- RNN åœ¨å¤„ç†å˜é•¿åºåˆ—æ—¶ï¼Œæ˜¯æŒ‰æ—¶é—´æ­¥ï¼ˆ`time step`ï¼‰ä¾æ¬¡è®¡ç®—çš„ã€‚
- **å¦‚æœæ‰¹æ¬¡å†…åºåˆ—é•¿åº¦é™åºæ’åˆ—**ï¼ŒçŸ­åºåˆ—ä¼šæå‰ç»ˆæ­¢è®¡ç®—ï¼Œé¿å…æ— æ•ˆçš„ padding è®¡ç®—ã€‚
  - **ç¤ºä¾‹**ï¼š
    - å‡è®¾ `batch_size=3`ï¼Œåºåˆ—é•¿åº¦åˆ†åˆ«ä¸º `[5, 3, 1]`ã€‚
    - åœ¨ `time_step=1`ï¼Œæ‰€æœ‰ 3 ä¸ªåºåˆ—éƒ½è®¡ç®—ã€‚
    - åœ¨ `time_step=4`ï¼Œåªæœ‰ç¬¬ 1 ä¸ªåºåˆ—ï¼ˆé•¿åº¦ 5ï¼‰ä»åœ¨è®¡ç®—ï¼Œå…¶ä½™ 2 ä¸ªåºåˆ—å·²ç»ˆæ­¢ã€‚
    - **å‡å°‘ä¸å¿…è¦çš„è®¡ç®—**ï¼Œæå‡ GPU åˆ©ç”¨ç‡ã€‚

**(2) é¿å… `CUDA` è®¡ç®—æ—¶çš„åŒæ­¥é—®é¢˜**
- GPU å¹¶è¡Œè®¡ç®—æ—¶ï¼Œå¦‚æœæŸäº›åºåˆ—å·²ç»ç»“æŸï¼ˆå¦‚ `[5, 3, 1]` çš„ `time_step=2` åªæœ‰å‰ 2 ä¸ªåºåˆ—éœ€è¦è®¡ç®—ï¼‰ï¼ŒPyTorch å¯ä»¥æå‰é‡Šæ”¾å·²å®Œæˆåºåˆ—çš„è®¡ç®—èµ„æºã€‚
- å¦‚æœåºåˆ—æœªæ’åºï¼ˆå¦‚ `[3, 5, 1]`ï¼‰ï¼ŒGPU å¯èƒ½è¢«è¿«åœ¨æ‰€æœ‰æ—¶é—´æ­¥ä¸ŠåŒæ­¥è®¡ç®—ï¼Œé™ä½æ•ˆç‡ã€‚ 

**(3) æ›´é«˜æ•ˆçš„å†…å­˜è®¿é—®**
- é™åºæ’åˆ—åï¼ŒRNN çš„å†…å­˜è®¿é—®æ¨¡å¼æ›´è¿ç»­ï¼ˆ`memory-coalesced`ï¼‰ï¼Œå‡å°‘ GPU è®¿å­˜å»¶è¿Ÿã€‚  


#### 2. å¸¸ç”¨å‡½æ•°é›†å’Œè¿ç®— 
**torch.nn.functional** åŒ…å«äº†**ç»å¤§å¤šæ•°çš„å¸¸ç”¨å‡½æ•°**: ä¾‹å¦‚ reluï¼Œ softmax, sigmoid ç­‰ç­‰ã€‚

```python
def SoftmaxRegression(nn.Module):
	self.w = torch.normal(0,simga, size =(num_inputs, num_outpus), requires_grad=True)
	self.b = torch.zeros(num_outputs, requires_grad = True)
	self.optim = torch.optim.SGD(self.parameters(), lr = 0.01)
	
	 def parameters(self) : 
		return [self.w, self.b]
	 def forward(self):
		 return softmax(torch.matmul(X.reshape((-1,  self.W.shape[0])),  self.W) + self.b)
```

å¯¹äº minist æ•°æ®é›†, ç”±äºxçš„ç»´åº¦ä¸º 784, num_inputs = 784, num_outputs = 10. å³ w ä¸ºä¸€ä¸ª 784 x 10 çš„æƒé‡çŸ©é˜µã€‚

torch.gather(): å–æŸä¸ªtensorä¸­æŒ‡å®šä¸‹æ ‡çš„å…ƒç´ , ä½†æ˜¯éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•æŒ‡å®š:
```Python
For a 3-D tensor the output is specified by: 
out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
```
#### 3. å–æŸäº›è¡Œæˆ–è€…æŸäº›åˆ—çš„å…ƒç´ (torch.gather)
ä¾‹å¦‚gather(dim = 0) æ˜¯æŒ‰è¡Œå–å…ƒç´ , 
```python
a = tensor([[ 1,  2,  3,  4,  5], [ 6,  7,  8,  9, 10]])
a.gather(0, torch.tensor([[0, 1],[0,1]]))   # ç¬¬ä¸€åˆ—è¿›è¡Œgather, 
# tensor([[1, 7], [1, 7]]) 
a.gather(1, torch.tensor([[0, 1],[0,1]]))
```
å½“ dim = 0 æ—¶, å–æ³•ä¸ºå–æŒ‡å®šè¡Œç›¸åº”åˆ—çš„å…ƒç´ 
$$\left[\begin{matrix}
0  &|&1 \\  0&  |  & 1
\end{matrix}\right]$$
dim = 0 æ—¶, å–æ³•ä¸ºå–æŒ‡å®šåˆ—ç›¸åº”è¡Œçš„å…ƒç´ ã€‚ç¬¬ä¸€åˆ—å– 0 è¡Œ, 0è¡Œ, ç¬¬äºŒåˆ—å–1è¡Œ, 1è¡Œ; è€Œ
$$\left[\begin{matrix}
0  &1   \\   -  &  -  \\  0  & 1
\end{matrix}\right]$$
æ­¤æ—¶å¾—åˆ°çš„æ˜¯`[[1,2],[6,7]]`ã€‚ 
**è¯´æ˜: æœ€å¸¸è§çš„åº”ç”¨æ˜¯åœ¨äº¤å‰ç†µçš„è®¡ç®—ä¸­, è·å–åˆ°çš„æ¦‚ç‡çŸ©é˜µ y_pred(ä¾‹å¦‚784* 10), æ­£ç¡®ç›®æ ‡æ˜¯ç¦»æ•£çš„ç‚¹ y(0,1,2,3..9), æ­¤æ—¶éœ€è¦y_predä¸­,yæ‰€å¯¹åº”åˆ—çš„éƒ¨åˆ†**, å³å¯é‡‡ç”¨:
```python 
y_pred.gather(1, y.view(-1, 1)) 
```

ä½†å®é™…ä¸Šç”¨çš„å¤šçš„è¿˜æ˜¯å¦‚ä¸‹çš„æ–¹æ³•: 
```python
criterion =  nn.CrossEntropyLoss()
y_pred = model(X)
loss = criterion(y_pred, y)
```


## 3. åŸºæœ¬å›å½’æ¨¡å‹å’Œè®­ç»ƒæ­¥éª¤
### (1) åŸºæœ¬æ¨¡å‹è®­ç»ƒæ­¥éª¤ 
åœ¨ Pytorch ä¸­, æœ€åŸºæœ¬çš„éƒ¨åˆ†åŒ…æ‹¬å¦‚ä¸‹å‡ æ­¥(å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ”¥Pytorch Basics/Implements/ç®€å• SGD è®­ç»ƒ MINIST æ•°æ®é›†ç¤ºä¾‹|ç®€å• SGD è®­ç»ƒ MINIST æ•°æ®é›†ç¤ºä¾‹]]): 

å¯¹äºè®­ç»ƒæ•°æ®é›†, ä¸€èˆ¬éœ€è¦è½¬æ¢ä¸º 4 ç»´å¼ é‡ï¼Œ å¦‚ (5000, 3, 32, 32),  åˆ†åˆ«å¯¹åº” (N, C, W, H)  
Nï¼šè¿™ä¸€ç»´é€šå¸¸ä»£è¡¨æ‰¹é‡å¤§å° (batch size)ï¼Œå³ä¸€æ¬¡æ€§è¾“å…¥åˆ°ç½‘ç»œä¸­çš„æ ·æœ¬æ•°é‡ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä¸€æ¬¡è¾“å…¥ 5000 ä¸ªæ ·æœ¬ã€‚
Cï¼šè¿™ä¸€ç»´è¡¨ç¤ºè¾“å…¥é€šé“æ•°ã€‚åœ¨å›¾åƒå¤„ç†ä¸­ï¼Œè¾“å…¥é€šé“é€šå¸¸å¯¹åº”äºé¢œè‰²é€šé“ï¼Œ`3`Â è¡¨ç¤º RGB å›¾åƒï¼ˆçº¢ã€ç»¿ã€è“ä¸‰ç§é¢œè‰²)
W, Hï¼šè¿™ä¸¤ä¸ªç»´åº¦åˆ†åˆ«è¡¨ç¤ºå›¾åƒçš„é«˜åº¦å’Œå®½åº¦ã€‚ 
#### 1. å®šä¹‰ç½‘ç»œæ¨¡å‹ 
1. ç»§æ‰¿ç»§æ‰¿ `nn.Module`, å¹¶è°ƒç”¨ `super().init()` 
2. **å®šä¹‰å˜æ¢ (forward) å‡½æ•°**
ä¸€èˆ¬åœ¨ forward å‡½æ•°ä¸­å®šä¹‰æ¯ä¸€å±‚å˜æ¢,  è€Œ 
self.net ç­‰ä¸€èˆ¬ä¹Ÿåœ¨   forward ä¸­è¢«è°ƒç”¨, å³å¯ä»¥ä¸ä½¿ç”¨ net, è€Œé‡‡ç”¨ forward å¾—åˆ°ç»“æœ, å®é™…ä¸Šéƒ½æ˜¯ä¸€ä¸ªè¾“å…¥ X 
```python
class CIFIAR_CNN(nn.Module):  
    def __init__(self, learning_rate = 0.02):  
        super(CIFIAR_CNN, self).__init__()  
  
        self.net = nn.Sequential(  
            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # é¦–å…ˆ, ç”±äºæ˜¯ RGB æ•°æ®, è¾“å…¥é€šé“æ•°ä¸º 3, å°ºå¯¸ä¸º (batch_size, 3, 32, 32)            nn.Conv2d( 32, 64, kernel_size=3, padding=1), # å½¢æˆ 64 *32 * 32 çš„è¾“å‡º  
            # nn.MaxPool2d(kernel_size=(2,2), stride=2),  # n * 64 * 16 * 16, or use the following:  
            nn.AdaptiveMaxPool2d((16, 16)) ,                # n * 64 * 16 * 16 target size  
            nn.AvgPool2d(kernel_size=(2,2), stride=(2,2)),  # n * 64 * 8 * 8  
            nn.Flatten(1, -1),    # å°† 64 * 8 * 8 çš„å¼ é‡å±•å¹³  
            nn.Linear(64 * 8 * 8, 512),  
            nn.ReLU(),         # æ”¾åœ¨ Linear ä¹‹å, æ¿€æ´»å‡½æ•°, ç”¨äºå®ç°éçº¿æ€§å­¦ä¹ , å¾—åˆ°æ›´å¤æ‚çš„æ¨¡å‹  
            nn.Linear(512, 10),  # CIFAR-10 has 10 classes, so output is 10 dimensions  
            nn.Softmax(dim=1)  # Softmax æ¿€æ´»å‡½æ•°ï¼šå°†è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ  
        )  
  
    def forward(self,X):  
        return self.net(X)
```

#### 2. åŠ è½½æ•°æ®å’Œè®­ç»ƒ
1. **åˆå§‹å˜æ¢å’ŒåŠ è½½æ•°æ®é›†** 
æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ªå‡½æ•°æˆ–è€… `torchvision.Compose()` å®¹å™¨, ç„¶ååœ¨ transform ä¸­æŒ‡å®šå‚æ•°ï¼Œ ç„¶å
> [!NOTE] è¯´æ˜
> åœ¨åŠ è½½æ•°æ®å’Œè®­ç»ƒçš„ç±»ä¸­,  æˆ‘ä»¬ç›´æ¥å°†ç›¸åº”çš„ `self.model` æŒ‡å®šä¸ºåˆšåˆšå®šä¹‰çš„ç±»

**éœ€è¦è¯´æ˜**:  `transform` åªä¼šå½±å“åœ¨ `DataLoader` ä¸­æ‰¹é‡åŠ è½½æ•°æ®æ—¶çš„å›¾åƒã€‚å¦‚æœæ‚¨æƒ³**æŸ¥çœ‹ç»è¿‡ç¼©æ”¾å¤„ç†åçš„å›¾åƒï¼Œæ‚¨åº”è¯¥æŸ¥çœ‹ `train_loader` ä¸­çš„æ•°æ®ï¼Œè€Œä¸æ˜¯ `train_dataset.data`**ã€‚`train_loader` æ‰¹é‡åŠ è½½æ•°æ®æ—¶ï¼Œä¼šåº”ç”¨ `transform`ã€‚

```python
t = v2.Compose([  
    v2.ToDtype(torch.float32, scale=True),  
    v2.ToTensor(),  
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  
])  
train_dataset = CIFAR10( root='./cifar10', train=True, download=True, transform=t)
```

2. **å®šä¹‰ä¸€ä¸ª DataLoader è¿­ä»£å™¨**
```python
self.train_loader = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=True)
```

3. **å®šä¹‰æŸå¤±å‡½æ•° (criterion æˆ–è€… loss) å’Œä¼˜åŒ–å™¨(optim)** 
```python
def criterion(y_true, y_pred):
	return nn.CrossEntropyLoss(y_true, y_pred)
```

å¦‚æœåˆ†ç±»æ–¹æ³•ä¾èµ–äºæ¢¯åº¦, åˆ™éœ€è¦å®šä¹‰ optim ä¼˜åŒ–å™¨: 
`optim` çš„ä½œç”¨åŒ…æ‹¬:
- **æ¢¯åº¦æ›´æ–°**ï¼šåŸºäºæ¨¡å‹å‚æ•°çš„å½“å‰æ¢¯åº¦è°ƒæ•´å‚æ•°å€¼ã€‚
- **æ§åˆ¶å­¦ä¹ ç‡**ï¼šå­¦ä¹ ç‡ï¼ˆ`learning_rate`ï¼‰æ˜¯ä¼˜åŒ–å™¨çš„å…³é”®è¶…å‚æ•°ï¼Œå†³å®šæ›´æ–°æ­¥ä¼å¤§å°ã€‚
```python
self.optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # å¸¸ç”¨: SGD, Adam, RMSprop, AdaGrad ç­‰ç­‰, 
# å…¶ä¸­ momentum ä¸ºç”¨äºåŠ é€Ÿæ”¶æ•›çš„åŠ¨é‡è®¾ç½®é¡¹
```
éœ€è¦è¯´æ˜,  å¦‚æœä¸ä½¿ç”¨ optim ä¼˜åŒ–å™¨, å¯ä»¥æ‰‹åŠ¨è¿›è¡Œå‚æ•°æ›´æ–°:
```python
with torch.no_grad():  # ç¦æ­¢è®¡ç®—å›¾æ„å»ºï¼Œç›´æ¥æ›´æ–°å‚æ•°
    for param in model.parameters():
        param -= learning_rate * param.grad  # æ‰‹åŠ¨æ›´æ–°å‚æ•°
```

4. è¿›è¡Œè®­ç»ƒ:  
å…¶ä¸­ï¼Œ è°ƒç”¨ self.model.train è·å–åˆ°
```python
def train_model(self, max_epoch = 100):  
    self.model.train()    # åªéœ€è¦è°ƒç”¨ train() å‡½æ•°å³å¯  
    # åˆå§‹æ—¶æ¨¡å‹æ˜¯æœªè®­ç»ƒçš„æ¨¡å‹  
    for epoch in range(max_epoch):  
        train_loss = 0.0  
        for data, labels in self.train_loader:  
            self.optim.zero_grad()   # åœ¨æ¯æ¬¡å°æ‰¹é‡è®­ç»ƒæ—¶, éƒ½éœ€è¦æ¸…é›¶æ¢¯åº¦  
            loss = self.criterion(self.model(data), labels)  
            loss.backward()         # åå‘ä¼ æ’­  
            self.optim.step()       # åˆ©ç”¨åå‘ä¼ æ’­å¾—åˆ°çš„æ¢¯åº¦, è¿›è¡Œå‚æ•°æ›´æ–°  
            train_loss += loss.item() * data.size(0)    # è®¡ç®—æ€»å’Œçš„è¯¯å·®
```

MaxPool æ± åŒ–çš„å‚æ•°æŒ‡å®šæ–¹æ³•: 
```python
# target output size of 5x7
m = nn.AdaptiveMaxPool2d((5, 7))
input = torch.randn(1, 64, 8, 9)
output = m(input)
# target output size of 7x7 (square)
m = nn.AdaptiveMaxPool2d(7)
input = torch.randn(1, 64, 10, 9)
output = m(input)
# target output size of 10x7
m = nn.AdaptiveMaxPool2d((None, 7))
input = torch.randn(1, 64, 10, 9)
output = m(input)
```


### (1)å¤šé¡¹å¼å›å½’æ¨¡å‹
ä¸‹é¢æ›´è¿›ä¸€æ­¥å°è¯•ä¸€ä¸‹å¤šé¡¹å¼å›å½’ï¼Œä¸‹é¢æ˜¯å…³äº x çš„å¤šé¡¹å¼ï¼š
$$
\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 
$$
è¿™æ ·å°±èƒ½å¤Ÿæ‹Ÿåˆæ›´åŠ å¤æ‚çš„æ¨¡å‹ï¼Œè¿™é‡Œä½¿ç”¨äº† $x$ çš„æ›´é«˜æ¬¡ï¼ŒåŒç†è¿˜æœ‰å¤šå…ƒå›å½’æ¨¡å‹ï¼Œå½¢å¼ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯é™¤äº†ä½¿ç”¨ $x$ï¼Œè¿˜æ˜¯æ›´å¤šçš„å˜é‡ï¼Œæ¯”å¦‚ $y$ã€$z$ ç­‰ç­‰ï¼ŒåŒæ—¶ä»–ä»¬çš„ $loss$ å‡½æ•°å’Œç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹æ˜¯ä¸€è‡´çš„ã€‚ 
```python
import numpy as np  
import torch
import matplotlib.pyplot as plt
# å®šä¹‰ä¸€ä¸ªå¤šå˜é‡å‡½æ•°
w_target = np.array([0.5, 3, 2.4]) # å®šä¹‰å‚æ•°
b_target = np.array([0.9]) # å®šä¹‰å‚æ•°

f_des = 'y = {:.2f} + {:.2f} * x + {:.2f} * x^2 + {:.2f} * x^3'.format(
    b_target[0], w_target[0], w_target[1], w_target[2]) # æ‰“å°å‡ºå‡½æ•°çš„å¼å­

print(f_des)
```

æˆ‘ä»¬ä¸‹é¢ä»¥ `w0 + w1 + w2 + b = y_sample` çš„å¼å­, ä»¥ randn ä½œä¸º w åˆå§‹å€¼, 0 ä½œä¸ºb åˆå§‹å€¼è¿›è¡Œå°æ‰¹é‡æ¢¯åº¦ç®—æ³•æ±‚è§£è·å–ç»“æœ(å®šä¹‰çš„ä¸¤ä¸ªå‡½æ•°ä¸­, ä¸€ä¸ªæ˜¯æ¨¡å‹, ä¸€ä¸ªæ˜¯æŸå¤±å‡½æ•°): 
```python
import numpy as np  
import matplotlib.pyplot as plt  
import torch  
  
fig = plt.figure()  
epochNum = 80  
lr = 0.001  

x_sample = np.arange(-3, 3.1, 0.1)    # å®šä¹‰ç»˜ç”»æ•°ç»„åŒºé—´  
y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample ** 2 + w_target[2] * x_sample ** 3  
# plt.plot(x_sample,y_sample)  
# ----------------------------------------------------------  
# æ„å»ºæ•°æ® x å’Œ y# x æ˜¯ä¸€ä¸ªå¦‚ä¸‹çŸ©é˜µ [x, x^2, x^3]# y æ˜¯å‡½æ•°çš„ç»“æœ [y]x_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)  # æ„å»ºä»ç›¸åº”çš„æ•°æ®å–å¾—çš„è‡ªå˜é‡èŒƒå›´çŸ©é˜µ  
x_train = torch.from_numpy(x_train).float() # è½¬æ¢æˆ float tensory_train = torch.from_numpy(y_sample).float().unsqueeze(1) # è½¬åŒ–æˆ float tensor  
# å®šä¹‰å‚æ•° wå’Œb  
w = torch.randn((3, 1), dtype=torch.float, requires_grad=True)  # æ¯ä¸€æ¬¡runå¾—åˆ°çš„åˆå§‹fitæ›²çº¿ä¸åŒ  
b = torch.zeros((1), dtype=torch.float, requires_grad=True)  

# å®šä¹‰æ¨¡å‹
def multi_linear(x):  
    return torch.mm(x, w) + b   # æ³¨: mmå¯ä»¥ä½¿ç”¨matmulæ¥è¿›è¡Œä»£æ›¿  
  
def loss_func(y_, y):  # ä½¿ç”¨å¹³æ–¹å‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°  
    return torch.mean((y_ - y) ** 2)  
y_pred = multi_linear(x_train) # æ„å»ºå¤šé¡¹å¼å‡½æ•°  
  
ax1 = fig.add_subplot(121)  
ax1.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='curve_prefit', color='r')  
ax1.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b') 

# ç”»å‡ºæ›´æ–°ä¹‹å‰çš„æ¨¡å‹  
# ============ è®­ç»ƒæ•°æ®é›† ====================
for epoch in range (epochNum):  
    loss = loss_func(y_pred, y_train)  
    try:  
        w.grad.zero_()  
        b.grad.zero_()  
    except:  
        pass  
    loss.backward()  
    w.data = w.data - lr * w.grad.data  
    b.data = b.data - lr * b.grad.data  
    y_pred = multi_linear(x_train)  
  
ax2 = fig.add_subplot(122)  
ax2.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve-', color='r')  
ax2.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b')  
  
plt.legend()  
plt.show()
```

### (2) Logistic å›å½’æ¨¡å‹
å¯¹äºç»å…¸çš„$(0,1)$æ¨¡å‹ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
$$loss = -[y * log(\hat{y}) + (1-y) *log(1-y)]$$
```python
def logistic_regression(x):
    return torch.sigmoid(torch.mm(x, w) + b)
## mm: matmul

y_pred = logistic_regression(x_data)
# è®¡ç®—loss, ä½¿ç”¨clampçš„ç›®çš„æ˜¯é˜²æ­¢æ•°æ®è¿‡å°è€Œå¯¹ç»“æœäº§ç”Ÿè¾ƒå¤§å½±å“ã€‚
def binary_loss(y_pred, y):
    logits = ( y * y_pred.clamp(1e-12).log() + \
              (1 - y) * (1 - y_pred).clamp(1e-12).log() ).mean()
    return -logits
```
åªéœ€è¦å°†lossæ”¹ä¸ºè¿™ä¸ªå³å¯ã€‚

å…¶ä¸­ï¼Œpytorchæä¾›äº†ç›¸å…³çš„å‡½æ•°ï¼Œé‡è¦çš„æ˜¯ torch.optim.SGD å¯ä»¥ç›´æ¥é‡‡ç”¨å°æ‰¹é‡æ¢¯åº¦å›å½’, å¦å¤–æ¯ä¸€æ¬¡éœ€è¦è°ƒç”¨ optimizer.zero_grad()æ¸…é›¶ä¼˜åŒ–å™¨æ¢¯åº¦å³å¯ã€‚
```python title:æœ€ç®€å•çš„å°æ‰¹é‡æ¢¯åº¦ä¸‹é™å›å½’ä»£ç (å¤–éƒ¨å®šä¹‰æ¨¡å‹y_predå’ŒæŸå¤±binary_loss)
# ä½¿ç”¨ torch.optim æ›´æ–°å‚æ•°
from torch import nn
import time
# use the neural network model for training
# å®šä¹‰ä¼˜åŒ–å‚æ•°
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

# ä¼˜åŒ–å™¨çš„è®¾å®šä»¥åŠç›¸å…³çš„ä½¿ç”¨
optimizer = torch.optim.SGD([w, b], lr=0.1) # ä½¿ç”¨SGDç®—æ³•

for e in range(1000):
    # å‰å‘ä¼ æ’­
    y_pred = logistic_regression(x_data)  # è¿™ä¸ªy_predå’Œlossæ˜¯è‡ªå·±å®šä¹‰å‡½æ•°è®¡ç®—çš„
    loss = binary_loss(y_pred, y_data)      # è®¡ç®— loss
    # åå‘ä¼ æ’­
    optimizer.zero_grad() #-----ä½¿ç”¨ä¼˜åŒ–å™¨å°†æ¢¯åº¦å½’ 0
    loss.backward()
    optimizer.step() # *** åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæ›´æ–°å‚æ•°è¢«å°è£…ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨æ¥æ›´æ–°å‚æ•° **** 
    # è®¡ç®—æ­£ç¡®ç‡
    mask = y_pred.ge(0.5).float()
    acc = (mask == y_data).sum().item() / y_data.shape[0]
```


## ä¸‰ã€PyTorchæä¾›çš„æŸå¤±å‡½æ•° 
å‰é¢ä½¿ç”¨äº†è‡ªå·±å†™çš„ losså‡½æ•°ï¼Œå…¶å® PyTorch å·²ç»æä¾›äº†ä¸€äº›å¸¸è§çš„ losså‡½æ•°ï¼Œæ¯”å¦‚çº¿æ€§å›å½’é‡Œé¢çš„ loss æ˜¯ `nn.MSE()`ï¼Œè€Œ Logistic å›å½’çš„äºŒåˆ†ç±» loss åœ¨ PyTorch ä¸­æ˜¯ `nn.BCEWithLogitsLoss()`ï¼Œå…³äºæ›´å¤šçš„ lossï¼Œå¯ä»¥æŸ¥çœ‹[æ–‡æ¡£](https://pytorch.org/docs/stable/nn.html#loss-functions)

PyTorch å®ç°çš„ losså‡½æ•°æœ‰ä¸¤ä¸ªå¥½å¤„ï¼šç¬¬ä¸€æ˜¯æ–¹ä¾¿ä½¿ç”¨ï¼Œä¸éœ€è¦é‡å¤é€ è½®å­ï¼›ç¬¬äºŒå°±æ˜¯å…¶å®ç°æ˜¯åœ¨åº•å±‚ C++ è¯­è¨€ä¸Šçš„ï¼Œæ‰€ä»¥é€Ÿåº¦ä¸Šå’Œç¨³å®šæ€§ä¸Šéƒ½è¦æ¯”è‡ªå·±å®ç°çš„è¦å¥½ã€‚

å¦å¤–ï¼ŒPyTorch å‡ºäºç¨³å®šæ€§è€ƒè™‘ï¼Œå°†æ¨¡å‹çš„ Sigmoid æ“ä½œå’Œæœ€åçš„ loss éƒ½åˆåœ¨äº† `nn.BCEWithLogitsLoss()`ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ PyTorch è‡ªå¸¦çš„ loss å°±ä¸éœ€è¦å†åŠ ä¸Š Sigmoid æ“ä½œäº†
```python
# ä½¿ç”¨è‡ªå¸¦çš„loss
criterion = nn.BCEWithLogitsLoss() # å°† sigmoid å’Œ loss å†™åœ¨ä¸€å±‚ï¼Œæœ‰æ›´å¿«çš„é€Ÿåº¦ã€æ›´å¥½çš„ç¨³å®šæ€§
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

def logistic_reg(x):
    return torch.mm(x, w) + b
    
optimizer = torch.optim.SGD([w, b], 1.)

y_pred = logistic_reg(x_data)
loss = criterion(y_pred, y_data)  # è¿™æ ·å®šä¹‰æŸå¤±å‡½æ•°
# ä¹‹åè°ƒç”¨
for ......
	loss.backward()
	optimizer.step()_
print(loss.data)
```

å¸¸è§çš„æŸå¤±å‡½æ•°ï¼š
æµ…å±‚ï¼š LRï¼ŒSCM, ELm
æ·±å±‚ï¼šCNN ..




### (3) æŸå¤±å‡½æ•°ä¸“é¢˜ 
æŸå¤±å‡½æ•°å®é™…ä¸Šæ˜¯ä¸€ç§ç»éªŒé£é™©æœ€å°åŒ–çš„æ€è·¯ï¼Œ ä½†æ˜¯éœ€è¦æ³¨æ„[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#1. Basic Concept|ç»éªŒé£é™©å’Œå®é™…é£é™©çš„åŒºåˆ«]]. å¸¸è§çš„æŸå¤±å‡½æ•°æœ‰: 0-1æŸå¤±å‡½æ•°ï¼Œç»å¯¹å€¼æŸå¤±å‡½æ•°ï¼Œå¹³æ–¹æŸå¤±å‡½æ•°ï¼ŒLogå¯¹æ•°æŸå¤±å‡½æ•°ï¼ŒæŒ‡æ•°æŸå¤±å‡½æ•°ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ŒHingeæŸå¤±å‡½æ•°ç­‰ç­‰;

#### 1. MSE, MAE æŸå¤±å‡½æ•°
MSE : å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°(MSEï¼Œmean square error), ä¸€èˆ¬ç”¨äºå›å½’é—®é¢˜(most commonly used loss function for regression tasks.) 
$$L = \frac{1}{n} \sum^{n}_{i=1} (\hat{y}_i -y_i)^2$$
```python
nn.MSELoss()
```

**Mean Absolute Error (MAE)** 
$$\text{L1Loss} = \frac{1}{n} \sum_{i=1}^n |\hat{y}_i - y_i|$$
å…¶è°ƒç”¨æ–¹æ³•æ˜¯
```python
nn.L1Loss() 
```

**Huber Loss** 
å¯¹ Huber Loss å®é™…ä¸Šæ˜¯ MSE å’Œ MAE çš„ç»„åˆ,   å…·ä½“å…¬å¼å¦‚ä¸‹ : 
$$L_{\delta}(a) =
\begin{cases} 
\frac{1}{2}a^2 & \text{if } |a| \leq \delta, \\
\delta (|a| - \frac{1}{2} \delta) & \text{otherwise.}
\end{cases}$$
```python
nn.HuberLoss
``` 

#### 2. äº¤å‰ç†µæŸå¤±å‡½æ•° 
äº¤å‰ç†µæŸå¤±å‡½æ•° (å¸¸ç”¨äºåˆ†ç±»é—®é¢˜)
$$H(y^{(i)}, \hat{y}^{(i)}) = -\overset{q}{\underset{j = 1}{\sum}}y_j^{(i)}\log(y_j^{(i)})$$
å…¶ä¸­, $\hat{y}_j^{(i)}$é0å³1  ä¸ºåˆ†ç±»æ ‡ç­¾;

ä¸€èˆ¬åœ°, ä¸ºäº†é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œ å¾€å¾€ä¼šåŠ å…¥<b><mark style="background: transparent; color: blue">æ­£åˆ™åŒ–å‚æ•°</mark></b>ï¼Œ å®é™…ä¸Šæ˜¯æƒé‡æŸå¤±çš„æ¦‚å¿µ, å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/9. æ­£åˆ™åŒ–æ–¹æ³•, æ¦‚ç‡å›¾æ¨¡å‹å’Œè´å¶æ–¯ç½‘ç»œ|æ­£åˆ™åŒ–æ–¹æ³•, æ¦‚ç‡å›¾æ¨¡å‹å’Œè´å¶æ–¯ç½‘ç»œ]] éƒ¨åˆ†;

## å››ã€ç¥ç»ç½‘ç»œæ± åŒ–å±‚çš„æ¦‚å¿µç®€ä»‹
æ± åŒ–å±‚ä¹Ÿæ˜¯ä¸€ä¸ªç±»ä¼¼äºŒç»´çš„å±‚;

æ± åŒ–å±‚åœ¨CNNä¸­,ç”¨äºå‡å°‘ç©ºé—´ç»´æ•°è¿›è¡Œé™ç»´ä½¿ç”¨, å®é™…ä¸Šæ˜¯ä½¿ç”¨ä¸€ä¸ªfeature map, å…¶å®½é«˜åˆ†åˆ«ä¸º nh å’Œ nw, ç±»ä¼¼äºå·ç§¯è¿‡ç¨‹, åœ¨ map çš„æŒ‡å®šèŒƒå›´å†…è¿›è¡Œæ‰«æ; ç„¶åè¿‡æ»¤å‡ºç¬¦åˆè¦æ±‚çš„éƒ¨åˆ†ã€‚
ä¸€èˆ¬æ± åŒ–éœ€è¦æŒ‡å®š feature map å¤§å°å’Œ stride, ç¤ºä¾‹å¦‚ä¸‹: 
![[attachments/1_fhdK1xJND_m1Rdmr1m_9PQ.webp]]
ä¾‹å¦‚16x16çš„è¾“å…¥åœ¨2x2çª—, æ­¥è¿›ä¸º(2x2)æ± åŒ–åæˆä¸º 8x8 çš„ç‰¹å¾; å¸¸è§çš„æ± åŒ–æœ‰å‡å€¼æ± åŒ–å’Œæœ€å¤§å€¼æ± åŒ–ç­‰ç­‰; 


# å›¾åƒå¤„ç†ä¸“é¢˜
### (1) å›¾ç‰‡ç¼©æ”¾ä»£ç  (Pytorch, sklearn å’Œ  cv2) 
#### 1.  é€šè¿‡ skimage åº“è¿›è¡Œå›¾å½¢ç¼©æ”¾ 
ä¸€èˆ¬åœ°,  å¯¹äº 32 x 32 çš„å›¾åƒæ•°æ®é›†, è¾“å…¥æ˜¯ä¸€ä¸ª (50000, 32, 32, 3) çš„å‘é‡ , å¯ä»¥é‡‡ç”¨ `cv2.resize` æˆ–è€… skimage.transform.resize è¿›è¡Œæ‰¹é‡è°ƒæ•´
```sh
pip install scikit-image
```

ä»¥ä¸‹æ˜¯ skimage çš„
```python
import numpy as np
from skimage.transform import resize

# å‡è®¾è¾“å…¥çš„å›¾åƒæ•°æ®æ˜¯ (50000, 32, 32, 3)
images = np.random.rand(50000, 32, 32, 3)

# ä½¿ç”¨ resize æ‰¹é‡è°ƒæ•´å¤§å°
resized_images = np.zeros((images.shape[0], 28, 28, 3))  # åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„æ•°ç»„ï¼Œç›®æ ‡å°ºå¯¸æ˜¯ (50000, 28, 28, 3)
for i in range(images.shape[0]):
    resized_images[i] = resize(images[i], (28, 28), mode='reflect', preserve_range=True)

print(resized_images.shape)  # è¾“å‡º (50000, 28, 28, 3)
```
å¤„ç† å¤§æ‰¹é‡å›¾åƒæ—¶, ä¹Ÿå¯ä»¥é‡‡ç”¨
```python
import numpy as np
import cv2

# å‡è®¾è¾“å…¥çš„å›¾åƒæ•°æ®æ˜¯ (50000, 32, 32, 3)
images = np.random.rand(50000, 32, 32, 3)

# ä½¿ç”¨ cv2.resize æ‰¹é‡è°ƒæ•´å¤§å°
resized_images = np.zeros((images.shape[0], 28, 28, 3), dtype=np.float32)
for i in range(images.shape[0]):
    resized_images[i] = cv2.resize(images[i], (28, 28))
print(resized_images.shape)  # è¾“å‡º (50000, 28, 28, 3)
```

#### 2. å¯¹äº  Pytorch å°æ‰¹é‡è®­ç»ƒ
é¦–å…ˆæˆ‘ä»¬é€šè¿‡ `train_dataset = CIFAR10( root='./cifar10', train=True, download=True, transform=t)  ` å¾—åˆ°äº†å®é™…çš„æ•°æ®é›†,  è€Œå®é™…ä¸Š
`self.train_dataset.data.shape` æ˜¯ (50000, 32, 32, 3) å¤§å°çš„æ•°æ®é›†(åŒ…å« RGB æ•°æ®), è€Œå°†å…¶è½¬æ¢ä¸º 28 * 28 å›¾åƒåªéœ€ `transforms.Resize((28, 28)),  # å°†å›¾åƒç¼©æ”¾åˆ° 28x28`  


å…¶ä¸­, å‚è€ƒ [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#flatten) éƒ¨åˆ†, å¦‚æœå¾—åˆ°çš„è¾“å‡ºéœ€è¦å˜æ¢ä¸ºä¸€ç»´è¾“å‡ºï¼Œåˆ™å¯ä»¥åœ¨ net ä¸­æ’å…¥ä¸€ä¸ª nn.Flatten å±‚,  ä¼šå°†åé¢çš„ä¸‰ç»´å±•å¼€æˆä¸€ç»´, 
```python
input = torch.randn(32, 1, 5, 5)
# With default parameters
m = nn.Flatten()
output = m(input)
output.size()  # torch.Size([32, 25])
# With non-default parameters
m = nn.Flatten(start_dim =0, end_dim=2) # å°† 0-2 ç»´å±•å¹³å¾—åˆ° (32 * 1 * 5 , 5)çš„å½¢çŠ¶
output = m(input)
output.size() # (160, 5)
```


### (2) æ¿€æ´»å‡½æ•°ä¸“é¢˜
åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ `ReLU`, `Softmax` ç­‰ï¼‰é€šå¸¸ç”¨äºæ·»åŠ éçº¿æ€§å˜æ¢ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å¤æ‚çš„æ¨¡å¼ã€‚ä½ å¯ä»¥å°†æ¿€æ´»å‡½æ•°æ’å…¥åˆ° `nn.Linear` å±‚ä¹‹åï¼Œé€šå¸¸æ˜¯åœ¨è¾“å‡ºå±‚ä¹‹å‰æˆ–æŸäº›ä¸­é—´å±‚ä¹‹åè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚

å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°å¦‚ä¸‹:
#### 1. **ReLU æ¿€æ´»å‡½æ•°**
$$ \text{ReLU} (x) = \max(0,x)$$
![[Excalidraw/Pytorch & sklearnçš„ä½¿ç”¨åŸºç¡€å’ŒåŸºæœ¬ä»£ç  2024-12-21 12.24.08|250]]
`ReLU` æ˜¯å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸ç”¨äº **éšè—å±‚**ï¼Œå®ƒä¼šå¯¹è¾“å…¥è¿›è¡Œéçº¿æ€§è½¬æ¢ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œ`ReLU` æ”¾åœ¨ `nn.Linear` å±‚ä¹‹åã€‚
ä¾‹å¦‚ï¼Œåœ¨ `nn.Linear` ä¹‹åæ·»åŠ  `ReLU` æ¿€æ´»å‡½æ•°çš„å†™æ³•ï¼š

```python
self.net = nn.Sequential(
	nn.Conv2d(3, 32, kernel_size=3, padding=1),   # è¾“å…¥ï¼šbatch_size, 3, 32, 32
	nn.Conv2d(32, 64, kernel_size=3, padding=1),  # è¾“å‡ºï¼šbatch_size, 64, 32, 32
	nn.AdaptiveMaxPool2d((16, 16)),               # è¾“å‡ºï¼šbatch_size, 64, 16, 16
	nn.AvgPool2d(kernel_size=(2, 2), stride=2),   # è¾“å‡ºï¼šbatch_size, 64, 8, 8
	nn.Flatten(),                                 # å±•å¹³ï¼šbatch_size, 64 * 8 * 8
	nn.Linear(64 * 8 * 8, 512),                   # è¾“å…¥ï¼šbatch_size, 64 * 8 * 8ï¼Œè¾“å‡ºï¼šbatch_size, 512
	nn.ReLU(),                                   # ReLU æ¿€æ´»å‡½æ•°
	nn.Linear(512, 10)                            # è¾“å‡ºå±‚ï¼š10ä¸ªç±»åˆ«
)
```

- åœ¨ `nn.Linear(64 * 8 * 8, 512)` åæ’å…¥ `nn.ReLU()`ï¼Œå¸®åŠ©ç½‘ç»œå­¦ä¹ æ›´å¤æ‚çš„è¡¨ç¤ºã€‚

#### 2. **Sigmoid æ¿€æ´»å‡½æ•°**
å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter3 Linear Neural  Networks for regression(back Propagation)#(3) Sigmoid function|Sigmoid function]], æœ‰å…¬å¼å…³ç³»:
$$\text{Sigmoid}(x) =  \frac{1}{1 + e^{-x}}$$
ç”¨äºå°† wx + b éƒ¨åˆ†æŠ•å½±åˆ° (0,1) ä¹‹é—´, åŸºæœ¬å›¾åƒå¦‚ä¸‹: 
![[Excalidraw/Pytorch & sklearnçš„ä½¿ç”¨åŸºç¡€å’ŒåŸºæœ¬ä»£ç  2024-12-21 12.07.40|250]]
ç›´æ¥é‡‡ç”¨å¦‚ä¸‹è®¿é—®:
```python
torch.sigmoid(x);
```

**ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ Sigmoid**:

- **è¾“å‡ºå±‚**: å¦‚æœéœ€è¦æ¦‚ç‡è¾“å‡ºï¼ˆå¦‚äºŒåˆ†ç±»é—®é¢˜ï¼‰ï¼Œ`Sigmoid` é€šå¸¸ç”¨äºæœ€åä¸€å±‚ã€‚
- **ç‰¹å®šä»»åŠ¡**: åœ¨æŸäº›éœ€è¦ä¸¥æ ¼çº¦æŸè¾“å‡ºèŒƒå›´ä¸º `[0, 1]` çš„ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚ç”Ÿæˆå™¨ç½‘ç»œï¼ˆGANsï¼‰æˆ–å›¾åƒå¤„ç†ä»»åŠ¡ä¸­åƒç´ å€¼çš„å½’ä¸€åŒ–ã€‚

#### 3. tanh æ¿€æ´»å‡½æ•°
$$\tanh  (x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}  = 1 - \frac{2\exp (-2x)}{1+ \exp (-2x)}$$
å…¶å¯¼æ•°ä¸º:
$$\boxed{\tanh '(x) = 1 - \tanh^{2} x}$$

![[Excalidraw/Pytorch & sklearnçš„ä½¿ç”¨åŸºç¡€å’ŒåŸºæœ¬ä»£ç  2024-12-21 12.12.08|250]]
#### 4. **Softmax å‡½æ•°** 
softmax å®é™…ä¸Šä¸æ˜¯æ¿€æ´»å‡½æ•°, æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å‡½æ•°, é€šå¸¸æ”¾åœ¨ **è¾“å‡ºå±‚**ï¼Œç”¨äºå¤šåˆ†ç±»ä»»åŠ¡çš„æœ€ç»ˆè¾“å‡ºå±‚ï¼Œä»¥å°†ç½‘ç»œçš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚`Softmax` å°†æ¯ä¸ªç±»çš„è¾“å‡ºè½¬æ¢ä¸º `[0, 1]` ä¹‹é—´çš„æ¦‚ç‡ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰ç±»çš„æ¦‚ç‡å’Œä¸º 1(å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/9. softmaxå‡½æ•°å’Œäº¤å‰ç†µæŸå¤±CrossEntropy|softmaxå‡½æ•°å’Œäº¤å‰ç†µæŸå¤±]]éƒ¨åˆ†). 

åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­ï¼Œ`Softmax` ä¸»è¦ç”¨äº **æœ€åä¸€å±‚**ï¼Œç”¨äºè¾“å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚( å¦‚æœæ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œ åˆ™é‡‡ç”¨ `Sigmoid`ï¼Œæˆ–è€…å¤šæ ‡ç­¾é—®é¢˜)  
ç¤ºä¾‹ : 
ä½†æ˜¯éœ€è¦æ³¨æ„, å½“é‡‡ç”¨ CrossEntropyLoss æ—¶,  æˆ‘ä»¬ä¸éœ€è¦ä½¿ç”¨ Softmax, å› ä¸ºå®é™…ä¸Šå·²ç»æ–½åŠ äº† `log_softmax` éƒ¨åˆ†; 
```python
self.net = nn.Sequential(
	nn.Conv2d(3, 32, kernel_size=3, padding=1),   # è¾“å…¥ï¼šbatch_size, 3, 32, 32
	nn.Conv2d(32, 64, kernel_size=3, padding=1),  # è¾“å‡ºï¼šbatch_size, 64, 32, 32
	nn.AdaptiveMaxPool2d((16, 16)),               # è¾“å‡ºï¼šbatch_size, 64, 16, 16
	nn.AvgPool2d(kernel_size=(2, 2), stride=2),   # è¾“å‡ºï¼šbatch_size, 64, 8, 8
	nn.Flatten(),                                 # å±•å¹³ï¼šbatch_size, 64 * 8 * 8
	nn.Linear(64 * 8 * 8, 512),                   # è¾“å…¥ï¼šbatch_size, 64 * 8 * 8ï¼Œè¾“å‡ºï¼šbatch_size, 512
	nn.ReLU(),                                   # ReLU æ¿€æ´»å‡½æ•°
	nn.Linear(512, 10),                           # è¾“å‡ºå±‚ï¼š10ä¸ªç±»åˆ«
	nn.Softmax(dim=1)                             # Softmax æ¿€æ´»å‡½æ•°ï¼šå°†è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
)
```
- `Softmax` æ”¾åœ¨ `nn.Linear(512, 10)` åï¼Œé€šå¸¸ä¼šåœ¨è®­ç»ƒæ—¶ä½¿ç”¨äº¤å‰ç†µæŸå¤± (`nn.CrossEntropyLoss`)ï¼Œè¿™ä¸ªæŸå¤±å‡½æ•°ä¼šè‡ªåŠ¨åº”ç”¨ `Softmax`ã€‚
- **ReLU** é€šå¸¸åº”ç”¨äº **éšè—å±‚**ï¼Œä½äºæ¯ä¸ª `nn.Linear` åï¼Œç”¨äºå¼•å…¥éçº¿æ€§ã€‚
- **Softmax** é€šå¸¸åº”ç”¨äº **è¾“å‡ºå±‚**ï¼Œç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ï¼Œå°†ç½‘ç»œçš„è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚`Softmax` åº”æ”¾åœ¨æœ€åçš„è¾“å‡ºå±‚ä¹‹åã€‚

æ¿€æ´»å‡½æ•°çš„ä¸€èˆ¬è§„åˆ™:
- **éšè—å±‚ï¼š** `ReLU`, `LeakyReLU`, `Tanh`, `Sigmoid` ç­‰ï¼ˆæ ¹æ®ä»»åŠ¡å’Œéœ€æ±‚é€‰æ‹©ï¼‰ã€‚
- **è¾“å‡ºå±‚ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰ï¼š** `Softmax`ï¼ˆå¤šåˆ†ç±»é—®é¢˜ï¼‰æˆ– `Sigmoid`ï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼Œæˆ–è€…å¤šæ ‡ç­¾é—®é¢˜ï¼‰ã€‚

### 3.  Logarithmic MSE è¯¯å·® 
å¯¹äºæˆ¿ä»·ç­‰ç­‰è¾ƒå¤§çš„æ•°æ®, ä¸€èˆ¬æˆ‘ä»¬æ›´åŠ å…³å¿ƒçš„æ˜¯ç›¸å¯¹äºåŸå§‹ä»·æ ¼çš„å‡†ç¡®ç¨‹åº¦, 
$$\sqrt{\frac{1}{n} \sum_{i = 1}^{n} (\log_{} y_{i}  - \log_{} \hat{y}_{i})^{2}}$$
å…·ä½“å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter5 Essential Concepts & Heuristics#(1) Logarithmic MSE Loss|Logarithmic MSE Loss]] éƒ¨åˆ†

Log-Cosh è¯¯å·®, å¯ä»¥é‡‡ç”¨:
$$\text{Log-Cosh}(x) = \sum_{i=1}^{n} \log(\cosh(\hat{y}_{i} - y_{i}))$$

### éªŒè¯é›†å’ŒkæŠ˜äº¤å‰éªŒè¯
- æ–¹æ³•æ˜¯å°†è®­ç»ƒæ•°æ®é›†åˆ†å‰²æˆkä¸ªä¸åŒçš„å­æ•°æ®é›†ï¼Œæ¯ä¸€æ¬¡ä½¿ç”¨1ä¸ªæ•°æ®é›†éªŒè¯ï¼Œk-1ä¸ªè®­ç»ƒ 
- åškæ¬¡æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶å°†kæ¬¡çš„è®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®åˆ†åˆ«æ±‚å¹³å‡å€¼
å‚è€ƒ  [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter5 Essential Concepts & Heuristics#(2) K-fold Cross-Validation|K-fold Cross-Validation]] éƒ¨åˆ†.


### å®šä¹‰è®­ç»ƒçš„å±‚ 
éœ€è¦è¯´æ˜,  Pytorch  ä¸­, æ”¯æŒè‡ªå®šä¹‰ä¸åŒçš„å±‚, ä»ç„¶å¯ä»¥é‡‡ç”¨ nn.Module è¿›è¡Œå®šä¹‰; ä»ç„¶éœ€è¦ç»§æ‰¿ nn.Module ç±»å‹,  å¹¶ä¸”é‡å†™ forward æ–¹æ³•; 

forward å¿…é¡»é‡‡ç”¨ `def forward(self, X):`  ä¸ºå®šä¹‰, å¦‚æœæœ‰ forward, åˆ™å¯ä»¥è§†ä¸ºä¸€å±‚;

```python
import torch
import torch.nn as nn

class EpochDropout(nn.Module):
    def __init__(self, p=0.5):
        super(EpochDropout, self).__init__()
        self.p = p
        self.mask = None

    def forward(self, x):
        if self.training:
            # å¦‚æœ mask ä¸ºç©ºï¼Œåˆ™ç”Ÿæˆ mask
            if self.mask is None or self.mask.shape != x.shape:
                self.mask = torch.bernoulli(torch.full(x.shape, 1 - self.p)).to(x.device) / (1 - self.p)
            return x * self.mask
        else:
            return x  # æµ‹è¯•é˜¶æ®µä¸ä½¿ç”¨ Dropout

    def reset_mask(self):
        """åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶é‡ç½® mask"""
        self.mask = None
```

```python
self.dropout = EpochDropout(p=0.5)
# æˆ–è€…ç›´æ¥æ·»åŠ åˆ° net ä¸­ 
```

### (4) å¹³å‡å€¼å±‚ç®€ä»‹ 
1. Batch Norm  
Add `nn.BatchNorm2d` after convolutional layers to stabilize learning and improve convergence speed.

`nn.BatchNorm2d` æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»ç½‘ç»œå±‚normalizationæŠ€æœ¯,ä¸»è¦ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œä¸­ã€‚å®ƒé€šå¸¸è¢«æ·»åŠ åœ¨å·ç§¯å±‚ä¹‹å,èµ·åˆ°ä»¥ä¸‹ä½œç”¨:

1. **åŠ é€Ÿè®­ç»ƒæ”¶æ•›**:
   - é€šè¿‡æ ‡å‡†åŒ–è¾“å…¥æ•°æ®,å¯ä»¥æœ‰æ•ˆç¼“è§£å†…éƒ¨åå˜ç§»ç§»(internal covariate shift)çš„é—®é¢˜,åŠ å¿«æ¨¡å‹çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€‚

2. **æé«˜æ³›åŒ–èƒ½åŠ›**:
   - BatchNormèƒ½å¤Ÿä½¿å¾—æ¨¡å‹å¯¹è¾“å…¥æ•°æ®çš„åˆ†å¸ƒå˜åŒ–æ›´åŠ é²æ£’,ä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚

3. **é™ä½å¯¹åˆå§‹åŒ–çš„ä¾èµ–**:
   - ç”±äºBatchNormèƒ½å¤Ÿå¯¹æ•°æ®è¿›è¡ŒåŠ¨æ€æ ‡å‡†åŒ–,å› æ­¤æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å¯¹åˆå§‹åŒ–å‚æ•°çš„ä¾èµ–æ€§å¤§å¤§é™ä½ã€‚

4. **é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**:
   - BatchNormèƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é—®é¢˜,ä¸ºç½‘ç»œçš„è®­ç»ƒæä¾›æ›´åŠ ç¨³å®šçš„ç¯å¢ƒã€‚

**ä¸€èˆ¬æƒ…å†µä¸‹, `nn.BatchNorm2d` å±‚ä¼šè¢«æ·»åŠ åœ¨å·ç§¯å±‚å’Œæ¿€æ´»å‡½æ•°å±‚ä¹‹é—´, è¿™æ ·å¯ä»¥åœ¨ç‰¹å¾æå–å’Œç‰¹å¾å˜æ¢ä¹‹åå¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–**,ä»è€Œè·å¾—æ›´åŠ ç¨³å®šå’Œé²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚


## å¸¸ç”¨çš„å±‚éƒ¨åˆ†
nn.GroupNorm,  nn.BatchNorm1d, nn.BatchNorm2d  ç­‰ç­‰

**æ­¤å¤–, `nn.BatchNorm2d` è¿˜æœ‰ä¸€äº›å…¶ä»–çš„å˜ä½“,å¦‚ `nn.BatchNorm1d` ç”¨äºå…¨è¿æ¥å±‚, `nn.InstanceNorm2d` ç”¨äºå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡**ã€‚å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å…·ä½“é—®é¢˜å’Œç½‘ç»œç»“æ„æ¥é€‰æ‹©åˆé€‚çš„normalizationæŠ€æœ¯ã€‚

 `@torch.no_grad()` æä¾›äº†å°†æ•´ä¸ªå‡½æ•°ä¸è¿›è¡Œæ¢¯åº¦çš„æ§åˆ¶ä¿®é¥°å™¨ã€‚

ResNet 


æœ‰ä¸€äº›æ¯” `AdaptiveMaxPool2d` æ›´åŠ ä¼˜è¶Šçš„æ± åŒ–æ–¹æ³•,å…·ä½“å–å†³äºå®é™…é—®é¢˜å’Œæ¨¡å‹éœ€æ±‚ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯¹æ¯”å’Œå»ºè®®:

1. **GlobalAveragePooling2d**:
   - è¿™ç§æ–¹æ³•å°†ç‰¹å¾å›¾ä¸Šæ¯ä¸ªé€šé“çš„ç‰¹å¾å€¼å–å¹³å‡,å¾—åˆ°ä¸€ä¸ªå…¨å±€ç‰¹å¾å‘é‡ã€‚
   - ç›¸æ¯” `AdaptiveMaxPool2d`ï¼ŒGlobalAveragePooling2d èƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™ç‰¹å¾ä¿¡æ¯,ä¸ä¼šä¸¢å¤±é‡è¦çš„ç‰¹å¾ã€‚
   - åœ¨ä¸€äº›ä»»åŠ¡ä¸Š,å¦‚å›¾åƒåˆ†ç±»,GlobalAveragePooling2d è¡¨ç°æ›´ä¼˜ã€‚

2. **Spatial Pyramid Pooling (SPP)**:
   - SPP åˆ©ç”¨å¤šå°ºåº¦çš„æ± åŒ–æ“ä½œ,èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸åŒå°ºåº¦çš„ç‰¹å¾ä¿¡æ¯ã€‚
   - ç›¸æ¯”å›ºå®šå¤§å°çš„æ± åŒ–,SPP èƒ½å¤Ÿé€‚åº”ä¸åŒå¤§å°çš„è¾“å…¥,æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚
   - SPP æ¨¡å—å¯ä»¥ä½œä¸ºå·ç§¯ç½‘ç»œçš„ä¸€éƒ¨åˆ†,ä¸å…¶ä»–å±‚ä¸€èµ·ç«¯åˆ°ç«¯è®­ç»ƒã€‚

3. **Dilated Convolution**:
   - Dilated Convolution å¯ä»¥åœ¨ä¸é™ä½åˆ†è¾¨ç‡çš„æƒ…å†µä¸‹æ‰©å¤§æ„Ÿå—é‡,æ•è·æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
   - ç›¸æ¯”æ± åŒ–,Dilated Convolution ä¸ä¼šä¸¢å¤±ç‰¹å¾ä¿¡æ¯,èƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™åŸå§‹çš„è¯­ä¹‰ç‰¹å¾ã€‚

å…³äºæ‚¨æåˆ°çš„GPUè¶³å¤Ÿçš„æƒ…å†µä¸‹,æ˜¯å¦åº”è¯¥ä½¿ç”¨æ± åŒ–å±‚:

- å¦‚æœ GPU èµ„æºè¶³å¤Ÿå……è£•,ä¸€èˆ¬æ¥è¯´ä¸ä½¿ç”¨æ± åŒ–å±‚è®­ç»ƒæ•ˆæœä¼šæ›´å¥½ã€‚è¿™æ˜¯å› ä¸ºæ± åŒ–ä¼šä¸¢å¤±ä¸€äº›æœ‰ä»·å€¼çš„ç‰¹å¾ä¿¡æ¯ã€‚
- ä½†å¦‚æœGPUèµ„æºå—é™,ä½¿ç”¨æ± åŒ–å±‚å¯ä»¥æœ‰æ•ˆé™ä½è®¡ç®—é‡å’Œå†…å­˜å ç”¨,ä»è€Œèƒ½å¤Ÿè®­ç»ƒæ›´æ·±æ›´å¤§çš„æ¨¡å‹ã€‚è¿™ç§æƒ…å†µä¸‹,æ± åŒ–çš„ä¼˜åŠ¿å¯èƒ½ä¼šå¤§äºå…¶å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚

ç»¼åˆè€ƒè™‘,å¦‚æœGPUèµ„æºå……è¶³,å¯ä»¥å°è¯•ä½¿ç”¨ GlobalAveragePooling2d æˆ– Dilated Convolution ç­‰æ›´é«˜çº§çš„æ± åŒ–/æ„Ÿå—é‡æ‰©å¼ æŠ€æœ¯ã€‚åŒæ—¶ä¹Ÿè¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ¨¡å‹éœ€æ±‚è¿›è¡Œå®éªŒéªŒè¯,æ‰¾åˆ°æœ€åˆé€‚çš„æ–¹æ¡ˆã€‚


### ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ 
ä¸€èˆ¬å¯¹äºä»»æ„ nn.Module æ¨¡å—ï¼Œ å¯ä»¥é€šè¿‡å­˜å‚¨ state_dict è·å–å…¶ä¸­æ‰€æœ‰å‚æ•°ä¿¡æ¯ã€‚ 
```python
torch.save(self.model.state_dict(), 'cnn_models.pth')
```

ç„¶åé‡‡ç”¨ load_state_dict å¾—åˆ°åŸå§‹æ¨¡å‹å‚æ•°
```python
model = EasyModule()
model.load_state_dict(torch.load('cnn_models.pth')) 
```



##  æ¨¡å‹æ˜¾ç¤ºæ–¹æ³•: 
ä¸€èˆ¬å¯¹äº  nn.Module ç±»å‹ç»§æ‰¿çš„æ¨¡å‹ï¼Œ å¯ä»¥ç›´æ¥ä½¿ç”¨ 
`print(model)` æ‰“å°æ¨¡å‹è¾“å…¥å’Œè¾“å‡ºéƒ¨åˆ†ï¼Œ 

æˆ–è€… module.named_parameters() æ‰“å°
```python
for name, param in model.named_parameters():
    print(f"Layer: {name}, Size: {param.size()}")
```

æ­¤å¤–å¯ä»¥é‡‡ç”¨Â `torchsummary`  åº“è·å–æ›´åŠ è¯¦ç»†çš„æ¨¡å‹ç»“æ„. 
```python
from torchsummary import summary

summary(model, input_size=(10,))
```

### Lazy ç½‘ç»œ 

> [!HINT] ä¸ºä½•é‡‡ç”¨ Lazy ç½‘ç»œ
> å¸¸ç”¨çš„ Lazy ç½‘ç»œåŒ…æ‹¬ LazyLinear,  LazyConv2d ç­‰, å…¶è¾“å…¥æ•°é‡æ˜¯æ ¹æ®å‰ä¸€å±‚çš„è¾“å‡ºç¡®å®šçš„ï¼Œå› æ­¤å¯ä»¥çµæ´»å¤„ç†ä¸åŒå½¢çŠ¶çš„è¾“å…¥æ•°æ®ï¼Œ å› æ­¤èƒ½å¤Ÿæ›´å¥½åœ°è¿ç§»åˆ°ä¸åŒçš„æ•°æ®é›†ä¸Šæ¥  
