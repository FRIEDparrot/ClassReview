## 1. 数据集与加载
一般训练中， 我们<b><mark style="background: transparent; color: orange">将数据分为训练集， 验证集和测试集</mark></b> 

| 数据集            | 作用                                   | 是否参与训练     | 使用场景             |
| -------------- | ------------------------------------ | ---------- | ---------------- |
| **训练集（train）** | 用于 **模型参数学习**                        | ✅ 直接参与梯度更新 | 训练阶段             |
| **验证集（val）**   | 用于 **超参数调优、早停（Early Stopping）、模型选择** | ❌ 不参与训练    | 训练过程中定期评估        |
| **测试集（test）**  | 用于 **最终模型性能评估**（模拟真实场景）              | ❌ 绝不参与任何调整 | 训练完成后 **仅一次** 测试 |

在训练时， 必须调用一次 `model.train()`, 实际上是加入 dropout 来增加泛化能力，如果
在测试和验证中,  调用 `model.eval()` ,  

通过 `tqdm` 包可将训练过程封装为 progress bar, 可视化训练过程 ; 
```python 
from tqdm import tqdm 
import torch 
from torch.utils.data import DataLoader

data = torch.rand(100000, 10)
dataloader = DataLoader(data, batch_size = 20) 
progress_bar = tqdm(dataloader, desc="Training")   # 第一个是迭代器对象 
sum = 0
for batch in progress_bar:
	sum += batch.shape[0]
print(sum)
```

## 2. 基本求导方法
### (1) 反向传播算法求导
以下为基本的求导方式代码, 实际上仅需要 torch.tensor 中定义 `requires_grad = True` 之后, 计算并采用 backward 求导 :  
```python
m = torch.tensor([[2, 3]], dtype=torch.float, requires_grad=True) # 构建一个 1 x 2 的矩阵
# 注意：[[]]是定义行向量，而[]定义列向量
n = torch.zeros(1, 2) # 构建一个相同大小的 0 矩阵
print(m)
print(n)
# 通过 m 中的值计算新的 n 中的值
print(m[0,0])
n[0, 0] = m[0, 0] ** 2
n[0, 1] = m[0, 1] ** 3
print(n)

n.backward(torch.ones_like(n)) # 将 (w0, w1) 取成 (1, 1), 进行自动求导
print(m.grad)
# tensor([[ 4., 27.]], grad_fn=<CopySlices>)
```
将上面的式子写成数学公式，可以得到: 
$$
n = (n_0,\ n_1) = (m_0^2,\ m_1^3) = (2^2,\ 3^3) 
$$
下面我们直接对 `n` 进行反向传播，也就是求 `n` 对 `m` 的导数。
这时我们需要明确这个导数的定义，即如何定义
$$
\frac{\partial n}{\partial m} = \frac{\partial (n_0,\ n_1)}{\partial (m_0,\ m_1)}
$$
$$
\frac{\partial n}{\partial m_0} = w_0 \frac{\partial n_0}{\partial m_0} + w_1 \frac{\partial n_1}{\partial m_0} = 2 m_0 + 0 = 2 \times 2 = 4
$$
$$
\frac{\partial n}{\partial m_1} = w_0 \frac{\partial n_0}{\partial m_1} + w_1 \frac{\partial n_1}{\partial m_1} = 0 + 3 m_1^2 = 3 \times 3^2 = 27
$$

### (2) 多次自动求导的使用 
通过调用 backward 我们可以进行一次自动求导，<mark style="background: transparent; color: red">如果我们再调用一次 backward，会发现程序报错，没有办法再做一次。这是因为 PyTorch 默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置, 即设定 backward 方法中 retain_graph = True 保留计算图</mark> 
```python
x = torch.tensor([3], dtype=torch.float, requires_grad=True)
y = x * 2 + x ** 2 + 3
print(y)
y.backward(retain_graph=True) # 设置 retain_graph 为 True 来保留计算图
print(x.grad)
```

> [!CAUTION] 梯度归零
> 注意：如果是循环迭代求解梯度的情况下, 我们希望每一次计算并仅求解一次梯度，则求导完毕之后, 需要使用grad.data.zero_()来归零梯度，否则将会使梯度累加。具体如下: 
>```python
>while (1) : 
>	w.requires_grad_(True);  # ask for the space for gradient.  
>	b.requires_grad_(True);  #  
>	try:  
>	    w.grad.zero_()  
>	    b.grad.zero_()  
>	except:  
>	    pass
>```


## 3.  网络序列 nn.Sequential 
一般空网络采用如下方法定义:
```python
net = nn.Sequential(); 
net.add(nn.Dense(256, activation='relu'), nn.Dense(10))  
```
