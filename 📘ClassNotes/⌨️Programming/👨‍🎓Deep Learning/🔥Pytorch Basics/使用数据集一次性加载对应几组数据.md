
例如， 我们有一个矩阵$A$， 以及另外一个矩阵 $B$, 两矩阵的行是相同的， <mark style="background: transparent; color: red">一次性同时从 A 和 B  中加载 30 行作为训练</mark>。 

如在 `"en"`, `"zh"` 两数据列中，我们在训练 machine translation 时， 希望一次性取英文和中文的两列作为对应的
```python
df = pd.DataFrame(list(zip(word_indices_en, word_indices_zh)), columns=["en", "zh"])
```

此时，可以定义一个  Dataset 类型的数据集类型如下 :  
```python
class BilingualDataset(Dataset):  
    def __init__(self, df):  
        self.data = df  
        
    def __len__(self):  
        return len(self.data)  
  
    def __getitem__(self, idx):  
        en_sample = self.data.iloc[idx]["en"]  
        zh_sample = self.data.iloc[idx]["zh"]  
        if not isinstance(en_sample, torch.Tensor):  
            en_sample = torch.tensor(en_sample)  
        if not isinstance(zh_sample, torch.Tensor):  
            zh_sample = torch.tensor(zh_sample)  
        return {  
            "en": en_sample,  
            "zh": zh_sample,  
        }
```

此时可以采用如下方法进行加载和批量训练 : 
```python
dataloader = DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=shuffle,
    num_workers=2,  # 可以并行加载数据的进程数
    pin_memory=True  # 如果使用GPU，可以加速数据传输
) 

for batch in dataloader:
    en_batch = batch["en"]  # 形状为 [batch_size, ...] 的张量
    zh_batch = batch["zh"]  # 形状为 [batch_size, ...] 的张量
    # 在这里进行你的训练/评估操作 
```
1. 如果数据量很大，设置 `num_workers` 可以加速数据加载
2. 如果使用 GPU，设置 `pin_memory=True` 可以提高数据传输效率 

