在 `DotProductAttention` 中，**Window Mask** 是一种用于限制注意力范围的掩码（mask），通常用于控制<b><mark style="background: transparent; color: orange">自注意力机制中每个位置只能关注其邻近区域（即“窗口”内的位置），而不是所有位置。这种设计在长序列任务（如图像、视频、长文本）中尤为重要，能显著减少计算开销并捕捉局部依赖性</mark></b>。


### **Window Mask 的作用**
1. **限制注意力范围**  
   - 强制每个查询（query）位置仅与窗口内的键（key）位置计算注意力分数，忽略窗口外的位置。
   - 例如，在图像处理中，一个像素可能只需关注周围 \( k \times k \) 的局部区域，而非整张图像。

2. **降低计算复杂度**  
   - 标准自注意力的复杂度为 \( O(N^2) \)（\( N \) 是序列长度），而窗口注意力可降至 \( O(N \times k) \)（\( k \) 为窗口大小）。

3. **保持局部归纳偏置**  
   - 类似==卷积神经网络（CNN）的局部感受野，适合处理具有局部相关性的数据==（如图像、语音）。

### **Window Mask 的尺寸变化**
假设输入序列长度为 \( N \)，窗口大小为 \( k \)，则：

1. **标准注意力掩码（无窗口）**  
   - 尺寸：\( (N, N) \)（全连接注意力，每个位置可关注所有其他位置）。

2. **窗口掩码（滑动窗口）**  
   - 尺寸：仍为 \( (N, N) \)，但掩码是稀疏的（大部分为 `-inf`）。
   - 对于每个位置 \( i \)，仅允许关注 \( [i-k//2, i+k//2] \) 范围内的位置，其余位置掩码值为 `-inf`（softmax 后概率为 0）。

3. **分块窗口（Blocked Window）**  
   - 将序列分为若干不重叠的块（如 Swin Transformer），每个块内独立计算注意力。
   - 掩码尺寸：\( (M \times k, M \times k) \)（\( M \) 是块数，每块大小为 \( k \)）。

4. **动态尺寸变化**  
   - 某些模型（如 Longformer）支持动态窗口大小，掩码尺寸可能随输入长度变化。

---

### **具体示例**
假设序列长度 \( N=5 \)，窗口大小 \( k=3 \)：
- 标准注意力掩码（无限制）：
  ```python
  [[0, 0, 0, 0, 0],
   [0, 0, 0, 0, 0],
   [0, 0, 0, 0, 0],
   [0, 0, 0, 0, 0],
   [0, 0, 0, 0, 0]]
  ```
- 窗口掩码（中心对齐，两侧填充 `-inf`）：
  ```python
  [[0, 0, -inf, -inf, -inf],
   [0, 0, 0, -inf, -inf],
   [-inf, 0, 0, 0, -inf],
   [-inf, -inf, 0, 0, 0],
   [-inf, -inf, -inf, 0, 0]]
  ```

---

### **实现要点**
1. **掩码生成**  
   - 可通过 `torch.triu`（上三角）或滑动窗口逻辑生成。
   - 例如：
     ```python
     mask = torch.ones(N, N) * (-float('inf'))
     for i in range(N):
         left = max(0, i - k // 2)
         right = min(N, i + k // 2 + 1)
         mask[i, left:right] = 0
     ```

2. **应用方式**  
   - 在计算注意力分数后加上掩码（`scores = scores + mask`），再通过 softmax 归一化。

---

### **总结**
**Window Mask 通过稀疏化注意力矩阵，将计算限制在局部窗口内，从而平衡模型性能和效率**。其尺寸通常与输入序列长度相同（\( N \times N \)），但实际有效区域仅占 \( O(N \times k) \)。这种设计在 Transformer 变体（如 Swin Transformer、Longformer）中广泛应用。