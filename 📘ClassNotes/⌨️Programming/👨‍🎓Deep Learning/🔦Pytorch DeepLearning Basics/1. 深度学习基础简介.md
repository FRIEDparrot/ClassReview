DeepLearning ä¸»è¦çš„å±‚æ¬¡åŒ…å« <mark style="background: transparent; color: red">æ„ŸçŸ¥, æ¨ç†, çŸ¥è¯†, è§„åˆ’</mark> å››ä¸ªæ–¹é¢ã€‚ æœ€å¸¸ä½¿ç”¨çš„éƒ¨åˆ†åŒ…å«è‡ªç„¶è¯­è¨€å¤„ç†, è®¡ç®—æœºè§†è§‰(å›¾ç‰‡åˆ†ç±»å’Œæ£€æµ‹éƒ¨åˆ†)ç­‰ç­‰ã€‚

ä¸€èˆ¬å¯¹äºè¶…å¤§æ•°æ®é›†éƒ¨åˆ†, é‡‡ç”¨ Top5 å‡†ç¡®ç‡éƒ¨åˆ†. High Level -> ç›®æ ‡æ£€æµ‹; Low Level çš„ç›®æ ‡ åˆ†å‰²æ–¹å¼ã€‚æ ·å¼è¿ç§»ç­‰ç­‰ã€‚
![[Pasted image 20221121201818.png|500]]

## ä¸€ã€çº¿æ€§ä»£æ•°åŸºç¡€çŸ¥è¯†

$$||a \cdot  b || = \left| a\right| \cdot  || b||$$
$$c = a B $$
$$y =  Ax$$
å®é™…ä¸Šæ˜¯**ç©ºé—´çš„å˜å½¢**, å®é™…ä¸Šæ˜¯å‘é‡çš„æ•°é‡ç§¯ä½œä¸ºåŸºæœ¬å•å…ƒè¿›è¡Œç†è§£

ç”¨çš„è¾ƒå¤šçš„æ˜¯ çŸ©é˜µçš„[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/âœ–ï¸Matrix Theory/2. èŒƒæ•°ç†è®ºåŠå…¶åº”ç”¨#(1) çŸ©é˜µçš„ M1, M2 å’Œ F èŒƒæ•°|F èŒƒæ•°]], å¸¸ç”¨äºæ„é€  Loss å‡½æ•°
$$\Large \boxed{||A||_{F} = \left( \sum_{i = 1}^{m} \sum_{j = 1}^{n}  \left|a_{ij}^{2} \right|\right)^{\frac{1}{2}} =  \text{tr} (A^{H} A )^{\frac{1}{2}}}$$

æ­£å®šçŸ©é˜µ:
$$ x^{T }x \geq  0 \overset{æ¨å¹¿}{\longrightarrow}  x^{T } A  x \geq 0$$
æ—¶ä¸ºæ­£å®šäºŒæ¬¡å‹

æ­£äº¤çŸ©é˜µå®šä¹‰åŸºäºæ­£äº¤çš„åˆ—å‘é‡:
$$A^{T} A = 1$$
ç½®æ¢çŸ©é˜µ:[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/âœ–ï¸Matrix Theory/4. çŸ©é˜µåˆ†è§£#(2) Hermite æ ‡å‡†å‹æ–¹æ³•|ç½®æ¢çŸ©é˜µéƒ¨åˆ†]]
ç‰¹å¾å€¼ç­‰ç­‰ 

å¯¹äºå›¾ç‰‡(B, C,H, W); ä¸€ç»„çš„  RGB è§†é¢‘éƒ¨åˆ†, ä¸€èˆ¬é‡‡ç”¨ $batch\_size \times  time \times  R * G * B$

ä¸€èˆ¬åœ°, å¯¹äºå°–ç‚¹ä¸å¯å¯¼ç‚¹, æˆ‘ä»¬ä¸€èˆ¬å–å…¶å¯¼æ•°ä¸º<b><mark style="background: transparent; color: orange">äºšå¯¼æ•°</mark></b>, ä¾‹å¦‚ä»»å– $a$ åœ¨$[-1, 1]$ä¹‹é—´:
$$\frac{\partial |x|}{\partial x} = \begin{cases}
1   \\
-1  \\
a \qquad   x = 0, a \in [-1,1]
\end{cases}$$

é¦–å…ˆ, æ±‚å¯¼æ‰©å……åŒ…å«:
$$\frac{\partial y}{\partial \boldsymbol{x}}\qquad  \frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}$$
å…·ä½“å‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/âœ–ï¸Matrix Theory/3. çŸ©é˜µåºåˆ—ï¼ŒçŸ©é˜µçº§æ•°ä¸çŸ©é˜µå‡½æ•°#(1) çŸ©é˜µçš„å¾®åˆ†|çŸ©é˜µçš„å¾®åˆ†]]éƒ¨åˆ†
è§„å®šæ ‡é‡å¯¹äºåˆ—å‘é‡æ±‚å¯¼æ˜¯è¡Œå‘é‡(å³å°†å‘é‡æ‹‰åˆ°è¡Œä¸Š)ä¾‹å¦‚
$$x = \left[\begin{matrix}
x_{1} \\  x_{2}  \\  \dots  \\ x_{n}
\end{matrix}\right] \qquad  \frac{\partial y}{\partial \boldsymbol{x}} = \left[  \frac{\partial y}{\partial x_{1}}, \dots \frac{\partial y}{\partial x_{n}}\right]$$
å®é™…ä¸Šçš„å¯¹å‘é‡æ±‚å¯¼çš„æ„ä¹‰æ˜¯**å¢å¤§æœ€å¿«çš„æ–¹å‘** 

å¸¸è§è§„åˆ™:
$$\frac{\partial \sum x}{\partial  x} = 1^{T}\qquad  \frac{\partial ||x||^{2}}{\partial x} = 2 x^{T } $$
å¯¹äºåŠ å’Œå’Œä¹˜ç§¯éƒ¨åˆ†, ä»ç„¶æ»¡è¶³åŸºæœ¬è¿ç®—å¾‹:
$$\frac{\partial  <u,v>}{\partial x} = <\frac{\partial u}{\partial x},  v>+ <\frac{\partial v}{\partial x}, u>$$
å‘é‡å¯¹å‘é‡æ±‚å¯¼(ç±»ä¼¼), ç¬¦åˆå‡ ä¹æ‰€æœ‰çš„è¿ç®—è§„åˆ™;è€Œ**é“¾å¼æ³•åˆ™å§‹ç»ˆæˆç«‹**

å®é™…ä¸Šæ±‚å¯¼çš„å½¢çŠ¶ ä¸º $n_{1 } +n_{2}$ (æ ‡é‡å–0)å¦‚ $\frac{\partial X}{\partial Y}$ éƒ¨åˆ†æ˜¯4ç»´çš„ç»“æœã€‚


## äºŒã€åŸºæœ¬åå‘ä¼ æ’­ç®—æ³•åŸç†
### (1) æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
ä¾‹å¦‚: å– w, b ä¸ºå‚æ•°, $y \in  R$, $z = (<w,x> + b - y)$ $l =  \frac{1}{n}z^{2}$ , åˆ™å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter3 Linear Neural  Networks for regression(back Propagation)|regression]] éƒ¨åˆ†, åˆ™æœ‰:
$$\hat{y}= \omega^Tx+b$$
å–æŸå¤±çš„å¹³å‡å€¼
$$\boxed {L = \frac{1}{n} \sum^{n}_{i=1} l^{(i)}(w, b) = \frac{1}{n} \sum^{n}_{i=1} \frac{1}{2} \left( w^{T} x^{(i)} + b - y^{(i)}\right)^{2}}$$
é¦–å…ˆæˆ‘ä»¬å–<mark style="background: transparent; color: red">æ­£å‘ä¼ æ’­</mark>ä¸º:
$$\frac{\partial l}{\partial w} = \frac{\partial l}{\partial z_{n} } \left(\frac{\partial z_{n}}{\partial z_{n-1}} \left(\frac{\partial z_{2}}{\partial z_{1}} \frac{\partial z_{1}}{\partial x}\right) \right)$$
è€Œåå‘ä¼ æ’­åŸºæœ¬å…¬å¼: 
$$\frac{\partial l}{\partial w} =\left(\left(\left(\frac{\partial l}{\partial z_{n} } \frac{\partial z_{n}}{\partial z_{n-1}} \right)\dots \frac{\partial z_{2}}{\partial z_{1}}\right) \frac{\partial z_{1}}{\partial x}\right)$$
å…¶ä¸­å¾€å¾€å¯¹äº $\frac{\partial z_1}{\partial w}$, å¯ä»¥ç›´æ¥é‡‡ç”¨ $x$ ç›´æ¥è¯»å–ä»£æ›¿, å› æ­¤é‡‡ç”¨åå‘ä¼ æ’­å¯ä»¥å‡å°‘å˜é‡, åŒæ—¶
> [!NOTE] æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­åŒºåˆ«
> æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä»£ä»·ç±»ä¼¼, è€Œåå‘ä¼ æ’­è™½ç„¶å¢å¤§äº†å†…å­˜, ä½†æ˜¯å‡å°‘äº†å˜é‡ç­‰ç­‰

å–å¦‚ä¸‹çš„åå‘æ¢¯åº¦å‚æ•°
$$(w, b) \overset{}{\longrightarrow}  (w,b) - \frac{|\eta|}{|B|} \sum_{i \in B_{t}} \partial_{w,b}  l^{(i)}   (w, b)$$
å…¶ä¸­ $\partial(w, b)$ éƒ¨åˆ†çš„å¯¼æ•°ä¸º:
$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial w} = \frac{2}{n}(w^{T}x^{(i)}  + b - y_{i})x^{(i)}$$
$$\frac{\partial L}{\partial b} = \frac{\partial L}{\partial z}\frac{\partial z}{\partial b}  = \frac{2}{n}(w^{T}x^{(i)} + b - y^{(i)})$$
![[Excalidraw/æ·±åº¦å­¦ä¹ åŸºç¡€ç®€ä»‹ 2024-12-11 15.58.22|200]]

éœ€è¦è¯´æ˜çš„æ—¶, ä¸€èˆ¬å–
$$l^{(i)}(w,b) = \frac{1}{2}(\hat{y}^{(i)} -y^{(i)})^2$$
éœ€è¦è¯´æ˜çš„æ˜¯, æŸå¤±å‡½æ•°å¿…é¡»æ˜¯ä¸¥æ ¼çš„[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/4. æœ€ä¼˜åŒ–æ–¹æ³•, æ¢¯åº¦å¯»ä¼˜æ³•åŠLogistic å›å½’#(3) å‡¸å‡½æ•°åŠå…¶æ€§è´¨|å‡¸å‡½æ•°]]; è€Œå¾€å¾€å¯èƒ½<mark style="background: transparent; color: red">æ·±åº¦å­¦ä¹ èƒ½å¤Ÿæ‰¾åˆ°çš„æ˜¯å±€éƒ¨æœ€ä¼˜è§£, è€Œéå…¨å±€æœ€ä¼˜è§£</mark> 

### (2) æ‰¹é‡
ä¸€èˆ¬è¿­ä»£å…¬å¼ä¸º($\eta$ æ˜¯**é‡è¦çš„å­¦ä¹ ç‡å‚æ•°**):
$$w_{t} =w_{t-1} - \eta \frac{\partial l }{\partial w_{t - 1}}$$
å¯¹äºå°æ‰¹é‡ä¸‹é™, å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter3 Linear Neural  Networks for regression(back Propagation)#2. Minibatch Stochastic gradient descent(SGD)|SGD ç®—æ³•]], å¯ä»¥é‡‡ç”¨:
$$(w, b) \overset{}{\longrightarrow}  (w,b) - \frac{|\eta|}{|B|} \sum_{i \in B_{t}} \partial_{w,b}  l^{(i)}   (w, b)$$
å…¶ä¸­ $B$ ä¸ºå­¦ä¹ çš„æ‰¹é‡å¤§å°; (batch ç†è®ºä¸Šè¶Šå¤§è¶Šå¥½, ä½†æ˜¯**å—åˆ°æ˜¾å­˜çš„é™åˆ¶**, å¹¶æ ¹æ®æ˜¾å­˜è°ƒèŠ‚)

1. $\eta$ ä¸€èˆ¬é‡‡ç”¨è‡ªé€‚åº”æ–¹æ³•(å‰æœŸ $\eta$ è¾ƒå¤§, åæœŸ $\eta$ è¾ƒå°)

### æ¨¡å‹é€‰æ‹©ï¼Œæ¬ æ‹Ÿåˆï¼Œè¿‡æ‹Ÿåˆ
1. è®­ç»ƒè¯¯å·®å’Œæ³›åŒ–è¯¯å·®
2. æœ€å°åŒ–è®­ç»ƒè¯¯å·®å¹¶ä¸ä¸€å®šèƒ½å¤Ÿæœ€å°åŒ–æ³›åŒ–è¯¯å·®
3. æœºå™¨å­¦ä¹ åº”å½“æ³¨é‡é™ä½æ³›åŒ–è¯¯å·®

- æ¬ æ‹Ÿåˆ(underfitting)ï¼šæ¨¡å‹æ— æ³•å¾—åˆ°è¾ƒä½çš„è®­ç»ƒè¯¯å·®
- è¿‡æ‹Ÿåˆ(overfitting): æ¨¡å‹çš„è®­ç»ƒè¯¯å·®è¿œå°äºæµ‹è¯•æ•°æ®é›†ä¸Šçš„è¯¯å·®


## Pretrained Networks 
The main works are as follows : 
1. the introduction of tensor, and how the  model expect the tensor to be shaped. 
2. gradient descent method.  
3. fully connected  model for image classification  
4. end -to -end strategy  
5. metrics to  for identify the  weaknesses in training as 
6. deploy the python into c++ web program service 

- Popular Deeplearning FreameWork relationships : 
1. Theano and TensorFlow are low-level libs (which defines the computation graph)
2. Lasagne and Kreas are high-level wrappers  
3. Caffe2 as Pytorch's backend 
4. Also support for ONNX 
JAX -> autograd and JIT capable 

the **torch original lib are based on C++ librarys**. and this can be compiled to run with parallelism on GPUS. 

criterion (loss function) -> 

torch.nn.parallel and torch.distributed model can be employed. 

ONNX is a standard  format for neural network, and <mark style="background: transparent; color: red">Pytorch provides a  way to compile the  models through <b>Torch Script</b></mark>. 

most used modules are as follows : 
```python
import torch
from torch import nn
import torch.optim as optim 
import torch.nn.functional as F

class MyModule(torch.nn.Module):  
    """  
    Transform for converting video frames as a list of tensors.    
    """    
    def __init__(self):  
        super().__init__()
```

## 1. Using Pre-Trained Models 
Go to https://pytorch.org/hub/ to see more repos. 
We firstly use the video classification model for the 
If we want to load the model, just use : 
```python
import warnings
import torch  
import torchvision  
from typing import Dict  
import json  
import urllib  
from torchvision.transforms import Compose, Lambda  
from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo  
from pytorchvideo.data.encoded_video import EncodedVideo  
from pytorchvideo.transforms import  ApplyTransformToKey, ShortSideScale, UniformTemporalSubsample, UniformCropVideo  
  
model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)  
device = "cpu"  
model = model.eval().to(device)  
  
model = torch.nn.DataParallel(model)  
  
json_url = "https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json"  
json_filename = "kinetics_classnames.json"  
  
try:  
    urllib.URLopener().retrieve(json_url, json_filename)  
except:  
    warnings.warn("request failed, trying with urllib2 instead")  
    urllib.request.urlretrieve(json_url, json_filename)  
  
with open(json_filename, "r") as f:  
    kinetics_classnames = json.load(f)  
  
# Create an id to label name mapping  
kinetics_id_to_classname = {}  
for k, v in kinetics_classnames.items():  
    kinetics_id_to_classname[v] = str(k).replace('"', "")  
  
side_size = 256  
mean = [0.45, 0.45, 0.45]  
std = [0.225, 0.225, 0.225]  
crop_size = 256  
num_frames = 32  
sampling_rate = 2  
frames_per_second = 30  
slowfast_alpha = 4  
num_clips = 10  
num_crops = 3  
  s
class PackPathway(torch.nn.Module):  
    """  
    Transform for converting video frames as a list of tensors.    
    """    
    def __init__(self):  
        super().__init__()  
  
    def forward(self, frames: torch.Tensor):  
        fast_pathway = frames  
        # Perform temporal sampling from the fast pathway.  
        slow_pathway = torch.index_select(  
            frames,  
            1,  
            torch.linspace(  
                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha  
            ).long(),  
        )  
        frame_list = [slow_pathway, fast_pathway]  
        return frame_list  
  
transform = ApplyTransformToKey(  
    key="video",  
    transform=Compose(  
        [  
            UniformTemporalSubsample(num_frames),  
            Lambda(lambda x: x / 255.0),  
            NormalizeVideo(mean, std),  
            ShortSideScale(  
                size=side_size  
            ),  
            CenterCropVideo(crop_size),  
            PackPathway()  
        ]  
    ),  
)  
  
"""  
Load the video and transform it to the input format required by the model.  
"""  
# The duration of the input clip is also specific to the model.  
clip_duration = (num_frames * sampling_rate) / frames_per_second  
  
# download example video  
url_link = "https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4"  
video_path = 'archery.mp4'  
try:  
    urllib.URLopener().retrieve(url_link, video_path)  
except:  
    urllib.request.urlretrieve(url_link, video_path)  
  
# Select the duration of the clip to load by specifying the start and end duration  
# The start_sec should correspond to where the action occurs in the video  
start_sec = 0  
end_sec = start_sec + clip_duration  
  
# Initialize an EncodedVideo helper class and load the video  
video = EncodedVideo.from_path(video_path)  
  
# Load the desired clip  
video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)  
  
# Apply a transform to normalize the video input  
video_data = transform(video_data)  
  
# Move the inputs to the desired device  
inputs = video_data["video"]  
inputs = [i.to(device)[None, ...] for i in inputs]  
  
""" Get Predictions """  
# Pass the input clip through the model  
preds = model(inputs)  
  
# Get the predicted classes  
post_act = torch.nn.Softmax(dim=1)  
preds = post_act(preds)  
pred_classes = preds.topk(k=5).indices[0]  
  
# Map the predicted classes to the label names  
pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]  
print("Top 5 predicted labels: %s" % ", ".join(pred_class_names))
```

For the issue of `No module named 'torchvision.transforms.functional_tensor'`, referring to [this GitHub link](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/13985), modifying the source code from `import torchvision.transforms.functional_tensor as F_t` to `import torchvision.transforms.functional as F_t` allows the compilation to succeed.

another example is the Image recognition model **ImageNet Large Scale Recognition**, which is downloaded at https://image-net.org/ 

```
model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)
```

dataset is downloaded at https://www.kaggle.com/c/imagenet-object-localization-challenge, refer to [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ”¦Pytorch DeepLearning Basics/implements/Download Dataset by API from kaggle|Download Dataset]] which can download by:
![[attachments/Pasted image 20241009101028.png]]
and run the code in pycharm Termial

