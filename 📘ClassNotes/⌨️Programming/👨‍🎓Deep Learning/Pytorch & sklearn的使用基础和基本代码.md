# 1. Sklearn åº“çš„ä½¿ç”¨ 
sklearn æ˜¯æœ€å¸¸ç”¨çš„æœºå™¨å­¦ä¹ åº“, å…¶åŒ…å«åˆ†ç±», å›å½’ç®—æ³•, èšç±»ç®—æ³•, ç»´åº¦åŒ–ç®€, æ¨¡å‹é€‰æ‹©, äº¤å‰éªŒè¯, æ•°æ®é¢„å¤„ç†ç­‰ç­‰åŠŸèƒ½. ä¾‹å¦‚å²­å›å½’ï¼Œ æ”¯æŒå‘é‡æœº, KNN , æœ´ç´ è´å¶æ–¯, å†³ç­–æ ‘, ç‰¹å¾é€‰æ‹©, ä¿åºå›å½’ç­‰ç­‰ç®—æ³•ã€‚
### (1) sklearn ä¸­çš„ Bunch ç±»
Bunch æ˜¯ sklearn ä¸­æœ€å¸¸ç”¨çš„ç»“æ„, ç±»ä¼¼äºå­—å…¸ï¼Œå…·ä½“å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸPython/2. Python åŸºæœ¬æ•°æ®ç»“æ„å’Œå¯è§†åŒ–æ–¹æ³•|2. Python åŸºæœ¬æ•°æ®ç»“æ„å’Œå¯è§†åŒ–æ–¹æ³•]] 
```python 
from sklearn.datasets._base import Bunch
```
ä¸€èˆ¬åœ°, å¯ä»¥é€šè¿‡
```python
print(bunch.keys())
```
è·å–å…¶ä¸­çš„keysé”®, å¯¹äºbunchçš„keyséƒ¨åˆ†å‡å¯ä»¥é€šè¿‡ `.` è¿›è¡Œè®¿é—®ã€‚

> [!CAUTION] scipy ä¸­çš„ toarray æ–¹æ³•
> éœ€è¦æ³¨æ„çš„æ˜¯, scipy.spare.toarray() å’Œ numpy.array å¹¶ä¸ç›¸é€š, ç¨€ç–çŸ©é˜µå¯ä»¥é€šè¿‡ toarray() æ–¹æ³•è¿›è¡Œè½¬æ¢ä¸ºæ ‡å‡†çš„ numpy æ•°ç»„


### (2) é’ˆå¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¸¸ç”¨éƒ¨åˆ†
sklearn å¯ä»¥ç›´æ¥è·å–å¤§é‡çš„å­¦ä¹ æ•°æ®é›†ï¼Œ åŒæ—¶æœ‰åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†åŠŸèƒ½
```python 
from sklearn.dataset import load_iris 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler   # æ ‡å‡†åŒ–å™¨ç±»å‹ 

X_train, X_test, y_train, y_test =  train_test_split(iris_data, iris_target, test_size=0.2, random_state=None)  
```

å¯¹äºæµ‹è¯•å‡†ç¡®åº¦, å¯ä»¥é‡‡ç”¨ [sklearn.metrics æ¨¡å—](https://scikit-learn.org/stable/api/sklearn.metrics.html)éƒ¨åˆ†: 
```python
from sklearn.metrics import accuracy_score, classification_report 
from sklearn.metrics import precision score, recall_score 
from sklearn.metrics import f1_score  # F1æ ‡å‡†, å‚è€ƒsklearn éƒ¨åˆ†
```

[`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score "sklearn.metrics.accuracy_score") ,[`average_precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score "sklearn.metrics.average_precision_score"), [`balanced_accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score "sklearn.metrics.balanced_accuracy_score"), [`brier_score_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss "sklearn.metrics.brier_score_loss") , 

æ­¤å¤–, æµ‹è¯• MSE, MAE ä¹Ÿå¯ä»¥é‡‡ç”¨ sklearn,metrics çš„å¦‚ä¸‹åŒ…è¿›è¡Œè·å–:

| [`mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error "sklearn.metrics.mean_squared_error")                 | Mean squared error regression loss.(MSE)        |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
| [`mean_squared_log_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error "sklearn.metrics.mean_squared_log_error") | Mean squared logarithmic error regression loss. |
| [`mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error "sklearn.metrics.mean_absolute_error")             | Mean absolute error regression loss.(MAE)       |

sklearn æ ‡ç­¾ç¼–ç : ä¾‹å¦‚å¯¹äºå¦‚ä¸‹å†³ç­–æ ‘è¡¨æ ¼, å…¶ä¸­ç¬¬ä¸€åˆ—ä¸éœ€è¦ç¼–ç :
![[attachments/Pasted image 20240912111152.png]]
å…¶ä¸­é‡‡ç”¨ apply_map çš„æ–¹æ³•, å»é™¤äº†æ¯ä¸ªå­—ç¬¦ä¸²å…ƒç´ çš„ç©ºæ ¼; åŒæ—¶é‡‡ç”¨stripå»é™¤äº†æ¯ä¸ªæ ‡ç­¾çš„ç©ºæ ¼ã€‚  
```python
import pandas as pd  
from sklearn.preprocessing import LabelEncoder  # encoder labels  
  
data_raw = pd.DataFrame(pd.read_excel("seals_data.xlsx"))  
data_proceed = pd.DataFrame()  
label_encoder = LabelEncoder()  
  
# eliminate all the white space  
data_raw = data_raw.applymap(lambda x: x.strip() if isinstance(x, str) else x)  
  
data_proceed['è®¡æ•°'] = data_raw.iloc[:,0]  
  
for column in data_raw.columns[1:]:  
    data_proceed[str(column).strip()] = label_encoder.fit_transform(data_raw[column])  
print(data_proceed)
```
å¾—åˆ°å†³ç­–æ ‘ç¼–ç ç»“æœå¦‚ä¸‹:
![[attachments/Pasted image 20240912113356.png]]

### (3) ç‰¹å¾æå–å’Œåˆ†ç±» 
```python
# ç‰¹å¾é€‰æ‹©å’Œç‰¹å¾æå–éƒ¨åˆ†
from sklearn import feature_extraction
from sklearn import feature_selection 
```
å…¶ä¸­ç‰¹å¾æå–ä¸­æœ‰ image, text ç­‰ç­‰ã€‚
![[attachments/Pasted image 20240910160939.png]]
å¯¹äºå¤šæ ‡ç­¾åˆ†ç±»éƒ¨åˆ†, éœ€è¦é‡‡ç”¨å¤šè¾“å‡º
```python
from sklearn.multioutput import MultiOutputClassifier, MultiOutputEstimator
# å°†ä¸€ä¸ªéƒ¨åˆ†è½¬æ¢ä¸ºå¤šè¾“å‡º:
model = MultiOutputClassifier(MultinomialNB(alpha=0.01))
```

ä¸»æˆåˆ†åˆ†æ:
```python 
from sklearn.decomposition import PCA
```

```python 
from sklearn.inspection import DecisionBoundaryDisplay 
```


# 2. Pytorch
## ä¸€ã€åŸºæœ¬æ•°æ®æ“ä½œ
#### 1. unsqueeze,squeezeå’Œreshapeä½¿ç”¨
å¢åŠ ç»´åº¦æˆ–è€…å‡å°‘ç»´åº¦: 
torch.squeeze(input,dim=0) **ç§»é™¤æŸä¸ªç»´åº¦**
torch.unsqueeze(input,dim=0) **åœ¨æŸä¸ªç»´åº¦ä¸Šå¢åŠ ç»´åº¦** 
reshape çš„ä½¿ç”¨: åœ¨ pytorch ç­‰ä¸­, æˆ‘ä»¬å¸¸å¸¸ä½¿ç”¨ reshape å°†æ•°æ®é›†è½¬æ¢ä¸ºæŒ‡å®šçš„å½¢çŠ¶, ä¾‹å¦‚å¯¹äº (60000, 28,28) çš„æ•°æ®é›†, æˆ‘ä»¬ä¼šä½¿ç”¨ `x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)` å¾—åˆ° (60000,28,28,1) çš„4dæ•°æ®é›†, å³å°†æœ€åçš„28ä¸ªä¸­, æ¯ä¸ªå‡ä½œä¸ºè¾“å…¥æ‹†åˆ†å¼€;

```python title:squeezeä½¿ç”¨ç¤ºä¾‹
train[1].unsqueeze(0).shape
Out[12]: torch.Size([1, 28, 28])
A = train[1].unsqueeze(0).unsqueeze(0)
A.shape
Out[13]: torch.Size([1, 1, 28, 28]) 
A.squeeze(0).shape
Out[17]: torch.Size([1, 28, 28])
A.squeeze(0).squeeze(0).shape
Out[19]: torch.Size([28, 28])
A.squeeze([0,1]).shape
Out[22]: torch.Size([28, 28])
B.unsqueeze(dim=2).shape
Out[28]: torch.Size([28, 28, 1])
```

å¦å¤–ï¼Œpytorchä¸­å¤§å¤šæ•°çš„æ“ä½œéƒ½æ”¯æŒÂ `inplace`Â æ“ä½œï¼Œä¹Ÿå°±æ˜¯å¯ä»¥ç›´æ¥å¯¹ tensor è¿›è¡Œæ“ä½œè€Œä¸éœ€è¦å¦å¤–å¼€è¾Ÿå†…å­˜ç©ºé—´ï¼Œæ–¹å¼éå¸¸ç®€å•ï¼Œä¸€èˆ¬éƒ½æ˜¯åœ¨æ“ä½œçš„ç¬¦å·åé¢åŠ `_` 

#### 2. å¸¸ç”¨å‡½æ•°é›†å’Œè¿ç®— 
**torch.nn.functional** åŒ…å«äº†**ç»å¤§å¤šæ•°çš„å¸¸ç”¨å‡½æ•°**: ä¾‹å¦‚ reluï¼Œ softmax, sigmoid ç­‰ç­‰ã€‚

```python
def SoftmaxRegression(nn.Module):
	self.w = torch.normal(0,simga, size =(num_inputs, num_outpus), requires_grad=True)
	self.b = torch.zeros(num_outputs, requires_grad = True)
	self.optim = torch.optim.SGD(self.parameters(), lr = 0.01)
	
	 def parameters(self) : 
		return [self.w, self.b]
	 def forward(self):
		 return softmax(torch.matmul(X.reshape((-1,  self.W.shape[0])),  self.W) + self.b)
```

å¯¹äº minist æ•°æ®é›†, ç”±äºxçš„ç»´åº¦ä¸º 784, num_inputs = 784, num_outputs = 10. å³ w ä¸ºä¸€ä¸ª 784 x 10 çš„æƒé‡çŸ©é˜µã€‚

torch.gather(): å–æŸä¸ªtensorä¸­æŒ‡å®šä¸‹æ ‡çš„å…ƒç´ , ä½†æ˜¯éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•æŒ‡å®š:
```Python
For a 3-D tensor the output is specified by: 
out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
```

#### 3. å–æŸäº›è¡Œæˆ–è€…æŸäº›åˆ—çš„å…ƒç´ (torch.gather)
ä¾‹å¦‚gather(dim = 0) æ˜¯æŒ‰è¡Œå–å…ƒç´ , 
```python
a = tensor([[ 1,  2,  3,  4,  5], [ 6,  7,  8,  9, 10]])
a.gather(0, torch.tensor([[0, 1],[0,1]]))   # ç¬¬ä¸€åˆ—è¿›è¡Œgather, 
# tensor([[1, 7], [1, 7]]) 
a.gather(1, torch.tensor([[0, 1],[0,1]]))
```
å½“ dim = 0 æ—¶, å–æ³•ä¸ºå–æŒ‡å®šè¡Œç›¸åº”åˆ—çš„å…ƒç´ 
$$\left[\begin{matrix}
0  &|&1 \\  0&  |  & 1
\end{matrix}\right]$$
dim = 0 æ—¶, å–æ³•ä¸ºå–æŒ‡å®šåˆ—ç›¸åº”è¡Œçš„å…ƒç´ ã€‚ç¬¬ä¸€åˆ—å– 0 è¡Œ, 0è¡Œ, ç¬¬äºŒåˆ—å–1è¡Œ, 1è¡Œ; è€Œ
$$\left[\begin{matrix}
0  &1   \\   -  &  -  \\  0  & 1
\end{matrix}\right]$$
æ­¤æ—¶å¾—åˆ°çš„æ˜¯`[[1,2],[6,7]]`ã€‚ 
**è¯´æ˜: æœ€å¸¸è§çš„åº”ç”¨æ˜¯åœ¨äº¤å‰ç†µçš„è®¡ç®—ä¸­, è·å–åˆ°çš„æ¦‚ç‡çŸ©é˜µ y_pred(ä¾‹å¦‚784* 10), æ­£ç¡®ç›®æ ‡æ˜¯ç¦»æ•£çš„ç‚¹ y(0,1,2,3..9), æ­¤æ—¶éœ€è¦y_predä¸­,yæ‰€å¯¹åº”åˆ—çš„éƒ¨åˆ†**, å³å¯é‡‡ç”¨:
```python 
y_pred.gather(1, y.view(-1, 1)) 
```

ä½†å®é™…ä¸Šç”¨çš„å¤šçš„è¿˜æ˜¯å¦‚ä¸‹çš„æ–¹æ³•: 
```python
criterion =  nn.CrossEntropyLoss()
y_pred = model(X)
loss = criterion(y_pred, y)
```


### (2) åŸºæœ¬æ±‚å¯¼æ–¹æ³•
#### 1. åå‘ä¼ æ’­ç®—æ³•æ±‚å¯¼
```python
m = torch.tensor([[2, 3]], dtype=torch.float, requires_grad=True) # æ„å»ºä¸€ä¸ª 1 x 2 çš„çŸ©é˜µ
# æ³¨æ„ï¼š[[]]æ˜¯å®šä¹‰è¡Œå‘é‡ï¼Œè€Œ[]å®šä¹‰åˆ—å‘é‡
n = torch.zeros(1, 2) # æ„å»ºä¸€ä¸ªç›¸åŒå¤§å°çš„ 0 çŸ©é˜µ
print(m)
print(n)
# é€šè¿‡ m ä¸­çš„å€¼è®¡ç®—æ–°çš„ n ä¸­çš„å€¼
print(m[0,0])
n[0, 0] = m[0, 0] ** 2
n[0, 1] = m[0, 1] ** 3
print(n)

n.backward(torch.ones_like(n)) # å°† (w0, w1) å–æˆ (1, 1), è¿›è¡Œè‡ªåŠ¨æ±‚å¯¼
print(m.grad)
# tensor([[ 4., 27.]], grad_fn=<CopySlices>)
```
å°†ä¸Šé¢çš„å¼å­å†™æˆæ•°å­¦å…¬å¼ï¼Œå¯ä»¥å¾—åˆ°: 
$$
n = (n_0,\ n_1) = (m_0^2,\ m_1^3) = (2^2,\ 3^3) 
$$
ä¸‹é¢æˆ‘ä»¬ç›´æ¥å¯¹ `n` è¿›è¡Œåå‘ä¼ æ’­ï¼Œä¹Ÿå°±æ˜¯æ±‚ `n` å¯¹ `m` çš„å¯¼æ•°ã€‚
è¿™æ—¶æˆ‘ä»¬éœ€è¦æ˜ç¡®è¿™ä¸ªå¯¼æ•°çš„å®šä¹‰ï¼Œå³å¦‚ä½•å®šä¹‰
$$
\frac{\partial n}{\partial m} = \frac{\partial (n_0,\ n_1)}{\partial (m_0,\ m_1)}
$$
$$
\frac{\partial n}{\partial m_0} = w_0 \frac{\partial n_0}{\partial m_0} + w_1 \frac{\partial n_1}{\partial m_0} = 2 m_0 + 0 = 2 \times 2 = 4
$$
$$
\frac{\partial n}{\partial m_1} = w_0 \frac{\partial n_0}{\partial m_1} + w_1 \frac{\partial n_1}{\partial m_1} = 0 + 3 m_1^2 = 3 \times 3^2 = 27
$$

#### 2. å¤šæ¬¡è‡ªåŠ¨æ±‚å¯¼
é€šè¿‡è°ƒç”¨ backward æˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€æ¬¡è‡ªåŠ¨æ±‚å¯¼ï¼Œå¦‚æœæˆ‘ä»¬å†è°ƒç”¨ä¸€æ¬¡ backwardï¼Œä¼šå‘ç°ç¨‹åºæŠ¥é”™ï¼Œæ²¡æœ‰åŠæ³•å†åšä¸€æ¬¡ã€‚è¿™æ˜¯å› ä¸º PyTorch é»˜è®¤åšå®Œä¸€æ¬¡è‡ªåŠ¨æ±‚å¯¼ä¹‹åï¼Œè®¡ç®—å›¾å°±è¢«ä¸¢å¼ƒäº†ï¼Œæ‰€ä»¥ä¸¤æ¬¡è‡ªåŠ¨æ±‚å¯¼éœ€è¦æ‰‹åŠ¨è®¾ç½®
```python
x = torch.tensor([3], dtype=torch.float, requires_grad=True)
y = x * 2 + x ** 2 + 3
print(y)
y.backward(retain_graph=True) # è®¾ç½® retain_graph ä¸º True æ¥ä¿ç•™è®¡ç®—å›¾
print(x.grad)
```

> [!CAUTION] æ¢¯åº¦å½’é›¶
> æ³¨æ„ï¼šå¦‚æœæ˜¯å¾ªç¯è¿­ä»£æ±‚è§£æ¢¯åº¦çš„æƒ…å†µä¸‹, æˆ‘ä»¬å¸Œæœ›æ¯ä¸€æ¬¡è®¡ç®—å¹¶ä»…æ±‚è§£ä¸€æ¬¡æ¢¯åº¦ï¼Œåˆ™æ±‚å¯¼å®Œæ¯•ä¹‹å, éœ€è¦ä½¿ç”¨grad.data.zero_()æ¥å½’é›¶æ¢¯åº¦ï¼Œå¦åˆ™å°†ä¼šä½¿æ¢¯åº¦ç´¯åŠ ã€‚

```python
while (1) : 
	w.requires_grad_(True);  # ask for the space for gradient.  
	b.requires_grad_(True);  #  
	try:  
	    w.grad.zero_()  
	    b.grad.zero_()  
	except:  
	    pass
```

## äºŒã€åŸºæœ¬å›å½’æ¨¡å‹
### (1)å¤šé¡¹å¼å›å½’æ¨¡å‹
ä¸‹é¢æ›´è¿›ä¸€æ­¥å°è¯•ä¸€ä¸‹å¤šé¡¹å¼å›å½’ï¼Œä¸‹é¢æ˜¯å…³äº x çš„å¤šé¡¹å¼ï¼š
$$
\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 
$$
è¿™æ ·å°±èƒ½å¤Ÿæ‹Ÿåˆæ›´åŠ å¤æ‚çš„æ¨¡å‹ï¼Œè¿™é‡Œä½¿ç”¨äº† $x$ çš„æ›´é«˜æ¬¡ï¼ŒåŒç†è¿˜æœ‰å¤šå…ƒå›å½’æ¨¡å‹ï¼Œå½¢å¼ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯é™¤äº†ä½¿ç”¨ $x$ï¼Œè¿˜æ˜¯æ›´å¤šçš„å˜é‡ï¼Œæ¯”å¦‚ $y$ã€$z$ ç­‰ç­‰ï¼ŒåŒæ—¶ä»–ä»¬çš„ $loss$ å‡½æ•°å’Œç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹æ˜¯ä¸€è‡´çš„ã€‚ 
```python
import numpy as np  
import torch
import matplotlib.pyplot as plt
# å®šä¹‰ä¸€ä¸ªå¤šå˜é‡å‡½æ•°
w_target = np.array([0.5, 3, 2.4]) # å®šä¹‰å‚æ•°
b_target = np.array([0.9]) # å®šä¹‰å‚æ•°

f_des = 'y = {:.2f} + {:.2f} * x + {:.2f} * x^2 + {:.2f} * x^3'.format(
    b_target[0], w_target[0], w_target[1], w_target[2]) # æ‰“å°å‡ºå‡½æ•°çš„å¼å­

print(f_des)
```

æˆ‘ä»¬ä¸‹é¢ä»¥ w0 + w1 + w2 + b = y_sample çš„å¼å­, ä»¥ randn ä½œä¸º w åˆå§‹å€¼, 0 ä½œä¸ºb åˆå§‹å€¼è¿›è¡Œå°æ‰¹é‡æ¢¯åº¦ç®—æ³•æ±‚è§£è·å–ç»“æœ(å®šä¹‰çš„ä¸¤ä¸ªå‡½æ•°ä¸­, ä¸€ä¸ªæ˜¯æ¨¡å‹, ä¸€ä¸ªæ˜¯æŸå¤±å‡½æ•°): 
```python
import numpy as np  
import matplotlib.pyplot as plt  
import torch  
  
fig = plt.figure()  
epochNum = 80  
lr = 0.001  

x_sample = np.arange(-3, 3.1, 0.1)    # å®šä¹‰ç»˜ç”»æ•°ç»„åŒºé—´  
y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample ** 2 + w_target[2] * x_sample ** 3  
# plt.plot(x_sample,y_sample)  
# ----------------------------------------------------------  
# æ„å»ºæ•°æ® x å’Œ y# x æ˜¯ä¸€ä¸ªå¦‚ä¸‹çŸ©é˜µ [x, x^2, x^3]# y æ˜¯å‡½æ•°çš„ç»“æœ [y]x_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)  # æ„å»ºä»ç›¸åº”çš„æ•°æ®å–å¾—çš„è‡ªå˜é‡èŒƒå›´çŸ©é˜µ  
x_train = torch.from_numpy(x_train).float() # è½¬æ¢æˆ float tensory_train = torch.from_numpy(y_sample).float().unsqueeze(1) # è½¬åŒ–æˆ float tensor  
# å®šä¹‰å‚æ•° wå’Œb  
w = torch.randn((3, 1), dtype=torch.float, requires_grad=True)  # æ¯ä¸€æ¬¡runå¾—åˆ°çš„åˆå§‹fitæ›²çº¿ä¸åŒ  
b = torch.zeros((1), dtype=torch.float, requires_grad=True)  
# å®šä¹‰æ¨¡å‹  
def multi_linear(x):  
    return torch.mm(x, w) + b   # æ³¨: mmå¯ä»¥ä½¿ç”¨matmulæ¥è¿›è¡Œä»£æ›¿  
  
def loss_func(y_, y):  # ä½¿ç”¨å¹³æ–¹å‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°  
    return torch.mean((y_ - y) ** 2)  
y_pred = multi_linear(x_train) # æ„å»ºå¤šé¡¹å¼å‡½æ•°  
  
ax1 = fig.add_subplot(121)  
ax1.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='curve_prefit', color='r')  
ax1.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b')  
  
# ç”»å‡ºæ›´æ–°ä¹‹å‰çš„æ¨¡å‹  
# ============ è®­ç»ƒæ•°æ®é›† ====================for epoch in range (epochNum):  
    loss = loss_func(y_pred, y_train)  
    try:  
        w.grad.zero_()  
        b.grad.zero_()  
    except:  
        pass  
    loss.backward()  
    w.data = w.data - lr * w.grad.data  
    b.data = b.data - lr * b.grad.data  
    y_pred = multi_linear(x_train)  
  
ax2 = fig.add_subplot(122)  
ax2.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve-', color='r')  
ax2.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b')  
  
plt.legend()  
plt.show()
```

### (2) Logistic å›å½’æ¨¡å‹
å¯¹äºç»å…¸çš„$(0,1)$æ¨¡å‹ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
$$loss = -[y * log(\hat{y}) + (1-y) *log(1-y)]$$
```python
def logistic_regression(x):
    return torch.sigmoid(torch.mm(x, w) + b)
## mm: matmul

y_pred = logistic_regression(x_data)
# è®¡ç®—loss, ä½¿ç”¨clampçš„ç›®çš„æ˜¯é˜²æ­¢æ•°æ®è¿‡å°è€Œå¯¹ç»“æœäº§ç”Ÿè¾ƒå¤§å½±å“ã€‚
def binary_loss(y_pred, y):
    logits = ( y * y_pred.clamp(1e-12).log() + \
              (1 - y) * (1 - y_pred).clamp(1e-12).log() ).mean()
    return -logits
```
åªéœ€è¦å°†lossæ”¹ä¸ºè¿™ä¸ªå³å¯ã€‚

å…¶ä¸­ï¼Œpytorchæä¾›äº†ç›¸å…³çš„å‡½æ•°ï¼Œé‡è¦çš„æ˜¯ torch.optim.SGD å¯ä»¥ç›´æ¥é‡‡ç”¨å°æ‰¹é‡æ¢¯åº¦å›å½’, å¦å¤–æ¯ä¸€æ¬¡éœ€è¦è°ƒç”¨ optimizer.zero_grad()æ¸…é›¶ä¼˜åŒ–å™¨æ¢¯åº¦å³å¯ã€‚
```python title:æœ€ç®€å•çš„å°æ‰¹é‡æ¢¯åº¦ä¸‹é™å›å½’ä»£ç (å¤–éƒ¨å®šä¹‰æ¨¡å‹y_predå’ŒæŸå¤±binary_loss)
# ä½¿ç”¨ torch.optim æ›´æ–°å‚æ•°
from torch import nn
import time
# use the neural network model for training
# å®šä¹‰ä¼˜åŒ–å‚æ•°
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

# ä¼˜åŒ–å™¨çš„è®¾å®šä»¥åŠç›¸å…³çš„ä½¿ç”¨
optimizer = torch.optim.SGD([w, b], lr=0.1) # ä½¿ç”¨SGDç®—æ³•

for e in range(1000):
    # å‰å‘ä¼ æ’­
    y_pred = logistic_regression(x_data)  # è¿™ä¸ªy_predå’Œlossæ˜¯è‡ªå·±å®šä¹‰å‡½æ•°è®¡ç®—çš„
    loss = binary_loss(y_pred, y_data)      # è®¡ç®— loss
    # åå‘ä¼ æ’­
    optimizer.zero_grad() #-----ä½¿ç”¨ä¼˜åŒ–å™¨å°†æ¢¯åº¦å½’ 0
    loss.backward()
    optimizer.step() # *** åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæ›´æ–°å‚æ•°è¢«å°è£…ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨æ¥æ›´æ–°å‚æ•° **** 
    # è®¡ç®—æ­£ç¡®ç‡
    mask = y_pred.ge(0.5).float()
    acc = (mask == y_data).sum().item() / y_data.shape[0]
```


## ä¸‰ã€PyTorchæä¾›çš„æŸå¤±å‡½æ•° 
å‰é¢ä½¿ç”¨äº†è‡ªå·±å†™çš„ losså‡½æ•°ï¼Œå…¶å® PyTorch å·²ç»æä¾›äº†ä¸€äº›å¸¸è§çš„ losså‡½æ•°ï¼Œæ¯”å¦‚çº¿æ€§å›å½’é‡Œé¢çš„ loss æ˜¯ `nn.MSE()`ï¼Œè€Œ Logistic å›å½’çš„äºŒåˆ†ç±» loss åœ¨ PyTorch ä¸­æ˜¯ `nn.BCEWithLogitsLoss()`ï¼Œå…³äºæ›´å¤šçš„ lossï¼Œå¯ä»¥æŸ¥çœ‹[æ–‡æ¡£](https://pytorch.org/docs/stable/nn.html#loss-functions)

PyTorch å®ç°çš„ losså‡½æ•°æœ‰ä¸¤ä¸ªå¥½å¤„ï¼šç¬¬ä¸€æ˜¯æ–¹ä¾¿ä½¿ç”¨ï¼Œä¸éœ€è¦é‡å¤é€ è½®å­ï¼›ç¬¬äºŒå°±æ˜¯å…¶å®ç°æ˜¯åœ¨åº•å±‚ C++ è¯­è¨€ä¸Šçš„ï¼Œæ‰€ä»¥é€Ÿåº¦ä¸Šå’Œç¨³å®šæ€§ä¸Šéƒ½è¦æ¯”è‡ªå·±å®ç°çš„è¦å¥½ã€‚

å¦å¤–ï¼ŒPyTorch å‡ºäºç¨³å®šæ€§è€ƒè™‘ï¼Œå°†æ¨¡å‹çš„ Sigmoid æ“ä½œå’Œæœ€åçš„ loss éƒ½åˆåœ¨äº† `nn.BCEWithLogitsLoss()`ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ PyTorch è‡ªå¸¦çš„ loss å°±ä¸éœ€è¦å†åŠ ä¸Š Sigmoid æ“ä½œäº†
```python
# ä½¿ç”¨è‡ªå¸¦çš„loss
criterion = nn.BCEWithLogitsLoss() # å°† sigmoid å’Œ loss å†™åœ¨ä¸€å±‚ï¼Œæœ‰æ›´å¿«çš„é€Ÿåº¦ã€æ›´å¥½çš„ç¨³å®šæ€§
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

def logistic_reg(x):
    return torch.mm(x, w) + b
    
optimizer = torch.optim.SGD([w, b], 1.)

y_pred = logistic_reg(x_data)
loss = criterion(y_pred, y_data)  # è¿™æ ·å®šä¹‰æŸå¤±å‡½æ•°
# ä¹‹åè°ƒç”¨
for ......
	loss.backward()
	optimizer.step()_
print(loss.data)
```

å¸¸è§çš„æŸå¤±å‡½æ•°ï¼š
æµ…å±‚ï¼š LRï¼ŒSCM, ELm
æ·±å±‚ï¼šCNN ..

## å››ã€ç¥ç»ç½‘ç»œæ± åŒ–å±‚çš„æ¦‚å¿µç®€ä»‹
æ± åŒ–å±‚ä¹Ÿæ˜¯ä¸€ä¸ªç±»ä¼¼äºŒç»´çš„å±‚;

æ± åŒ–å±‚åœ¨CNNä¸­,ç”¨äºå‡å°‘ç©ºé—´ç»´æ•°è¿›è¡Œé™ç»´ä½¿ç”¨, å®é™…ä¸Šæ˜¯ä½¿ç”¨ä¸€ä¸ªfeature map, å…¶å®½é«˜åˆ†åˆ«ä¸º nh å’Œ nw, ç±»ä¼¼äºå·ç§¯è¿‡ç¨‹, åœ¨ map çš„æŒ‡å®šèŒƒå›´å†…è¿›è¡Œæ‰«æ; ç„¶åè¿‡æ»¤å‡ºç¬¦åˆè¦æ±‚çš„éƒ¨åˆ†ã€‚
ä¸€èˆ¬æ± åŒ–éœ€è¦æŒ‡å®š feature map å¤§å°å’Œ stride, ç¤ºä¾‹å¦‚ä¸‹: 
![[attachments/1_fhdK1xJND_m1Rdmr1m_9PQ.webp]]
ä¾‹å¦‚16x16çš„è¾“å…¥åœ¨2x2çª—, æ­¥è¿›ä¸º(2x2)æ± åŒ–åæˆä¸º 8x8 çš„ç‰¹å¾; å¸¸è§çš„æ± åŒ–æœ‰å‡å€¼æ± åŒ–å’Œæœ€å¤§å€¼æ± åŒ–ç­‰ç­‰; 


