## ä¸€ã€éœ€æ±‚æœå¯»ç®—æ³•ç®€ä»‹ 
æ¨èç³»ç»Ÿçš„éƒ¨åˆ†éœ€è¦å¯¹äºéœ€æ±‚çš„æ·±å…¥ç†è§£ï¼Œå¦‚æœä»…åŸºäºä¼ ç»Ÿçš„æœç´¢å¼•æ“ç®—æ³•ï¼Œå…³é”®èµ“ç»­å“Ÿå•Šçš„å­—æŸ¥è¯¢ä¿¡æ¯éš¾ä»¥å®šä½éœ€æ±‚ã€‚è€Œå®é™…ä¸Šç”¨æˆ·ä¸€èˆ¬æ›´éœ€è¦çš„æ˜¯ç¬¦åˆä¸ªäººåå¥½çš„ç»“æœã€‚

ä¸€èˆ¬çš„æ¨èç³»ç»Ÿçš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ : 
1. **æ‰“åŒ…é”€å”®(Cross Selling)**:å³åœ¨æ¨èæŸäº›äº§å“æ—¶, ä¹Ÿä¼šåŒæ—¶æ¨èä¸€åŒè´­ä¹°çš„äº§å“
2. **ååŒè¿‡æ»¤**: ä¸»è¦æ˜¯å®šä½ç”¨æˆ·çš„è´­ä¹°éœ€æ±‚ã€‚åŸºäºæ¨¡ç³Šå…³é”®å­—ä»åˆ—è¡¨ä¸­æ˜ç¡®éœ€æ±‚ã€‚æ­¤å¤–ä¹ŸåŒ…æ‹¬çœ‹è¿‡æ­¤å•†å“ä¹‹åè´­ä¹°çš„å…¶å®ƒå•†å“ã€‚åæ˜ äº†éšå¼éœ€æ±‚(æ›¾ç»æµè§ˆè¿‡æŸé¡µé¢çš„ç”¨æˆ·æ‰€è´­ä¹°çš„äº§å“)ã€‚
3. ç”¨æˆ·å•†å“è¯„è®ºåˆ—è¡¨çš„æŠ½å’Œæ±‡æ€»åˆ†æ, ç”¨äºè¯„ä¼°äº§å“è´¨é‡çš„åˆ†å¸ƒæ°´å¹³ã€‚

æ¨èç³»ç»Ÿçš„ä¸»è¦ç®—æ³•åŒ…å« : 
1. åŸºäºäººå£ç»Ÿè®¡å­¦çš„æ¨èæœºåˆ¶ : æ ¹æ®ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯è·å–ç›¸å…³ç¨‹åº¦, å¹¶æ ¹æ®ç›¸ä¼¼ç”¨æˆ·å–œå¥½çš„çš„å…¶ä»–ç‰©å“æ¨èç»™å½“å‰ç”¨æˆ·ã€‚
2. åŸºäºååŒè¿‡æ»¤çš„æ¨èæœºåˆ¶ : ä¸»è¦åˆ†ä¸º<b><mark style="background: transparent; color: blue">åŸºäºç”¨æˆ·çš„æ¨èå’ŒåŸºäºé¡¹ç›®çš„æ¨è</mark></b>ã€‚
3. åŸºäºéšè¯­ä¹‰çš„æ¨èæ¨¡å‹ : å…¶ä¸­, **ç›®å‰ç²¾åº¦æœ€é«˜çš„ç®—æ³•æ˜¯ SVD éšè¯­ä¹‰æ¨¡å‹**ã€‚

ä¸€èˆ¬åœ¨è®­ç»ƒé˜¶æ®µ, å±äº CPU å¯†é›†å‹ä»»åŠ¡, è€Œåœ¨åˆ†ç±»æˆ–è€…é¢„æµ‹é˜¶æ®µå±äº IO å¯†é›†å‹ä»»åŠ¡ã€‚ ä¸€èˆ¬ä¸ä½¿ç”¨è„šæœ¬è¯­è¨€è¿›è¡Œè®¾è®¡ã€‚

ååŒè¿‡æ»¤çš„æ¨¡ä»¥åŠç®—æ³•åŒ…å«: 
1. æ•°æ®å¤„ç†ä¸ UI çŸ©é˜µ 
2. UserCF å’Œ ItemCF æ¨èæ¨¡å‹
3. KMeans ç›¸ä¼¼æ€§è®¡ç®— 
4. SVD ç›¸ä¼¼æ€§è®¡ç®—  

### (1) ç‰©å“æ•°æ®çš„ KMeans èšç±»
KMeans èšç±»å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/4. KNNç®—æ³•å’ŒKMeansèšç±»ç›¸å…³ç®—æ³•#äºŒã€K-Meansç®—æ³•(Kå‡å€¼ç®—æ³•)|KMeansç®—æ³•]] , å¹¶ç»™å‡ºä¸€ä¸ªç®€å•çš„ iris æ•°æ®é›† KMeans èšç±»ä»£ç :
```python
import sklearn  
from sklearn.datasets import load_iris  
from sklearn.cluster import KMeans  
from sklearn.preprocessing import StandardScaler  
from sklearn.model_selection import train_test_split  
import matplotlib.pyplot as plt  
  
iris = load_iris()  
  
iris_data = iris.data  
iris_target = iris.target  
  
X_train, X_test, y_train, y_test =  train_test_split(iris_data, iris_target, test_size=0.2, random_state=None)
  
kms = KMeans(n_clusters=3)  
kms.fit(X_train, y_train)  
  
y_pred = kms.predict(iris_data)  
  
fig, axes = plt.subplots(2,1, figsize=(5,10))  
axes[0].scatter(iris_data[:,0], iris_data[:,1], c=iris_target)  
axes[0].set_title("real results")  
axes[1].scatter(iris_data[:,0], iris_data[:,1], c=y_pred)  
axes[1].set_title("KMeans results")  
plt.show()
```

KMeans èšç±»ç»“æœå¦‚ä¸‹:
![[attachments/Pasted image 20240914173557.png]]

```python
# è·å–èšç±»ä¸­å¿ƒç‚¹
centers = kms.cluster_centers_
print("Cluster centers:\n", centers)
```

äºŒåˆ† KMeans ä»ç„¶å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/4. KNNç®—æ³•å’ŒKMeansèšç±»ç›¸å…³ç®—æ³•|4. KNNç®—æ³•å’ŒKMeansèšç±»ç›¸å…³ç®—æ³•]]

### (2) ååŒè¿‡æ»¤ User CF å’Œ Item CF åŸç†
ååŒè¿‡æ»¤(Collabrate Filtering, CF) åˆ†ä¸º User CF å’Œ Item CF éƒ¨åˆ†;
é¦–å…ˆéœ€è¦å»ºç«‹<b><mark style="background: transparent; color: orange">ç”¨æˆ·åå¥½çŸ©é˜µ</mark></b>, æ ¹æ® KNN ç®—æ³•ï¼Œä»¥è·ç¦»æˆ–è€…å¤¹è§’ä½™å¼¦ä¸ºè·ç¦», åˆ†åˆ«è®¡ç®—å…¶åˆ°æ¯ä¸€ç±»ç”¨æˆ·çš„è·ç¦»;
![[Excalidraw/3. æ¨èç³»ç»Ÿå’Œéœ€æ±‚æœå¯»ç®—æ³• 2024-09-14 18.03.57|550]]

ç”¨æˆ·åå¥½çŸ©é˜µæ˜¯ç¨€ç–çš„çŸ©é˜µ, è¡¨ç¤ºç”¨æˆ·æ˜¯å¦å’ŒæŸä¸ªç‰©å“å‘ç”Ÿäº¤äº’, ä¾‹å¦‚ä¸‹å›¾:

| UserID/ItemID | Item1 | Item2 | Item3 | Item4 | Item5 |
|---------------|-------|-------|-------|-------|-------|
| User1         | 5     | 3     | 0     | 1     | 0     |
| User2         | 4     | 0     | 0     | 1     | 1     |
| User3         | 1     | 1     | 0     | 5     | 0     |
| User4         | 0     | 0     | 5     | 4     | 4     |
| User5         | 0     | 0     | 4     | 0     | 5     |

**User CF å³é‡‡ç”¨KNNç­‰å°†ç”¨æˆ·(USER C)é€šè¿‡usr_vector å½’ç±»åˆ°æŸä¸€ç±»ç”¨æˆ·(Aæˆ–B)ä¸­ã€‚å¹¶å°†è¯¥ç±»ç”¨æˆ·æ²¡æœ‰ä¹°è¿‡çš„ç‰©å“è¿›è¡Œæ¨è**
**Item CF å³é€‰å–æ–°ç”¨æˆ·(USER C)æ¯”è¾ƒåå¥½çš„ç‰©å“ (item E), å¹¶é€šè¿‡åˆ†ç±»è·å–ä¸ item E ç›¸ä¼¼çš„ç‰©å“ (é€šè¿‡item_vecå¯ä»¥æ‰¾å‡ºå¦‚item Aç›¸ä¼¼,åˆ™å°†item A ä¹Ÿæ¨èç»™ item Eçš„ç”¨æˆ·)**

ä¸€èˆ¬é‡‡ç”¨å¦‚ä¸‹çš„ç›¸ä¼¼åº¦:
ä½™å¼¦ç›¸ä¼¼åº¦: 
$$\text{sim}(u, v) = \frac{R_u \cdot R_v}{\|R_u\| \|R_v\|}$$
çš®å°”é€Šç›¸ä¼¼åº¦:
$$\text{sim}(u, v) = \frac{\sum_{i \in I} (R_{ui} - \bar{R}_u)(R_{vi} - \bar{R}_v)}{\sqrt{\sum_{i \in I} (R_{ui} - \bar{R}_u)^2} \sqrt{\sum_{i \in I} (R_{vi} - \bar{R}_v)^2}}$$

ç„¶å<b><mark style="background: transparent; color: orange">é€šè¿‡æ¯ä¸¤ä¸ªç”¨æˆ·è¿›è¡Œè¯„åˆ†å‘é‡ç‚¹ç§¯</mark></b>, å¾—åˆ°çš„ç›¸ä¼¼åº¦çŸ©é˜µå³ä¸º UserCF çŸ©é˜µ, å³æœ‰

| UserCF | 1        | 2        | 3        | 4        | 5        |
| ------ | -------- | -------- | -------- | -------- | -------- |
| 1      | sim(1,1) | sim(1,2) |          |          |          |
| 2      | sim(2,1) | $\ddots$ |          |          |          |
| 3      |          |          | $\ddots$ |          |          |
| 4      |          |          |          | $\ddots$ |          |
| 5      |          |          |          |          | sim(5,5) |

### (3) ä¸€èˆ¬çŸ©é˜µçš„ SVD åˆ†è§£åŠå…¶è¯æ˜ 
å¯¹äºåŸºäºæ¨¡å‹çš„ CF æ–¹æ³•ä¸­, æœ€å¸¸ç”¨çš„æ˜¯éšè¯­ä¹‰æ¨¡å‹, é‡‡ç”¨çš„æ˜¯ SVD åˆ†è§£, å³çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£(å‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/âœ–ï¸Matrix Theory/4. çŸ©é˜µåˆ†è§£#å››ã€å¥‡å¼‚å€¼åˆ†è§£(SVD åˆ†è§£)|å››ã€å¥‡å¼‚å€¼åˆ†è§£(SVD åˆ†è§£)]])ã€‚ 
é¦–å…ˆ, å‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/ğŸ“linear algebra/ç¬¬äº”ç«  çŸ©é˜µçš„ç›¸ä¼¼å˜æ¢#(3) çŸ©é˜µçš„ç›¸ä¼¼å¯¹è§’åŒ–|ç¬¬äº”ç«  çŸ©é˜µçš„ç›¸ä¼¼å˜æ¢]], æœ‰: 
å¯¹äº n é˜¶æ–¹é˜µ $A^{n\times n}$, æœ‰ç›¸ä¼¼çŸ©é˜µ:
$$A = Q \Sigma Q ^{-1}$$
å…¶ä¸­ $\Sigma$ ä¸ºç‰¹å¾å€¼æ„æˆçš„å¯¹è§’é˜µ, $Q$ ä¸ºç‰¹å¾å‘é‡æ’æˆçš„åˆ—é˜µã€‚ä½†æ˜¯,æ­¤æ–¹æ³•ä»…ä»…æ˜¯å¯¹æ–¹é˜µè€Œè¨€çš„ã€‚å®é™…**å¤šæ•°æƒ…å†µä¸‹æ ·æœ¬ä¸æ˜¯æ–¹é˜µ**ï¼Œæ­¤æ—¶å¯ä»¥<mark style="background: transparent; color: red">ä»¥å¥‡å¼‚å€¼åˆ†è§£æè¿°æ™®é€šçŸ©é˜µçš„é‡è¦ç‰¹å¾</mark>

<b><mark style="background: transparent; color: blue">å®šä¹‰(çŸ©é˜µçš„SVDåˆ†è§£)</mark></b>:è®¾ $A$ æ˜¯ä»»æ„çš„ $M \times N$ çŸ©é˜µ, åˆ™æœ‰çŸ©é˜µçš„ SVD åˆ†è§£ä¸º:
$$\Large\boxed{A =  U \Sigma V^{T}}$$
å…¶ä¸­ $U$ æ˜¯ $M \times M$ æ–¹é˜µ, $\Sigma$ æ˜¯ä¸€ä¸ª $M \times N$ çŸ©é˜µ, ä»…æœ‰å¯¹è§’çº¿ä¸Šæœ‰å…ƒç´ , ä¸”<mark style="background: transparent; color: red">å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ç§°ä¸ºå¥‡å¼‚å€¼</mark>, å…¶å®ƒå…ƒç´ å‡ä¸º0. è€Œ $V^T$ æ˜¯ä¸€ä¸ª $N \times N$ æ–¹é˜µã€‚å³:
$$\Large\boxed{A_{m \times  n} = U _{m\times m} \Sigma_{m \times  n} V^{T}_{n \times n}}$$
å…¶ä¸­ U çš„å…ƒç´ ç§°ä¸º<mark style="background: transparent; color: red">å·¦å¥‡å¼‚å‘é‡</mark>, V ä¸­çš„å…ƒç´ ç§°ä¸º<mark style="background: transparent; color: red">å³å¥‡å¼‚å‘é‡</mark>, å¹¶æœ‰å…³ç³» $U^{T} U = V^{T}V= I$ 
å‚è€ƒ [Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) è¡¥å…… [polar_decomposition](https://en.wikipedia.org/wiki/Polar_decomposition#Matrix_polar_decomposition) æ¨å¯¼å¯ä»¥å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/æ¨å¯¼éƒ¨åˆ†/SVD decomposition.pdf|SVD decomposition.pdf]]  å’Œ [SVDåˆ†è§£çš„è¯æ˜](https://zhuanlan.zhihu.com/p/399547902) 

**ç»™å‡ºæ¨å¯¼**: é¦–å…ˆ, è®¾Açš„ç§©ä¸ºr, å³æœ‰ $r = \min \left\{ m,n\right\}$, æ­¤æ—¶ $A^T$ ç§©æ˜¾ç„¶ä¹Ÿä¸º $r$, ç»è¿‡[ä¸€æ–‡å…³äºçŸ©é˜µç§©å…¬å¼r(AA^T)=r(A^T A)=r(A)çš„è¯æ˜](https://zhuanlan.zhihu.com/p/447703725) å¾—åˆ°: 
$$rank (A^{T}A ) =  rank (A A^{T} )$$
åˆ™ $\text{rank}(A^T A) = r$, æ­¤æ—¶æ ¹æ®$A^TA$ä¸ºåŠæ­£å®šçŸ©é˜µ(æ˜¾ç„¶$x^TA^TAx \geq 0$), å› æ­¤å…¶ç‰¹å¾å€¼ä¸ºéè´Ÿå®æ•°ã€‚å…¶ä¸­**ç§©çš„ä¸ªæ•°å³ä¸ºéé›¶ç‰¹å¾å€¼çš„ä¸ªæ•°**ï¼Œå–å…¶ç‰¹å¾å€¼å¾—åˆ°:
$$\boxed{\Large  (A^{T}A) v_{i}= \lambda_{i} v_{i}}$$
å…¶ä¸­ $A$ ä¸º $m \times n$ çŸ©é˜µ, **åŒæ—¶,ç”±äº$AA^T$ä¸ºå®å¯¹ç§°é˜µ, ç›¸ä¼¼å¯¹è§’åŒ–å $v_i, v_j$ æ˜¯æ­£äº¤å‘é‡, åˆ™**:
$$<Av_{i} ,Av_{j}> = v_{j}^{T} A^{T} A v_{i}  = \lambda v_{j}^{T}  v_{i} = \begin{cases}
\lambda_{i} \qquad  1 \leq  i = j \leq  r  \\
0 \qquad  i \neq j  \text{ or } i = j > r
\end{cases}$$
æ˜¾ç„¶ $<Av_i, Av_j>$ ä¸ºæ­£äº¤å‘é‡ç»„, æ­¤æ—¶å– i = j å³å¾—å¦‚ä¸‹å¼å­:
$$\boxed{\Large ||Av_{i}|| = \sqrt{\lambda_{i}}\qquad i = 1, 2, \dots  r}$$
$$\Large\boxed{||Av_{i}|| = 0 \qquad  i = r + 1, \dots n}$$
å…¶ä¸­$v_{i}$å°±æ˜¯ä¸Šé¢çš„<mark style="background: transparent; color: red">å³å¥‡å¼‚å‘é‡</mark>, æˆ‘ä»¬å– $\sigma_i  = \sqrt{\lambda_i}$ å³ä¸º<b><mark style="background: transparent; color: blue">å¥‡å¼‚å€¼</mark></b>ï¼Œè€Œä»¤:
$$\Large \boxed{u_{i}  = \frac{Av_{i}}{||Av_{i}||} =  \frac{1}{\sqrt{\lambda_{i}}} Av_{i} \qquad  i= 1, \dots  n} $$
**å…¶ä¸­ç”±äº A ä¸º $m*n$ æ•… $\boldsymbol{u}$ ä¸º $m \times 1$ çš„å‘é‡, å®é™…ä¸Šå°±æ˜¯å‘é‡u**<b><mark style="background: transparent; color: blue">(å·¦å¥‡å¼‚å‘é‡)</mark></b>ã€‚ç„¶åç»è¿‡å¦‚ä¸‹è¯æ˜è¿‡ç¨‹: 
![[attachments/Pasted image 20240919170741.png|400]]
å¾—åˆ° SVD åˆ†è§£:
$$\Large\boxed {A_{m \times  n} = U_{m \times  m }\Sigma_{m \times  n} V_{n \times n}^{T}}$$
### (4) çŸ©é˜µçš„éƒ¨åˆ†å¥‡å¼‚å€¼(SVD)åˆ†è§£ 
å¥‡å¼‚å€¼$\sigma$å’Œç‰¹å¾å€¼ç±»ä¼¼, æ˜¾ç„¶**åœ¨ $\Sigma$ ä¸­, $\sigma$ å€¼ä¹Ÿæ˜¯ä»å¤§åˆ°å°æ’åˆ—çš„**ã€‚æ­¤æ—¶**å¯èƒ½çŸ©é˜µå‰ 10% ç”šè‡³ 1% çš„ç‰¹å¾å€¼çš„å’Œå°±å æ®äº†å…¨éƒ¨å¥‡å¼‚å€¼ä¹‹å’Œçš„99%ä»¥ä¸Š**ã€‚æˆ‘ä»¬å–å‰ $r$ ä¸ªå¥‡å¼‚å€¼, å…¶ä½™çš„ r+1é˜¶ä»¥åçš„å¥‡å¼‚å€¼éå¸¸æ¥è¿‘0,åˆ™å®šä¹‰<b><mark style="background: transparent; color: blue">çŸ©é˜µçš„éƒ¨åˆ†SVDåˆ†è§£</mark></b>ä¸º:
$$\boxed{\Large A_{m \times  r} \approx U_{m \times r} \Sigma_{r \times r} V^{T}_{r\times  n}}$$
åœ¨numpy.linalg ä¸­æä¾›äº†SVDåˆ†è§£å‡½æ•°:
```python 
import numpy.linalg import svd
U, S, VT  = svd(A)
print(U, S, VT)
```

æ­¤å¤–, å¯¹äºç¨€ç–çŸ©é˜µ, ä¹Ÿå¯ä»¥è¿›è¡Œ svd åˆ†è§£, ç¤ºä¾‹ä»£ç å¦‚ä¸‹:
```python
import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse.linalg import svds

# åˆ›å»ºä¸€ä¸ªç¨€ç–çŸ©é˜µ (Compressed Sparse Row format)
data = np.array([4, 5, 3, 2, 6])          # éé›¶å…ƒç´ 
row_indices = np.array([0, 0, 1, 2, 3])   # å…ƒç´ å¯¹åº”çš„è¡Œç´¢å¼•
col_indices = np.array([0, 1, 2, 3, 4])   # å…ƒç´ å¯¹åº”çš„åˆ—ç´¢å¼•

# æ„é€ ç¨€ç–çŸ©é˜µ
sparse_matrix = coo_matrix((data, (row_indices, col_indices)), shape=(5, 5))

print("ç¨€ç–çŸ©é˜µï¼š")
print(sparse_matrix.toarray())

# å¯¹ç¨€ç–çŸ©é˜µè¿›è¡Œ SVD åˆ†è§£
k = 3  # è®¡ç®—å‰ k ä¸ªå¥‡å¼‚å€¼å’Œå¯¹åº”çš„å¥‡å¼‚å‘é‡
u, s, vt = svds(sparse_matrix, k=k)

# SVD ç»“æœ
print("\nå·¦å¥‡å¼‚å‘é‡ (U)ï¼š")
print(u)

print("\nå¥‡å¼‚å€¼ (S)ï¼š")
print(s)

print("\nå³å¥‡å¼‚å‘é‡ (V^T)ï¼š")
print(vt)
```

## äºŒã€PCA ä¸»æˆåˆ†åˆ†æå’Œ SVD åˆ†è§£çš„åº”ç”¨
éœ€è¦è¯´æ˜, PCA å’Œ SVD éƒ½æ˜¯ sklearn.decomposition ä¸­çš„å†…å®¹ã€‚
### (1) PCA ä¸»æˆåˆ†åˆ†æåŸç†
[sklearnå®æˆ˜ä¹‹é™ç»´ç®—æ³•PCAä¸SVD_ä½¿ç”¨svdè¿›è¡Œä¸»æˆåˆ†åˆ†æé¸¢å°¾èŠ±åˆ†ç±»-CSDNåšå®¢](https://blog.csdn.net/qq_48314528/article/details/119845670)
PCA çš„åŸºæœ¬åŸç†æ˜¯ï¼š**å¦‚æœä¸€ä¸ª==é«˜ç»´æ•°æ®é›†å¦‚æœèƒ½å¤Ÿè¢«ç›¸å…³å˜é‡è¡¨ç¤º, é‚£ä¹ˆä»…æœ‰ä¸€äº›ç»´æœ‰æ„ä¹‰ã€‚æ ¹æ®æ­¤åŸç†, å¯ä»¥æå–å‡ºé«˜ç»´å˜é‡ä¸­æŸäº›ç‰¹å¾æˆ–è€…ç›¸å…³å˜é‡, å³å¯é‡‡ç”¨ä½ç»´æ•°æ®è¡¨ç¤ºè¯¥å˜é‡==**, è€Œ<mark style="background: transparent; color: red">ä¸é‡è¦çš„ç»´å¯ä»¥åœ¨è®¡ç®—ä¸­å¿½ç•¥</mark>ã€‚æ­¤æ—¶<b><mark style="background: transparent; color: blue">å¦‚æœå¯»æ‰¾åˆ°æ•°æ®ä¸­æ–¹å·®æœ€å¤§çš„æ–¹å‘, åˆ™è¢«ç§°ä¸ºå‘é‡çš„ä¸»æˆåˆ†</mark></b> 

åœ¨é«˜ç»´æ•°æ®ä¸­, å¾€å¾€å™ªå£°ä¸å¸¦æœ‰æœ‰æ•ˆä¿¡æ¯, ä¸”éƒ¨åˆ†ç‰¹å¾æ˜¯ç›¸å…³çš„, å› æ­¤éœ€è¦åœ¨å‡å°‘ç‰¹å¾æ•°é‡åŒæ—¶ä¿æŒä¿ç•™æœ‰æ•ˆä¿¡æ¯ã€‚<mark style="background: transparent; color: red">æ ¹æ®æ ·æœ¬æ–¹å·®, æ–¹å·®è¶Šå¤§, ç‰¹å¾æ‰€å«æœ‰ä¿¡æ¯è¶Šå¤šã€‚ ä¸€èˆ¬åœ°, æ–¹å·®å°çš„æ•°æ®, å…¶ä½“ç°ä¿¡æ¯æ¯”è¾ƒå°‘, å› æ­¤å¯ä»¥èˆå»</mark>; å³<b><mark style="background: transparent; color: orange">ä¿ç•™æ•°æ®é›†ä¸­å«æœ‰æœ€å¤§æ–¹å·®çš„ä¸€éƒ¨åˆ†æ–¹å‘ï¼Œç§°ä¸ºå‘é‡çš„ä¸»æˆåˆ†</mark></b> 

ä¸€èˆ¬åœ°, æ ·æœ¬æ–¹å·®è®¡ç®—ä¸º(é‡‡ç”¨ n-1 æ˜¯æ ·æœ¬çš„æ— åä¼°è®¡): 
$$\text{Var} = \frac{1}{n -1} \sum^{n}_{i=1} (x_{i} - \overline{x})^{2}$$
PCA çš„ä¸€èˆ¬æ­¥éª¤å¦‚ä¸‹: é¦–å…ˆ, è®¡ç®—<b><mark style="background: transparent; color: orange">æ ·æœ¬çš„å‡å€¼å‘é‡å’Œåæ–¹å·®çŸ©é˜µ</mark></b>$S$: 
$$\mu =\frac{1}{n} \sum^{n}_{i=1} x_{i} \qquad  S = \frac{1}{n}\sum^{n}_{i=1}(x_{i}- \mu) (x_{i} - \mu)^{T}$$
ç„¶åè®¡ç®— $S$ çš„ç‰¹å¾å€¼ $\lambda_{i}$ å’Œå¯¹åº”çš„ç‰¹å¾å‘é‡ $v_i$  
$$Sv_{i}  = \lambda_{i} v_{i}  \qquad  i = 1,2,\dots n$$
ç„¶å<mark style="background: transparent; color: red">å¯¹ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡è¿›è¡Œé€’å‡æ’åº, å¦‚æœå®šä¹‰ä¸€ä¸ªå‘é‡æœ‰ k ä¸ªä¸»æˆåˆ†, åˆ™å¯¹åº”çš„ k ä¸ªä¸»æˆåˆ†å³ä¸º k ä¸ªæœ€å¤§çš„ç‰¹å¾å€¼åŠæ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡</mark>ã€‚æ³¨æ„: è¿™ä¸ªé€šè¿‡ PCA `(n_components = x)` è¿›è¡ŒæŒ‡å®š(å¦‚æœä»…ä»…å¯è§†åŒ–,å¯ä»¥é™ç»´åˆ°äºŒç»´) 

æ­¤æ—¶, æˆ‘ä»¬è®¾ $x$ æœ‰ $n$ ä¸ªåˆ†é‡, é™ç»´åˆ° $r$, åˆ™å–ä¸»æˆåˆ†å‘é‡çŸ©é˜µ $W_{(n \times  r)}$ã€‚è€Œå¯¹äºåæ–¹å·®çŸ©é˜µ $S$, è®¾æŸä¸ªæ–¹å‘å‘é‡ä¸º $w$, åˆ™åœ¨å¯¹åº”æ–¹å‘ä¸Šçš„æ–¹å·®æŠ•å½±ä¸º $w^{T} (x_i - \mu)(x_{i} - \mu)^{T} w =wSw^{T}$, å³å¯¹äº$w$ ä¸ºç‰¹å¾å‘é‡æ—¶, ç‰¹å¾å€¼å€¼å³ä¸ºæ–¹å·®, åˆå‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/âœ–ï¸Matrix Theory/5. ç‰¹å¾å€¼ä¼°è®¡å’Œå¯¹ç§°çŸ©é˜µçš„ææ€§#(2) å®å¯¹ç§°çŸ©é˜µ Rayleigh å•†çš„ææ€§|å®å¯¹ç§°çŸ©é˜µ Rayleigh å•†çš„ææ€§]]ã€‚æ•…**å…¶ä¸­ç¬¬ $v_i$ ä¸ªç‰¹å¾å‘é‡æ˜¯ç‰¹å¾å€¼ç›¸å·®æœ€å¤§çš„æ–¹å‘, æ­¤æ—¶æˆ‘ä»¬åªéœ€è¦å°† $x$ æŠ•å½±åˆ°è¿™äº›æ–¹å‘ä¸Š, å³å¯¹åº”ä¸»æˆåˆ†æ–¹å‘ $v_i$ ä¸‹çš„åæ ‡ä¸º: $(x- \mu) \cdot v_{i}$** , å¾—åˆ°çš„ r ä¸ªä¸»æˆåˆ†ä¸º $y$, åˆ™æœ‰å…¬å¼: 
$$y = W^{T}  (x-\mu)   \qquad  W = (v_{1}, v_{2}, \dots  v_{r})$$
è€Œ <b><mark style="background: transparent; color: orange">PCA åŸºçš„å˜æ¢</mark></b>ä¸º:
$$x = W y + \mu \qquad  W =  (v_{1}, v_{2}, \dots  v_{r})$$
ä¸‹é¢é‡‡ç”¨ PCA é™ç»´ä¹‹åé¢„æµ‹ Iris æ•°æ®é›†å¹¶ç»˜åˆ¶å›¾åƒã€‚

```python
from sklearn.datasets import load_iris  
from sklearn.decomposition import PCA  
from sklearn.preprocessing import StandardScaler  
from sklearn.model_selection import train_test_split, cross_val_score  
from sklearn.naive_bayes import GaussianNB  
from sklearn.metrics import accuracy_score  
import matplotlib.pyplot as plt  
  
X,y = load_iris(return_X_y=True)  
x_train, x_test,y_train, y_test = train_test_split(X,y,test_size=0.35,random_state=42)  
  
pca = PCA(n_components=2).fit(x_train) # 2 components  
print(pca.components_)     # ä¸»æˆåˆ†çš„ç‰¹å¾å‘é‡(æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªä¸»æˆåˆ†ï¼Œ å³å¯¹åº”çš„ V çŸ©é˜µ)
print("variance:" ,pca.explained_variance_)         # æŸ¥çœ‹é™ç»´åæ¯ä¸ªæ–°ç‰¹å¾å‘é‡ä¸Šæ‰€å¸¦çš„ä¿¡æ¯é‡å¤§å°ï¼ˆå¯è§£é‡Šæ€§æ–¹å·®çš„å¤§å°ï¼‰  
print("variance_ratio:", pca.explained_variance_ratio_)   # æŸ¥çœ‹é™ç»´åæ¯ä¸ªæ–°ç‰¹å¾å‘é‡ä¸Šæ‰€å¸¦çš„ä¿¡æ¯é‡å æ€»ä¿¡æ¯é‡çš„æ¯”ä¾‹ï¼ˆå¯è§£é‡Šæ€§æ–¹å·®çš„ç™¾åˆ†æ¯”ï¼‰  
  
x_train_new = pca.transform(x_train)
x_test_new = pca.transform(x_test)  
  
gnb = GaussianNB()  
gnb.fit(x_train_new,y_train)  
y_pred = gnb.predict(x_test_new)  
  
fig, axes = plt.subplots(1,2, figsize=(12,5), subplot_kw = {"xticks":[],"yticks":[]})  # subplot_kw æŒ‡å®šç»˜åˆ¶é¡ºåº
axes[0].scatter(x_test_new[:,0],x_test_new[:,1],c=y_test)  
axes[1].scatter(x_test_new[:,0],x_test_new[:,1],c=y_pred)  
plt.show()  
  
print("predict accuracy:", accuracy_score(y_test,y_pred, normalize=True))
```

> [!caution] PCA çš„ n_components å‚æ•°
> 1. ä¸ºæ•´æ•°æ—¶, åˆ™ä¿ç•™ç›¸åº”çš„ç»´æ•°ã€‚
> 2. å¦‚æœä¸å†™ä»»ä½•å€¼, åˆ™è¿”å› min(X.shape) (ç”±äºæ ·æœ¬é‡ä¸€èˆ¬å¤§äºç‰¹å¾ä¸ªæ•°, ç›¸å½“äºæ²¡æœ‰è½¬æ¢ç‰¹å¾, å› æ­¤é™¤äº†ç»˜åˆ¶ç´¯è®¡å¯è§£é‡Šæ–¹å·®è´¡çŒ®ç‡æ›²çº¿ `np.cumsum(pca_line.explained_variance_ratio_)` ä»¥å¤–, ä¸€èˆ¬ä¸é‡‡ç”¨è¯¥æ–¹å¼)
> 3. å¯ä»¥é‡‡ç”¨ PCA ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡è¿›è¡Œè‡ªé€‰è¶…å‚æ•° `n_components` , åªéœ€æŒ‡å®š
> `pca = PCA(n_components = "mle")` å³å¯ 
> 4. è¾“å…¥ [0,1] é—´çš„æµ®ç‚¹æ•°å¹¶å– `svd_solver = full` æ—¶, æœ‰ `pca_f = PCA(n_components=0.97,svd_solver="full")` åˆ™é™ç»´åˆ°ä¿è¯é™ç»´å**æ€»è§£é‡Šæ€§æ–¹å·®è´¡çŒ®ç‡å¤§äºn_componentsæŒ‡å®šç™¾åˆ†æ¯”çš„ä¿¡æ¯é‡**çš„ç»´æ•°ã€‚

å¦å¤–ç»˜åˆ¶ä¸‰ç»´å›¾åƒä¹Ÿéå¸¸ç®€å•(ä¸è¿›è¡Œé¢„æµ‹):
```python
from sklearn.decomposition import PCA  
# unused but required import for doing 3d projections with matplotlib < 3.2  
import mpl_toolkits.mplot3d

fig = plt.figure(1, figsize=(8, 6))  
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)  
  
X_reduced = PCA(n_components=3).fit_transform(iris.data)  
ax.scatter(  
    X_reduced[:, 0],  
    X_reduced[:, 1],  
    X_reduced[:, 2],  
    c=iris.target,  
    s=40, # marker size )
plt.show()
```
### (2) PCA ä¸­çš„ SVD 
SVD å®é™…ä¸Šæ˜¯<b><mark style="background: transparent; color: blue">ä¸è®¡ç®—åæ–¹å·®çŸ©é˜µ, ç›´æ¥æ‰¾å‡ºä¸€ä¸ªæ–°ç‰¹å¾å‘é‡ç»„æˆçš„ n ç»´ç©ºé—´</mark></b>, å³ç›´æ¥è·å–å³åˆ†è§£å‘é‡çŸ©é˜µ $V^T$, è€Œ $U$ å’Œ $\Sigma$ åœ¨ fit å®Œæˆä¹‹å, å³è¢«èˆå¼ƒã€‚
è€Œ `print(pca.components_)` å¯ä»¥ç›´æ¥è·å– PCA ä¸­ SVD åˆ†è§£å™¨çš„$V$å‚æ•°ã€‚

å®é™…ä¸ŠPCA çš„ SVD æ±‚è§£å™¨æ˜¯é€šè¿‡ PCA (svd_solver) æ§åˆ¶çš„, å‚æ•°åŒ…å«:
**â€œautoâ€ï¼Œâ€œfullâ€ï¼Œâ€œarapckâ€ï¼Œâ€œrandomizedâ€ï¼Œé»˜è®¤"auto"**ï¼Œå…·ä½“è§£é‡Šå¦‚ä¸‹: 
![[attachments/Pasted image 20240922105941.png|450]]

éœ€è¦è¯´æ˜ PCAå’Œç‰¹å¾é€‰æ‹©çš„åŒºåˆ«: **ç‰¹å¾é€‰æ‹©åçš„ç‰¹å¾çŸ©é˜µæ˜¯å¯è§£è¯»çš„ï¼Œè€ŒPCAé™ç»´åçš„ç‰¹å¾çŸ©é˜µæ˜¯ä¸å¯è§£è¯»çš„, å³å°†å·²ç»å­˜åœ¨çš„ç‰¹å¾è¿›è¡Œå‹ç¼©, ä¸”é™ç»´å®Œæ¯•åç‰¹å¾ä¸æ˜¯åŸçŸ©é˜µçš„ä»»ä½•ç‰¹å¾, è€Œæ˜¯é€šè¿‡æŸäº›æ–¹å¼è¿›è¡Œç»„åˆèµ·æ¥çš„ï¼Œæ–°çš„ç‰¹å¾**.
**é‡è¦çš„æ˜¯ï¼š å¦‚æœåŸç‰¹å¾çŸ©é˜µæ˜¯å›¾åƒï¼ŒV(k,n)è¿™ ä¸ªç©ºé—´çŸ©é˜µä¹Ÿå¯ä»¥è¢«å¯è§†åŒ–çš„è¯ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡ä¸¤å¼ å›¾æ¥æ¯”è¾ƒï¼Œå°±å¯ä»¥çœ‹å‡ºæ–°ç‰¹å¾ç©ºé—´ç©¶ç«Ÿä»åŸå§‹æ•°æ®é‡Œæå–äº† ä»€ä¹ˆé‡è¦çš„ä¿¡æ¯**ã€‚ä¾‹å¦‚äººè„¸è¯†åˆ«ä¸­æœ‰è¾ƒä¸ºå¥½çš„åº”ç”¨ã€‚

ä½†æ˜¯åœ¨çŸ©é˜µåˆ†è§£æ—¶, PCAä¸€èˆ¬åœ¨åŸæœ‰ç‰¹å¾åŸºç¡€ä¸Š, æ‰¾å‡ºèƒ½å¤Ÿè®©ä¿¡æ¯å°½é‡èšé›†çš„æ–°çš„ç‰¹å¾å‘é‡ã€‚

> [!NOTE] è¡¥å……: PCA çš„ inverse_transform éƒ¨åˆ†
> PCA å¯ä»¥é€šè¿‡ X **å³ä¹˜æ‰€æå–çš„ç‰¹å¾çŸ©é˜µ V** ç”Ÿæˆæ–°çŸ©é˜µ $X_{dr}$ åˆ™è®© $X_{dr}$ å³ä¹˜ $V(k,n)$ é€†çŸ©é˜µ $V^{-1}_{(k,n)}$ å³å¯å°† $X_{dr}$ è¿˜åŸä¸º $X$, ä½†èˆå¼ƒäº†é™ç»´åçš„éƒ¨åˆ†çš„ä¿¡æ¯ã€‚

## ä¸‰ã€Kaggle å®æˆ˜ PCA, SVD åˆ†è§£ä¸æ¨èç³»ç»Ÿ
ä» Kaggle ä¸Šè·å–æ¨èç³»ç»Ÿæ•°æ®é›† [retailrocket](https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset) è¿›è¡Œåˆ†æ
- Task 1
**When a customer comes to an e-commerce site, he looks for a product with particular properties: price range, vendor, product type and etc. These properties are implicit**, so it's hard to determine them through clicks log.

Try to **create an algorithm which predicts properties of items in "addtocart" event by using data from "view" events for any visitor in the published log**.

- Task 2
Description:
Process of analyzing ecommerce data include very important part of data cleaning. Researchers noticed that in some cases browsing data include up to 40% of abnormal traffic.

Firstly, abnormal users add a lot of noise into data and make recommendation system less effective. In order to increase efficiency of recommendation system, abnormal users should be removed from the raw data.

Secondly, abnormal users add bias to results of split tests, so this type of users should be removed also from split test data.

Goals:
- The main goal is to find abnormal users of e-shop.

Subgoals:
- Generate features
- Build a model
- Create a metric that helps to evaluate quality of the model

### (1) Retailrocket æ•°æ®é›†ä¸åŸºæœ¬åˆ†æ
é¦–å…ˆç»˜åˆ¶ä¸€äº›ç®€å•çš„å›¾åƒå¯¹æ•´ä½“è¿›è¡Œåˆ†æ:
```python
def __show_data_analysis(self):  
    eve = self.events_data['event']  
    print(eve.value_counts())  
    sns.countplot(x ='event', hue='timestamp', data=self.events_data)  
    plt.show()

# all unique visitors 
all_customers = events['visitorid'].unique()
print("Unique visitors:", all_customers.size)

# all visitors
print('Total visitors:', events['visitorid'].size)
customer_purchased = events[events.transactionid.notnull()].visitorid.unique()
customer_purchased.size

# æå–æŸä¸ªåˆ—ç­‰äºæŸä¸ªå€¼çš„éƒ¨åˆ†
cart_data  = self.events_data['event'].loc[self.events_data.event=="addtocart"]

# use "isin" to filter the column which satisfy  multiple conditions 
items_new = items.loc[items.property.isin(['categoryid', 'available']), :]
print("items with categoryid and available as propery:", items_new.size)
items_new.head(20)

# åˆ†å‰²æ•°æ®é›†çš„æŒ‡ä»¤ç¤ºä¾‹: 
X_train, X_test, y_train, y_test =  train_test_split(iris_data, iris_target, test_size=0.2, random_state=None)
# ä¹Ÿå¯ä»¥æŒ‡å®šä¸€ä¸ªé¡¹ç›®, å¦‚:
train_data, test_data = train_test_split(unique_visitors, test_size=0.2)  
print(train_data.shape, test_data.shape)

#grouping itemid by its event type and creating list of each of them (use groupby method)
grouped = events.groupby('event')['itemid'].apply(list)
```

ç»“æœå¦‚ä¸‹:
1. æœ€å¤šçš„ 10 ä¸ª view å¯¹åº”çš„ itemid :
![[attachments/Pasted image 20241118124143.png|500]]
2. æœ€å¤šçš„ 10 ä¸ª addtocart å¯¹åº”çš„ itemid : 
![[attachments/Pasted image 20241118132455.png|500]]
3.  æœ€å¤šçš„ 10 ä¸ª  transaction å¯¹åº”çš„  itemid:  
![[attachments/Pasted image 20241118132611.png|500]]

æ¨èç³»ç»Ÿçš„æ€è·¯1 æ˜¯æå–æ¯ä¸ªå®¢æˆ·ä¹°çš„ä¸œè¥¿, å¯¹äºä¹°çš„ç‰©å“æœ‰å¤šä¸ªæ—¶, åˆ™å¯¹ç›¸åº”çš„ç‰©å“æ·»åŠ ç›¸å…³æ€§, å¹¶å°†ç›¸å…³æ€§è¾ƒé«˜çš„æ¨èç»™ä¹°è¯¥ç‰©å“çš„å®¢æˆ· (ItemCF) 

éœ€è¦è¯´æ˜çš„æ˜¯, å¯¹äºç¨€ç–çŸ©é˜µçš„æ„å»º, å¦‚æœé‡‡ç”¨ä¸€ä¸ªç¨€ç–çŸ©é˜µæå–åŒ…å«è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„çŸ©é˜µ, å¯èƒ½ä¼šæ˜¾å¾—æ¯”è¾ƒéº»çƒ¦, æ­¤å¤„å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸPython/ğŸŒŸPython åŸºç¡€éƒ¨åˆ†/5. Scipy ç¨€ç–çŸ©é˜µç›¸å…³å†…å®¹|5. Scipy ç¨€ç–çŸ©é˜µç›¸å…³å†…å®¹]] è¿›è¡Œæ„å»ºç¨€ç–çŸ©é˜µ; 

å¾ªç¯ç±»åˆ«ç¼–ç æ–¹æ¡ˆ(ä¾‹å¦‚, å¯¹äºä¸€ä¸ªè¡¨æ ¼, å…¶å¤šä¸ªå±æ€§å‡éœ€è¦è¿›è¡Œç¼–ç ):
```python
train_data, test_data = train_test_split(self.events_data, test_size=0.2)  
trans_cat_train = dict()  
trans_cat_test = dict()  
  
# é‡‡ç”¨å¾ªç¯ç¼–ç æ–¹æ¡ˆ, å¯¹ visitorid å’Œ itemid è¿›è¡Œç±»åˆ«ç¼–ç ï¼š  
for k in ['visitorid', 'itemid']:  
    cate_enc = LabelEncoder()  
    trans_cat_train[k] = cate_enc.fit_transform(self.events_data[k].values)  
    trans_cat_test[k] = cate_enc.transform(test_data[k].values)  
  
"""
trans_cat_train = {  
    'visitorid': [0, 1, 2, ...],  # æ¯ä¸ª visitorid è¢«ç¼–ç æˆå”¯ä¸€æ•´æ•°  
    'itemid': [0, 1, 2, ...]     # æ¯ä¸ª itemid è¢«ç¼–ç æˆå”¯ä¸€æ•´æ•°  
}  
"""  
cate_enc = LabelEncoder()  
trans_cat_train = cate_enc.fit_transform(train_data.event)  
trans_cat_test = cate_enc.transform(test_data.event)
```

ä¸€èˆ¬è€Œè¨€åœ°, ç¨€ç–çŸ©é˜µçš„å€¼åº”è¯¥æ˜¯ **è¡Œä¸ºçš„æ¬¡æ•°ï¼ˆå¦‚ view æˆ– add_to_cartï¼‰**ï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªæƒé‡åˆ†æ•°ï¼Œè€Œä¸æ˜¯ç±»åˆ«ç¼–ç . å®é™…ä¸Šåº”è¯¥è¡¨ç¤ºç”¨æˆ·å¯¹ç‰©å“çš„äº¤äº’å¼ºåº¦ï¼Œé€šå¸¸æ¯ä¸ªå•å…ƒæ ¼æ˜¯ç”¨æˆ·å’Œç‰©å“åœ¨æŸè¡Œä¸ºä¸‹çš„æ¬¡æ•°æˆ–æƒé‡ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå›ºå®šçš„ç±»åˆ«ç¼–å·ã€‚

å¦å¤–, <b><mark style="background: transparent; color: orange">å®é™…ä¸Šå¯¹äºç©ºçš„ç‰©å“ id ä»¥åŠç©ºçš„å®¢æˆ· id, æ²¡æœ‰å¿…è¦å°†æ•´ä¸ªçŸ©é˜µå‹ç¼©ä¸ºç¨ å¯†çŸ©é˜µ, å®é™…ä¸Šå°±æŒ‰ç…§ item id  å’Œ  user id è¿›è¡Œç›´æ¥çš„å»ºç«‹å³å¯</mark></b>.
**ä»£ç é€»è¾‘è¯¦è§£**
1. **ç»Ÿè®¡è¡Œä¸ºæ¬¡æ•°**  
    `events_view.groupby(['visitorid', 'itemid']).size()` ä¼šç»Ÿè®¡æ¯ä¸ª `(visitorid, itemid)` ç»„åˆå‘ç”Ÿçš„ `view` æˆ– `add_to_cart` æ¬¡æ•°ã€‚    
2. **ç¼–ç ç”¨æˆ·å’Œç‰©å“**  
    ä½¿ç”¨ä¹‹å‰çš„ `trans_cat_train` å­—å…¸ï¼Œå°† `visitorid` å’Œ `itemid` è½¬æ¢ä¸ºæ•°å€¼ç¼–ç ã€‚
3. **æ„é€ ç¨€ç–çŸ©é˜µ**
    - ç¨€ç–çŸ©é˜µçš„å€¼æ˜¯ç»Ÿè®¡å‡ºçš„è¡Œä¸ºæ¬¡æ•°ã€‚
    - è¡Œæ˜¯ `visitorid`ï¼Œåˆ—æ˜¯ `itemid`ã€‚

### (2) ç”¨æˆ·ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ¡ˆ
æ˜¾ç„¶, å¯¹äº UserCF çŸ©é˜µæ•´ä½“, æˆ‘ä»¬éš¾ä»¥ç›´æ¥æ„é€ , å¾€å¾€ä¼šé‡‡ç”¨ä¸€äº›æŠ€æœ¯å‡å°‘ç›¸ä¼¼åº¦çŸ©é˜µçš„è®¡ç®—é‡

ä¾‹å¦‚, å¯¹äºä¸€ä¸ªæ•ˆç‡æä½çš„ç®—æ³•, æ˜¯æŒ‰ç…§å¾ªç¯è¿›è¡Œèµ‹å€¼çš„, å¯¹äº 100000 é‡çº§çš„çŸ©é˜µå‡ ä¹æ²¡æœ‰æ±‚è§£èƒ½åŠ›, å¦‚ä¸‹æ‰€ç¤º:
```python
def get_usrCF(self, sim_mat):  
    """  
    get the  userCF matrix by similarity matrix    """    userCF_mat = lil_matrix((sim_mat.shape[0], sim_mat.shape[0]), dtype=np.float32)    # user x user  
    for i in range(sim_mat.shape[0]):  
        for j in range(sim_mat.shape[0]):  
            if j == i:  
                userCF_mat[i,j] = 1  
                continue  
            # calculate the similarity of user only if the overlap elements is greater than 0  
            idx_i = sim_mat[i, :].nonzero()[1]  
            idx_j = sim_mat[j,: ].nonzero()[1]  
            if np.intersect1d(idx_i, idx_j).size != 0:  
                vec1 = sim_mat[i,:]  
                vec2 = sim_mat[j,:]  
                userCF_mat[i,j] = np.dot(vec1, vec2.T).data/ np.sqrt(np.dot(vec1, vec1.T) * np.dot(vec2,vec2.T).data)  
    return userCF_mat
```

è€Œå¯¹äº retail market çš„è®­ç»ƒæ•°æ®, å®é™…çš„ç”¨æˆ·é‡çº§è¾¾åˆ°äº† 140 ä¸‡ä»¥ä¸Š, å®é™…å»ºç«‹çš„ç¨€ç–çŸ©é˜µå¤§å°åœ¨ 140w * 140 w å·¦å³, å› æ­¤æˆ‘ä»¬å¿…é¡»è€ƒè™‘æ›´åŠ é«˜æ•ˆçš„è®¡ç®—æ–¹å¼:

#### 1. è‡ªé€‚åº” KMeans èšç±»ç®—æ³•çš„é€‚ç”¨æ€§
åŒæ—¶, å³ä½¿æ˜¯åœ¨ 140w æƒ…å†µä¸‹è¿›è¡Œå•å¾ªç¯, æ¶ˆè€—æ—¶é—´ä¹Ÿæ˜¯æƒŠäººçš„, å› æ­¤, æˆ‘ä»¬å°½å¯èƒ½å‡å°‘è®¡ç®—æ¬¡æ•°, **å¯ä»¥è€ƒè™‘å…ˆé‡‡ç”¨ KMeans  [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/4. KNNç®—æ³•å’ŒKMeansèšç±»ç›¸å…³ç®—æ³•|4. KNNç®—æ³•å’ŒKMeansèšç±»ç›¸å…³ç®—æ³•]] è‡ªé€‚åº”èšç±»** (å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/4. KNNç®—æ³•å’ŒKMeansèšç±»ç›¸å…³ç®—æ³•#ä¸‰ã€è‡ªé€‚åº” KMeans èšç±»æ–¹æ¡ˆ|è‡ªé€‚åº” KMeans èšç±»æ–¹æ¡ˆ]]å®ç°), å°†ç”¨æˆ·åˆ†ä¸ºå¤šä¸ªç”¨æˆ·ç»„; ç„¶åä»ç»„ä¸­è·å–ç›¸ä¼¼åº¦è®¡ç®—ç»“æœã€‚

ä½†æ˜¯, å®é™…ä¸Š<mark style="background: transparent; color: red">èšç±»ä¹‹åç»è¿‡ç»“æœæ£€éªŒ, ç»å¤§å¤šæ•°ç”¨æˆ· (çº¦ 95%, å‡æ˜¯èšç±»åˆ°ç±»åˆ« 0 ä¸­</mark>, è¿™ç›¸å¯¹äºèšç±»å¯»ä¼˜è€Œè¨€, å®é™…ä¸Šæ²¡æœ‰å¢åŠ å¤šå°‘è¿ç®—é‡, åè€Œå¢åŠ äº†å®é™…è®¡ç®—çš„æ—¶é—´æˆæœ¬, å› æ­¤æ”¾å¼ƒç›´æ¥è‡ªé€‚åº”èšç±»çš„æ–¹æ¡ˆ), å› æ­¤å®é™…ä¸Šå¯ä»¥è€ƒè™‘ç›´æ¥è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µã€‚

éœ€è¦è¯´æ˜çš„æ˜¯, å¯¹äºç»´åº¦æå¤§çš„çŸ©é˜µ, æˆ‘ä»¬ä¸€èˆ¬é‡‡ç”¨<b><mark style="background: transparent; color: blue">é™ç»´ä¹‹åèšç±»çš„æ–¹å¼</mark></b>:
- é«˜ç»´ç¨€ç–çŸ©é˜µï¼ˆå°¤å…¶æ˜¯ç”¨æˆ·-ç‰©å“åå¥½çŸ©é˜µï¼‰é€šå¸¸å«æœ‰å¤§é‡çš„ 0ï¼Œç›´æ¥åœ¨é«˜ç»´ç©ºé—´ä¸­èšç±»ä¼šä½¿è®¸å¤šç®—æ³•ï¼ˆä¾‹å¦‚ K-Meansï¼‰éš¾ä»¥æœ‰æ•ˆåº¦é‡ç›¸ä¼¼æ€§ï¼Œå› ä¸ºåœ¨é«˜ç»´ç¨€ç–æ•°æ®ä¸­ï¼Œç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»é€šå¸¸ä¼šè¶‹äºç›¸ç­‰ã€‚ åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œè·ç¦»åº¦é‡ï¼ˆå¦‚æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰å¯èƒ½å¤±æ•ˆï¼Œå› ä¸ºæ•°æ®ç‚¹çš„ç›¸å¯¹è·ç¦»å·®å¼‚ä¼šå˜å°ï¼ˆå³æ‰€è°“çš„â€œç»´åº¦è¯…å’’â€ï¼‰
- PCA ç­‰é™ç»´æ–¹æ³•æå–äº†çŸ©é˜µçš„ä¸»è¦ä¿¡æ¯ï¼ˆé€šè¿‡æ•è·ä¸»æˆåˆ†ï¼‰ï¼Œä»è€Œå°†é«˜ç»´ç¨€ç–çŸ©é˜µè½¬åŒ–ä¸ºä½ç»´å¯†é›†çŸ©é˜µï¼Œèšç±»ç®—æ³•å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ›´å¥½åœ°å·¥ä½œã€‚

#### 2. ç¨€ç–ç”¨æˆ·ç›¸ä¼¼çŸ©é˜µ UserCF çš„å»ºç«‹å’Œè®¡ç®—
ç›¸ä¼¼åº¦çš„å¿«é€Ÿè®¡ç®—æ–¹æ³•åŒ…æ‹¬:
1. é‡‡ç”¨ scipy.distance ç›´æ¥è®¡ç®—è·ç¦», é€‚ç”¨äºçŸ©é˜µä¸å¤§çš„æƒ…å†µ
2. æ‰¾ sklearn çš„ pairwise åº“, å…¶ä¸­èƒ½å¤Ÿè®¡ç®—è¾ƒå¤§è§„æ¨¡çš„ç¨€ç–çŸ©é˜µ
```python 
from scipy.spatial import distance
from sklearn.metrics.pairwise import cosine_similarity
```
å¯¹äº cosine_similarity è®¡ç®—å¤§è§„æ¨¡ç¨€ç–çŸ©é˜µ, å¯¹äº 30000 å·¦å³è§„æ¨¡çš„çŸ©é˜µæ•ˆæœæ˜¯éå¸¸å¥½çš„; è€Œå¯¹äº 1000000 å¤§å°çŸ©é˜µä¼šåˆ†é…çº¦ 5TB ç©ºé—´, æ— æ³•è¿›è¡Œåˆ†é….

å› æ­¤æˆ‘ä»¬é‡‡ç”¨ PCA é™ç»´å’Œé™ç»´ä¹‹åèšç±»çš„æ–¹å¼;
é¦–å…ˆ, ==**ç¨€ç–çŸ©é˜µæ˜¯ä¸æ”¯æŒç›´æ¥ PCA çš„, å¦‚æœå¿…é¡»ä½¿ç”¨ PCAï¼Œè¾“å…¥éœ€è¦å…ˆè½¬ä¸ºç¨ å¯†çŸ©é˜µ**==ï¼ˆè¿™å¯èƒ½ä¼šæ¶ˆè€—å¤§é‡å†…å­˜ï¼‰ï¼Œç„¶åæŒ‡å®š `svd_solver='arpack'`ã€‚

å®é™…ä¸Šå¯¹äºç¨€ç–çŸ©é˜µ, æ›´åŠ æ–¹ä¾¿çš„æ–¹æ³•æ˜¯ `TruncateSVD` , å³é‡‡ç”¨:
```python
from sklearn.decomposition import TruncatedSVD 
```
å®é™…ä¸Šæ˜¯é‡‡ç”¨ SVD åˆ†è§£æå–å…¶ä¸­çš„ä¸»æˆåˆ†éƒ¨åˆ†, å®é™…ä¸Šæ•ˆç‡æ›´åŠ é«˜æ•ˆ;

SVD åˆ†è§£:  é€šè¿‡ `scipy.sparse.linalg.svds` æˆ– `sklearn.decomposition.TruncatedSVD` è¿›è¡Œåˆ†è§£ï¼š
`svds`Â æ˜¯ç”¨äºè®¡ç®—ç¨€ç–çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSingular Value Decomposition, SVDï¼‰çš„å‡½æ•°ï¼Œå®ƒæ¥è‡ªÂ `scipy.sparse.linalg`Â æ¨¡å—ã€‚å‡½æ•°çš„ä¸»è¦å‚æ•°ä¹‹ä¸€æ˜¯Â `k`ï¼Œå®ƒä»£è¡¨è¦è®¡ç®—çš„å¥‡å¼‚å€¼çš„æ•°é‡ã€‚

æ ¹æ® [TruncatedSVD ç®€ä»‹](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), å…¶  arpack æ˜¯ä¸€ç§æœ´ç´ çš„ç®—æ³•, ä½†æ˜¯é€Ÿåº¦å¾ˆæ…¢, ä¸é€‚åˆå¤§è§„æ¨¡ç¨€ç–çŸ©é˜µè¿ç®—è¿‡ç¨‹,  å› æ­¤å¾€å¾€é‡‡ç”¨ randomized ç®—æ³•å¤„ç† 100w ä»¥ä¸Šçš„ç¨€ç–çŸ©é˜µ;

æ ¹æ®ä¸€èˆ¬ç»éªŒ, é™ç»´çš„ `n_components` é€‰ç”¨ä¸º 50-300 ä¸ºåˆé€‚, è€Œ `n_components` è¶Šå¤§, è¶Šè€—æ—¶å’Œæ¶ˆè€—è®¡ç®—å†…å­˜ã€‚
ä¸€èˆ¬å¤§è§„æ¨¡çŸ©é˜µé€‰ç”¨ 50 å³å¯, è€Œæœ€åå°†ç”¨æˆ·èšç±»ä¸º 100 ç±», ä¹‹ååªéœ€åœ¨ç±»å†…ç»†åˆ†;

#### 3. MiniBatchKMeans å’ŒåŸºäºæ¬§æ°è·ç¦»çš„ UserCF èšç±»
åœ¨ `MiniBatchKMeans` ä¸­ï¼Œ<mark style="background: transparent; color: red">é»˜è®¤çš„è·ç¦»åº¦é‡æ˜¯æ¬§å‡ é‡Œå¾—è·ç¦»</mark>ã€‚å¦‚æœè¦ä½¿ç”¨ä½™å¼¦è·ç¦»ï¼ˆcosine distanceï¼‰è¿›è¡Œèšç±»ï¼Œå¯ä»¥é€šè¿‡è‡ªå®šä¹‰ `MiniBatchKMeans` çš„è·ç¦»åº¦é‡æ¥å®ç°ã€‚é—æ†¾çš„æ˜¯ï¼Œ`MiniBatchKMeans` æœ¬èº«å¹¶æ²¡æœ‰ç›´æ¥æ”¯æŒä½™å¼¦è·ç¦»ä½œä¸ºè·ç¦»åº¦é‡ã€‚å› æ­¤éœ€è¦æˆ‘ä»¬è¿›è¡Œè‡ªå·±å®ç°åŸºäºä½™å¼¦è·ç¦»çš„èšç±»;

ä½†æ˜¯ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ç»è¿‡ PCA(TruncatedSVD) é™ç»´ä¹‹åçš„çŸ©é˜µ, å…¶ Item ç‰¹å¾å·²ç»ç¨ å¯†, æ­¤æ—¶é‡‡ç”¨æ¬§å¼è·ç¦»æ˜¯æ›´å¥½çš„é€‰æ‹©; 
```python
def __get_pref_mat(self, usr_mat):  
    """  
    get the new preference matrix (new User-Item matrix) based on sparse PCA and Minibatch KMeans clustering    
    this must be called in train process    
    :param usr_mat: usr preference mat (storge the user and item which has operated)    
    :return:  
        model:  the model of the sparse PCA and KMeans (transformed data should use this model)        
        usr_pref_mat: user preference matrix        usr_labels: the cluster labels of the user   
    """
model = TruncatedSVD(n_components=self.features, algorithm='randomized')  
    pca_mat_train = model.fit_transform(usr_mat)  
  
    # MiniBatchKMeans with precomputed cosine distance matrix  
    mb_clusters = MiniBatchKMeans(n_clusters=self.cluster_number, batch_size=self.kmeans_batch_size,  
                                  random_state=42)  
    mb_clusters.fit(pca_mat_train)  # we finally use the cosine distance for fit the model  
    usr_pref_mat = mb_clusters.cluster_centers_  # pref_mat is the cluster center  
    usr_labels = mb_clusters.labels_  
    if self.ouput_info:  
        """ print detailed infomation of the clustering model """  
        print("========= model build succeed =============")  
        show_cluster_counters(usr_labels, show_detailed=False)  
        print("cluster inertia :", mb_clusters.inertia_)  
    return model, usr_pref_mat, usr_labels
```

å…¶ä¸­ æˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªç®€å•çš„å‡½æ•°æ£€æŸ¥æ¯ä¸ªèšç±»çš„ç»“æœ, å®é™…ä¸Šæˆ‘ä»¬åªéœ€æœ€åæ£€æŸ¥èšç±»ç»“æœå°äºæŸä¸€é˜ˆå€¼(å¦‚ = 1) 
```python
def show_cluster_counters(self,labels, cnt_per_row = 3):  
    unique_labels, cnt = np.unique(labels, return_counts=True)  
    idx = 0  
    for (l, c) in zip(unique_labels, cnt):  
        print("num of ",l ,":",c, end=" ")  
        if idx % cnt_per_row == 0:  
            print()  
        idx = idx + 1
    print("signle element clusters number", np.sum(cnt[cnt==1]))
```

å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/5. è¿‘ä¼¼æœ€é‚»è¿‘ç®—æ³•(kdtree,Faiss,Annoy, ScaNN)|5. è¿‘ä¼¼æœ€é‚»è¿‘ç®—æ³•(kdtree,Faiss,Annoy, ScaNN)]], é‡‡ç”¨å¦‚ä¸‹è®¡ç®— Transform ä¹‹åçš„æ¯ä¸€é¡¹çš„è·ç¦», å¹¶èšç±»åˆ°å¯¹åº”çš„ User ç»„ä¸­.
```python
def __nearest_index(self, mat_from, mat_target, metric = 'euclidean', use_ANN=True):  
    """  
    We use the nearlest Neibour to calculate the nearest index of the matrix,    note: the input are all dense matrix, but the second dim shouldn't too large    :param mat_from: the matrix to be compared    :param mat_target: the matrix to be compared    :param metric: the metric to be used, default is cosine    :param use_ANN: use Approximate Nearest Neighbor Algorithm,           much faster when the matrix to calculate nearlest is large    :return: the index of the nearest matrix  
    """    
    if not use_ANN:  
        # calculate the cosine similarity for each user in the new data  
        dist = distance.cdist(mat_from, mat_target, metric=metric)  
        min_dist_idx = np.argmin(dist, axis=1)  
    else:  
        # Approximate Nearest Neighbor using Faiss  
        if metric == 'cosine':  
            # Normalize the vectors in row direction for cosine similarity  
            mat1_normalized = mat_from / np.linalg.norm(mat_from, axis=1, keepdims=True)  
            mat2_normalized = mat_target / np.linalg.norm(mat_target, axis=1, keepdims=True)  
            index = faiss.IndexFlatIP(mat_target.shape[1])  # Inner Product by line direction  
        elif metric == 'euclidean':  
            mat1_normalized = mat_from  
            mat2_normalized = mat_target  
            index = faiss.IndexFlatL2(mat_target.shape[1])  # L2 distance  
        else:  
            raise ValueError("Currently, only 'cosine' and 'euclidean' metric is supported for ANN.")  
        # Add reference data to the index  
        index.add(mat2_normalized.astype(np.float32))  
  
        # Perform nearest neighbor search  
        _, min_dist_idx = index.search(mat1_normalized.astype(np.float32), 1)  
        min_dist_idx = min_dist_idx.flatten()  
    return min_dist_idx
```

åªéœ€è¦é‡‡ç”¨å¦‚ä¸‹æ–¹æ³•éªŒè¯:
```python
label_test = model.fit_transform(trans_data_test.visitorid, trans_data_test.itemid, item_max=self.itemid_max)  
label_validation = model.transform(trans_data_test.visitorid, trans_data_test.itemid, use_ANN=True)  
print("validation correction: ", np.sum(label_validation == label_test) / len(label_test)) 
>>> validation correction:  1.0   # å¯¹åŸå§‹é›†å…¨éƒ¨æ­£ç¡®èšç±»
```

æœ€ç»ˆå¾—åˆ°çš„ cluster_center ä½œä¸ºæ–°çš„ç”¨æˆ·åå¥½çŸ©é˜µ. å³ä¸ºæˆ‘ä»¬éœ€è¦çš„ç”¨æˆ·åˆ†ç±»çŸ©é˜µ.æ­¤ååªéœ€è¦å¯¹æ–°çš„æ•°æ®è¿›è¡Œåˆ†ç±».
> [!NOTE] è¡¥å……
> åœ¨èšç±»æ—¶, æˆ‘ä»¬å¸Œæœ›æ–¹ä¾¿åœ°è®¡ç®—ä¸€ä¸ªçŸ©é˜µå‘é‡åˆ°å¦ä¸€ä¸ªçŸ©é˜µå‘é‡çš„æ‰€æœ‰è·ç¦», æ­¤æ—¶å¯é‡‡ç”¨
> scipy.spatial.distance ä¸­çš„ `distance.cdist` å‡½æ•°:
> `dist_matrix = distance.cdist(pref_mat_data, self.pref_mat, metric='cosine')` 
> æ¬§å‡ é‡Œå¾—è·ç¦»åˆ™é‡‡ç”¨ eculidian
> è¿™ä¸ªå‡½æ•°å·²ç»é«˜åº¦ä¼˜åŒ–å¹¶é€‚ç”¨äº 3000 å·¦å³çš„ä¸­è§„æ¨¡çŸ©é˜µ. å¦å¤–å¯¹äºè¶…å¤§è§„æ¨¡çŸ©é˜µï¼Œå¯ä»¥è€ƒè™‘è¿‘ä¼¼æœ€é‚»è¿‘ç®—æ³•ã€‚

### (3) ç»“åˆ UserCF çŸ©é˜µå’Œ ItemCF çŸ©é˜µçš„æ¨èç®—æ³•ä¼˜åŒ– 
æˆ‘ä»¬ä¸Šé¢ç¡®å®šäº†è¯¥é—®é¢˜æ±‚è§£çš„ä¸€èˆ¬æ€è·¯: <mark style="background: transparent; color: red">é€šè¿‡ UserCF çŸ©é˜µ, é™ç»´å¹¶ç­›é€‰å‡ºç”¨æˆ·æ‰€åœ¨çš„åˆ†ç»„; </mark> 
ä½†æ˜¯, è™½ç„¶ä¸Šè¿°æ–¹æ³•å¾ˆæ–¹ä¾¿åœ°ç­›é€‰å‡ºäº†åˆ†ç»„, æœ‰æ•ˆå‡å°‘äº†ç”¨æˆ·åˆ†ç»„çš„æ•°é‡, ä½†æ˜¯æ ¹æ®æ–°ç”Ÿæˆçš„ PCA é™ç»´åçš„çŸ©é˜µ, ç”±äºç‰©å“ Item äº¤äº’ä¿¡æ¯è¢« PCA é™ç»´å¹¶æŠ¹å», æ— æ³•å¾—çŸ¥å“ªä¸€ç±»ç”¨æˆ·å…·ä½“å–œå¥½å“ªä¸€ç±»ç‰©å“. è¿‡æ»¤ç›¸ä¼¼ç‰©å“çš„æˆæœ¬, å®é™…ä¸Šæ˜¯å¾ˆé«˜çš„; 

ä½†æ˜¯, å¯ä»¥é€šè¿‡ç”¨æˆ·åˆ†ç»„ç¼©å°æˆ‘ä»¬æ‰€æ±‚è§£çš„ç‰©å“èŒƒå›´, ä»è€Œç¡®å®šæ€»ä½“æ¶æ„å’Œæ±‚è§£æ€è·¯:s

#### 1. **UserCF ç¨€ç–çŸ©é˜µé™ç»´èšç±»**
2æ‰¾åˆ°ç›¸ä¼¼ç”¨æˆ· : é‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æˆ–åŸºäºç¨€ç–çŸ©é˜µä¼˜åŒ–çš„å¿«é€Ÿæ–¹æ³•, è¿›è¡Œé™ç»´å’Œèšç±»,  å°†ç›®æ ‡ç”¨æˆ·åˆ†ä¸ºä¸åŒçš„ç±»åˆ«, è®¡ç›®æ ‡ç”¨æˆ· $(u)$ çš„ä¸€ç»„ç›¸ä¼¼ç”¨æˆ· $( U_{sim}(u) )$, å¹¶é€šè¿‡èšç±», å¤§å¤§å‡å°‘å®é™…æ€»ç”¨æˆ·ç§ç±»æ•°; 
> [!caution] è¯´æ˜
> éœ€è¦è¯´æ˜çš„æ˜¯, åœ¨ç¬¬ä¸€é˜¶æ®µèšç±»æ—¶, èšç±»ä¸­å¿ƒä¸è¦è¿‡å¤š, è¿™ä¼šå¯¼è‡´å¾ˆå¤šå•ä¸ª User è¢«åˆ†ä¸ºä¸€ç±», ä»è€Œå¯¼è‡´ä¹‹åç­›é€‰å‡ºç°é—®é¢˜; å› æ­¤ä¸€èˆ¬ 8000 å·¦å³çš„ç”¨æˆ·, èšç±»ç±»åˆ«æ”¾åœ¨ 100 å·¦å³æ˜¯åˆé€‚çš„; åŒæ—¶æ§åˆ¶èšç±»å†…ç”¨æˆ·æ•°é‡ä¸è¦å¤ªå¤š. ä¸€èˆ¬æœ€å¤§èšç±»ä¸­çš„ç”¨æˆ·æ•°é‡ < 500 - 1500 ä¸ºåˆé€‚;

#### 2. **ç”¨æˆ·åˆ†ç»„, ItemCF å’Œè¿‘ä¼¼æœ€é‚»è¿‘ç®—æ³•è¿‡æ»¤**
åŸºæœ¬çš„æ€è·¯å¯¹ç›¸ä¼¼ç”¨æˆ·çš„äº¤äº’æ•°æ®åˆ†ç»„, å¹¶åœ¨ç»„å†…è¿‡æ»¤å…¶å…±åŒç‰©å“
æˆ‘ä»¬é¦–å…ˆæ ¹æ® UserCF çš„èšç±»ç»“æœ, æå–ç›¸ä¼¼ç”¨æˆ·ç»„, è€Œ<mark style="background: transparent; color: red">ä»…ä»…é’ˆå¯¹ç›¸ä¼¼ç”¨æˆ·ç»„å†…çš„ç‰©å“, è¿›è¡Œ ItemCF åˆ†æ</mark>;

è¿™æ ·åšçš„ä¼˜ç‚¹æœ‰å¦‚ä¸‹å‡ æ¡: 
- é¦–å…ˆå¯ä»¥**è¿‡æ»¤æ‰å¤§é‡çš„å…¶ä»–ç‰©å“**, ä»…åœ¨ç”¨æˆ·ç»„ä¸­èšç±», å¤§å¤§æé«˜äº† ItemCF çš„æ•ˆç‡;
- å…¶ä¸­ä¸€ä¸ªé—®é¢˜æ˜¯, <b><mark style="background: transparent; color: orange">ä¸åŒçš„ç”¨æˆ·ç»„çš„ç‰©å“ç»„ä¼šæœ‰é‡å , ä½†æ˜¯é‡æ–°è®¡ç®—è¿™ä¸ªç›¸ä¼¼åº¦æ˜¯åˆç†çš„;</mark></b> å› ä¸ºä¸åŒç”¨æˆ·ç»„å¾€å¾€ä¼šæœ‰ç›¸åŒçš„æ¨èç‰©å“, è€Œæ ¹æ®ç”¨æˆ·ç»„è¿›è¡Œæ¨èç‰©å“å®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„æ•ˆæœã€‚ä¸Šé¢çš„èšç±»å°±å®ç°äº†è¿™ä¸€ç‚¹ã€‚
- ä¸Šè¿°æ–¹æ³•å®é™…ä¸Š<mark style="background: transparent; color: red">æ¨èæ˜¯æŒ‰ç…§ç”¨æˆ·ç»„åˆ«å†…è¿›è¡Œæ¨èçš„</mark>, è€Œç»„åˆ«å†…çš„æ¨è, æ˜¯æŒ‰ç…§ç”¨æˆ·è´­ä¹°çš„ç‰©å“è¿›è¡Œç»„åˆ«å†…å…³è”æ¨èçš„<b><mark style="background: transparent; color: orange">(å³ä¸€ä¸ªç”¨æˆ·ä¹°äº†æŸä¸ªç‰©å“, åˆ™ä¼šæ¨èæ­¤ç”¨æˆ·åˆ†ç»„å†…å…¶ä»–å…³è”æ€§æœ€é«˜çš„ç‰©å“)</mark></b>, è¿™æ ·æœ‰æ•ˆåœ°è¾¾åˆ°äº†ç»†åŒ–æ¨èæ•ˆæœã€‚
- å¯¹äºè®­ç»ƒé›†ä¸­æœ‰æ–°ç‰©å“çš„æƒ…å†µ, ä»ç„¶å¯ä»¥é€šè¿‡è®­ç»ƒé›†ä¸­å…¶ä»–æ–°ç‰©å“çš„å…³è”æ€§å¯»æ‰¾ç”¨æˆ·

ç”±äº ItemCF éƒ¨åˆ†çš„çŸ©é˜µä¸æ˜¯éå¸¸å¤§, å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/5. è¿‘ä¼¼æœ€é‚»è¿‘ç®—æ³•(kdtree,Faiss,Annoy, ScaNN)|5. è¿‘ä¼¼æœ€é‚»è¿‘ç®—æ³•(kdtree,Faiss,Annoy, ScaNN)]] è¿›è¡Œè¿‡æ»¤, (å®é™…ä¸Šæ˜¯ä¸¤å±‚è¿‡æ»¤, è€Œç”±äºç¬¬ä¸€å±‚çš„è¿‡æ»¤è§„æ¨¡åœ¨ 400-600 å·¦å³, ä½†æ˜¯å®é™…ä¸Šä»ç„¶æ˜¯ç¨€ç–çŸ©é˜µ); æˆ‘ä»¬å¸Œæœ›ç›´æ¥ä¸€å±‚è¿‡æ»¤å¾—åˆ° k ä¸ªç›¸ä¼¼ç‰©å“ï¼Œä½†æ˜¯ Faiss æ˜¯ä¸æ”¯æŒç¨€ç–çŸ©é˜µçš„, å› æ­¤<mark style="background: transparent; color: red">æˆ‘ä»¬å¯ä»¥è€ƒè™‘ç›´æ¥é‡‡ç”¨ cosine_similarity ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—</mark>(å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/5. è¿‘ä¼¼æœ€é‚»è¿‘ç®—æ³•(kdtree,Faiss,Annoy, ScaNN)|5. è¿‘ä¼¼æœ€é‚»è¿‘ç®—æ³•(kdtree,Faiss,Annoy, ScaNN)]]); å¦å¤–, ä¹Ÿå¯ä»¥é‡‡ç”¨ KNN, MiniBatchKMeans ç­‰æ–¹æ³•è¿›è¡Œè¿‡æ»¤å¾—åˆ°ç›¸åº”çš„ itemdict, ä¸ºåˆ†ç»„å†…çš„æœ€ç›¸ä¼¼ç‰©å“

#### 3. è¯„ä¼°å®é™…è®¡ç®—ç»“æœå¹¶è¿›è¡Œé¢„æµ‹
é¦–å…ˆ, ç›®å‰å·²å¾—åˆ°äº†æ¯ä¸ªç”¨æˆ·ç»„çš„ç‰©å“ç›¸ä¼¼çŸ©é˜µ similar_item_dict, è¿™ä¸ªçŸ©é˜µè¡¨å¾äº†åœ¨ç”¨æˆ·ç»„å†…, å’Œç‰©å“ v æœ€ç›¸ä¼¼çš„ k ä¸ªç‰©å“,  ä½†æ˜¯ä»ç„¶æœ‰é—®é¢˜éœ€è¦è§£å†³:

1. å¦‚ä½•é€šè¿‡æ•°æ®é¢„æµ‹æ‰€å»ºç«‹ itemCF çŸ©é˜µçš„æ­£ç¡®æ€§, å³å¦‚ä½•ç”¨è®­ç»ƒé›†è¡¡é‡ ItemCF çŸ©é˜µæ˜¯å¦å‡†ç¡®
2. è€ƒè™‘ä»ç»“åˆæˆ‘ä»¬åœ¨ itemCF ä¸­å»ºç«‹çš„å­—å…¸, ä» view é¢„æµ‹ addtocart. æ˜¾ç„¶æ¯ä¸ª addtocart éƒ½ä¼šæœ‰å¯¹åº”çš„ View, ç›´æ¥ç›²ç›®é¢„æµ‹ä¸å¯è¡Œ.

æˆ‘ä»¬å…ˆè§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜:
é¦–å…ˆ, æˆ‘ä»¬å°† View æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†, éœ€è¦è¯´æ˜çš„æ˜¯, æˆ‘ä»¬è¿™é‡Œä»…ä»…é‡‡ç”¨éƒ¨åˆ†ç‹¬ç«‹ç‰©å“å»å»ºç«‹, è®­ç»ƒæ—¶é‡‡ç”¨ 80% æ•°æ®é›†è¿›è¡Œè®­ç»ƒ; è€Œé‡‡ç”¨å¦å¤– 20% çš„éƒ¨åˆ†å»è¡¡é‡å‡†ç¡®æ€§ã€‚

é¦–å…ˆè€ƒè™‘, å¦‚æœè®­ç»ƒä¸­æ¯ä¸ªç”¨æˆ·ä»…æå–å…¶äº¤äº’ç‰©å“çš„ä¸€éƒ¨åˆ†, å¦‚æœæ˜¯é‡‡ç”¨éƒ¨åˆ†åšè®­ç»ƒé›†ï¼Œ è‚¯å®šä¼šå‡ºç°æœ‰çš„ç‰©å“ç›¸ä¼¼åº¦ç›´æ¥æ‰¾ä¸åˆ°çš„é—®é¢˜ï¼Œ å»ºç«‹çš„ ItemCF æ˜¯ä¸å®Œæ•´, ä¸å‡†ç¡®çš„ã€‚å› æ­¤å®é™…ä¸Š<mark style="background: transparent; color: red">å¯ä»¥é‡‡ç”¨ç›´æ¥åˆ’åˆ†çš„æ–¹æ³•å»åšè®­ç»ƒé›†</mark>, ä½†å¦‚æœç›´æ¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå°†è®­ç»ƒé›†ä¸­çš„ç”¨æˆ·äº¤äº’ç‰©å“å…¨éƒ½æå–è‚¯å®šæ˜¯ä¸åˆç†çš„, å› ä¸ºè¿™æ ·ä¼šå¯¼è‡´æµ‹è¯•é›†ä¸­å¯èƒ½å…¨éƒ½è¢«é€‰è¿‡, æ²¡æœ‰ä»»ä½•å¯é¢„æµ‹çš„ç‰©å“ã€‚

æˆ‘ä»¬å¯ä»¥ä»¥æ•´ä¸ªæ•°æ®é›†ä½œä¸ºè®­ç»ƒé›†, æ„å»º ItemCF çŸ©é˜µï¼Œè€Œåœ¨è¯„ä¼°é˜¶æ®µå¯¹ç”¨æˆ·çš„äº¤äº’è®°å½•è¿›è¡Œåˆ†å‰²ï¼ˆéƒ¨åˆ†ä½œä¸ºè¾“å…¥ï¼Œéƒ¨åˆ†ä½œä¸ºç›®æ ‡ï¼‰æ¥éªŒè¯æ¨¡å‹çš„é¢„æµ‹æ•ˆæœï¼Œå¯ä»¥æœ‰æ•ˆåœ°è§£å†³æ··æ‚é—®é¢˜ï¼ŒåŒæ—¶é¿å…å› æ•°æ®ç¨€ç–å¯¼è‡´çš„å†·å¯åŠ¨æ•ˆåº”ã€‚

æ³¨æ„: test_item_size ä¸è¦è®¾ç½®å¤ªå°, å› ä¸ºè®¾ç½®å¤ªå°ä¼šä½¿å¾—æ¨¡å‹çš„é¢„æµ‹é›†ä¸­åœ¨æ›´å°‘çš„ç›®æ ‡ä¸Šï¼Œå°½ç®¡æ€»ä½“é¢„æµ‹ç»“æœçš„æ•°é‡ï¼ˆ`tot_rec_itm_num`ï¼‰å¯èƒ½ä¸å˜ï¼Œä½†æ­£ç¡®å‘½ä¸­çš„æ•°é‡ï¼ˆ`hit_itm_num`ï¼‰ä¹Ÿå¯èƒ½å‡å°‘ã€‚ 

æœ€ç»ˆå¾—åˆ°çš„ç»“æœå¦‚ä¸‹: 
é¢„æµ‹å‘½ä¸­ç‡çº¦ä¸º 80%, è€Œé¢„æµ‹çš„å æ¯”ä¸º 14.8% å·¦å³
![[attachments/Pasted image 20241124090503.png|500]]

