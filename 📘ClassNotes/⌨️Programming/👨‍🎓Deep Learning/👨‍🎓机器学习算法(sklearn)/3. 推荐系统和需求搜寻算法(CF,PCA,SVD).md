## 一、需求搜寻算法简介 
推荐系统的部分需要对于需求的深入理解，如果仅基于传统的搜索引擎算法，关键赓续哟啊的字查询信息难以定位需求。而实际上用户一般更需要的是符合个人偏好的结果。

一般的推荐系统的主要功能包括 : 
1. **打包销售(Cross Selling)**:即在推荐某些产品时, 也会同时推荐一同购买的产品
2. **协同过滤**: 主要是定位用户的购买需求。基于模糊关键字从列表中明确需求。此外也包括看过此商品之后购买的其它商品。反映了隐式需求(曾经浏览过某页面的用户所购买的产品)。
3. 用户商品评论列表的抽和汇总分析, 用于评估产品质量的分布水平。

推荐系统的主要算法包含 : 
1. 基于人口统计学的推荐机制 : 根据用户的基本信息获取相关程度, 并根据相似用户喜好的的其他物品推荐给当前用户。
2. 基于协同过滤的推荐机制 : 主要分为<b><mark style="background: transparent; color: blue">基于用户的推荐和基于项目的推荐</mark></b>。
3. 基于隐语义的推荐模型 : 其中, **目前精度最高的算法是 SVD 隐语义模型**。

一般在训练阶段, 属于 CPU 密集型任务, 而在分类或者预测阶段属于 IO 密集型任务。 一般不使用脚本语言进行设计。

协同过滤的模以及算法包含: 
1. 数据处理与 UI 矩阵 
2. UserCF 和 ItemCF 推荐模型
3. KMeans 相似性计算 
4. SVD 相似性计算  

### (1) 物品数据的 KMeans 聚类
KMeans 聚类参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/4. KNN算法和KMeans聚类相关算法#二、K-Means算法(K均值算法)|KMeans算法]] , 并给出一个简单的 iris 数据集 KMeans 聚类代码:
```python
import sklearn  
from sklearn.datasets import load_iris  
from sklearn.cluster import KMeans  
from sklearn.preprocessing import StandardScaler  
from sklearn.model_selection import train_test_split  
import matplotlib.pyplot as plt  
  
iris = load_iris()  
  
iris_data = iris.data  
iris_target = iris.target  
  
X_train, X_test, y_train, y_test =  train_test_split(iris_data, iris_target, test_size=0.2, random_state=None)
  
kms = KMeans(n_clusters=3)  
kms.fit(X_train, y_train)  
  
y_pred = kms.predict(iris_data)  
  
fig, axes = plt.subplots(2,1, figsize=(5,10))  
axes[0].scatter(iris_data[:,0], iris_data[:,1], c=iris_target)  
axes[0].set_title("real results")  
axes[1].scatter(iris_data[:,0], iris_data[:,1], c=y_pred)  
axes[1].set_title("KMeans results")  
plt.show()
```

KMeans 聚类结果如下:
![[attachments/Pasted image 20240914173557.png]]

```python
# 获取聚类中心点
centers = kms.cluster_centers_
print("Cluster centers:\n", centers)
```

二分 KMeans 仍然参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/4. KNN算法和KMeans聚类相关算法|4. KNN算法和KMeans聚类相关算法]]

### (2) 协同过滤 User CF 和 Item CF 原理
协同过滤(Collabrate Filtering, CF) 分为 User CF 和 Item CF 部分;
首先需要建立<b><mark style="background: transparent; color: orange">用户偏好矩阵</mark></b>, 根据 KNN 算法，以距离或者夹角余弦为距离, 分别计算其到每一类用户的距离;
![[Excalidraw/3. 推荐系统和需求搜寻算法 2024-09-14 18.03.57|550]]

用户偏好矩阵是稀疏的矩阵, 表示用户是否和某个物品发生交互, 例如下图:

| UserID/ItemID | Item1 | Item2 | Item3 | Item4 | Item5 |
|---------------|-------|-------|-------|-------|-------|
| User1         | 5     | 3     | 0     | 1     | 0     |
| User2         | 4     | 0     | 0     | 1     | 1     |
| User3         | 1     | 1     | 0     | 5     | 0     |
| User4         | 0     | 0     | 5     | 4     | 4     |
| User5         | 0     | 0     | 4     | 0     | 5     |

**User CF 即采用KNN等将用户(USER C)通过usr_vector 归类到某一类用户(A或B)中。并将该类用户没有买过的物品进行推荐**
**Item CF 即选取新用户(USER C)比较偏好的物品 (item E), 并通过分类获取与 item E 相似的物品 (通过item_vec可以找出如item A相似,则将item A 也推荐给 item E的用户)**

一般采用如下的相似度:
余弦相似度: 
$$\text{sim}(u, v) = \frac{R_u \cdot R_v}{\|R_u\| \|R_v\|}$$
皮尔逊相似度:
$$\text{sim}(u, v) = \frac{\sum_{i \in I} (R_{ui} - \bar{R}_u)(R_{vi} - \bar{R}_v)}{\sqrt{\sum_{i \in I} (R_{ui} - \bar{R}_u)^2} \sqrt{\sum_{i \in I} (R_{vi} - \bar{R}_v)^2}}$$

然后<b><mark style="background: transparent; color: orange">通过每两个用户进行评分向量点积</mark></b>, 得到的相似度矩阵即为 UserCF 矩阵, 即有

| UserCF | 1        | 2        | 3        | 4        | 5        |
| ------ | -------- | -------- | -------- | -------- | -------- |
| 1      | sim(1,1) | sim(1,2) |          |          |          |
| 2      | sim(2,1) | $\ddots$ |          |          |          |
| 3      |          |          | $\ddots$ |          |          |
| 4      |          |          |          | $\ddots$ |          |
| 5      |          |          |          |          | sim(5,5) |

### (3) 一般矩阵的 SVD 分解及其证明 
对于基于模型的 CF 方法中, 最常用的是隐语义模型, 采用的是 SVD 分解, 即矩阵的奇异值分解(参考[[📘ClassNotes/📐Mathmatics/✖️Matrix Theory/4. 矩阵分解#四、奇异值分解(SVD 分解)|四、奇异值分解(SVD 分解)]])。 
首先, 参考[[📘ClassNotes/📐Mathmatics/📏linear algebra/第五章 矩阵的相似变换#(3) 矩阵的相似对角化|第五章 矩阵的相似变换]], 有: 
对于 n 阶方阵 $A^{n\times n}$, 有相似矩阵:
$$A = Q \Sigma Q ^{-1}$$
其中 $\Sigma$ 为特征值构成的对角阵, $Q$ 为特征向量排成的列阵。但是,此方法仅仅是对方阵而言的。实际**多数情况下样本不是方阵**，此时可以<mark style="background: transparent; color: red">以奇异值分解描述普通矩阵的重要特征</mark>

<b><mark style="background: transparent; color: blue">定义(矩阵的SVD分解)</mark></b>:设 $A$ 是任意的 $M \times N$ 矩阵, 则有矩阵的 SVD 分解为:
$$\Large\boxed{A =  U \Sigma V^{T}}$$
其中 $U$ 是 $M \times M$ 方阵, $\Sigma$ 是一个 $M \times N$ 矩阵, 仅有对角线上有元素, 且<mark style="background: transparent; color: red">对角线上的元素称为奇异值</mark>, 其它元素均为0. 而 $V^T$ 是一个 $N \times N$ 方阵。即:
$$\Large\boxed{A_{m \times  n} = U _{m\times m} \Sigma_{m \times  n} V^{T}_{n \times n}}$$
其中 U 的元素称为<mark style="background: transparent; color: red">左奇异向量</mark>, V 中的元素称为<mark style="background: transparent; color: red">右奇异向量</mark>, 并有关系 $U^{T} U = V^{T}V= I$ 
参考 [Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) 补充 [polar_decomposition](https://en.wikipedia.org/wiki/Polar_decomposition#Matrix_polar_decomposition) 推导可以参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/推导部分/SVD decomposition.pdf|SVD decomposition.pdf]]  和 [SVD分解的证明](https://zhuanlan.zhihu.com/p/399547902) 

**给出推导**: 首先, 设A的秩为r, 即有 $r = \min \left\{ m,n\right\}$, 此时 $A^T$ 秩显然也为 $r$, 经过[一文关于矩阵秩公式r(AA^T)=r(A^T A)=r(A)的证明](https://zhuanlan.zhihu.com/p/447703725) 得到: 
$$rank (A^{T}A ) =  rank (A A^{T} )$$
则 $\text{rank}(A^T A) = r$, 此时根据$A^TA$为半正定矩阵(显然$x^TA^TAx \geq 0$), 因此其特征值为非负实数。其中**秩的个数即为非零特征值的个数**，取其特征值得到:
$$\boxed{\Large  (A^{T}A) v_{i}= \lambda_{i} v_{i}}$$
其中 $A$ 为 $m \times n$ 矩阵, **同时,由于$AA^T$为实对称阵, 相似对角化后 $v_i, v_j$ 是正交向量, 则**:
$$<Av_{i} ,Av_{j}> = v_{j}^{T} A^{T} A v_{i}  = \lambda v_{j}^{T}  v_{i} = \begin{cases}
\lambda_{i} \qquad  1 \leq  i = j \leq  r  \\
0 \qquad  i \neq j  \text{ or } i = j > r
\end{cases}$$
显然 $<Av_i, Av_j>$ 为正交向量组, 此时取 i = j 即得如下式子:
$$\boxed{\Large ||Av_{i}|| = \sqrt{\lambda_{i}}\qquad i = 1, 2, \dots  r}$$
$$\Large\boxed{||Av_{i}|| = 0 \qquad  i = r + 1, \dots n}$$
其中$v_{i}$就是上面的<mark style="background: transparent; color: red">右奇异向量</mark>, 我们取 $\sigma_i  = \sqrt{\lambda_i}$ 即为<b><mark style="background: transparent; color: blue">奇异值</mark></b>，而令:
$$\Large \boxed{u_{i}  = \frac{Av_{i}}{||Av_{i}||} =  \frac{1}{\sqrt{\lambda_{i}}} Av_{i} \qquad  i= 1, \dots  n} $$
**其中由于 A 为 $m*n$ 故 $\boldsymbol{u}$ 为 $m \times 1$ 的向量, 实际上就是向量u**<b><mark style="background: transparent; color: blue">(左奇异向量)</mark></b>。然后经过如下证明过程: 
![[attachments/Pasted image 20240919170741.png|400]]
得到 SVD 分解:
$$\Large\boxed {A_{m \times  n} = U_{m \times  m }\Sigma_{m \times  n} V_{n \times n}^{T}}$$
### (4) 矩阵的部分奇异值(SVD)分解 
奇异值$\sigma$和特征值类似, 显然**在 $\Sigma$ 中, $\sigma$ 值也是从大到小排列的**。此时**可能矩阵前 10% 甚至 1% 的特征值的和就占据了全部奇异值之和的99%以上**。我们取前 $r$ 个奇异值, 其余的 r+1阶以后的奇异值非常接近0,则定义<b><mark style="background: transparent; color: blue">矩阵的部分SVD分解</mark></b>为:
$$\boxed{\Large A_{m \times  r} \approx U_{m \times r} \Sigma_{r \times r} V^{T}_{r\times  n}}$$
在numpy.linalg 中提供了SVD分解函数:
```python 
import numpy.linalg import svd
U, S, VT  = svd(A)
print(U, S, VT)
```

此外, 对于稀疏矩阵, 也可以进行 svd 分解, 示例代码如下:
```python
import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse.linalg import svds

# 创建一个稀疏矩阵 (Compressed Sparse Row format)
data = np.array([4, 5, 3, 2, 6])          # 非零元素
row_indices = np.array([0, 0, 1, 2, 3])   # 元素对应的行索引
col_indices = np.array([0, 1, 2, 3, 4])   # 元素对应的列索引

# 构造稀疏矩阵
sparse_matrix = coo_matrix((data, (row_indices, col_indices)), shape=(5, 5))

print("稀疏矩阵：")
print(sparse_matrix.toarray())

# 对稀疏矩阵进行 SVD 分解
k = 3  # 计算前 k 个奇异值和对应的奇异向量
u, s, vt = svds(sparse_matrix, k=k)

# SVD 结果
print("\n左奇异向量 (U)：")
print(u)

print("\n奇异值 (S)：")
print(s)

print("\n右奇异向量 (V^T)：")
print(vt)
```

## 二、PCA 主成分分析和 SVD 分解的应用
需要说明, PCA 和 SVD 都是 sklearn.decomposition 中的内容。
### (1) PCA 主成分分析原理
[sklearn实战之降维算法PCA与SVD_使用svd进行主成分分析鸢尾花分类-CSDN博客](https://blog.csdn.net/qq_48314528/article/details/119845670)
PCA 的基本原理是：**如果一个==高维数据集如果能够被相关变量表示, 那么仅有一些维有意义。根据此原理, 可以提取出高维变量中某些特征或者相关变量, 即可采用低维数据表示该变量==**, 而<mark style="background: transparent; color: red">不重要的维可以在计算中忽略</mark>。此时<b><mark style="background: transparent; color: blue">如果寻找到数据中方差最大的方向, 则被称为向量的主成分</mark></b> 

在高维数据中, 往往噪声不带有有效信息, 且部分特征是相关的, 因此需要在减少特征数量同时保持保留有效信息。<mark style="background: transparent; color: red">根据样本方差, 方差越大, 特征所含有信息越多。 一般地, 方差小的数据, 其体现信息比较少, 因此可以舍去</mark>; 即<b><mark style="background: transparent; color: orange">保留数据集中含有最大方差的一部分方向，称为向量的主成分</mark></b> 

一般地, 样本方差计算为(采用 n-1 是样本的无偏估计): 
$$\text{Var} = \frac{1}{n -1} \sum^{n}_{i=1} (x_{i} - \overline{x})^{2}$$
PCA 的一般步骤如下: 首先, 计算<b><mark style="background: transparent; color: orange">样本的均值向量和协方差矩阵</mark></b>$S$: 
$$\mu =\frac{1}{n} \sum^{n}_{i=1} x_{i} \qquad  S = \frac{1}{n}\sum^{n}_{i=1}(x_{i}- \mu) (x_{i} - \mu)^{T}$$
然后计算 $S$ 的特征值 $\lambda_{i}$ 和对应的特征向量 $v_i$  
$$Sv_{i}  = \lambda_{i} v_{i}  \qquad  i = 1,2,\dots n$$
然后<mark style="background: transparent; color: red">对特征值和特征向量进行递减排序, 如果定义一个向量有 k 个主成分, 则对应的 k 个主成分即为 k 个最大的特征值及所对应的特征向量</mark>。注意: 这个通过 PCA `(n_components = x)` 进行指定(如果仅仅可视化,可以降维到二维) 

此时, 我们设 $x$ 有 $n$ 个分量, 降维到 $r$, 则取主成分向量矩阵 $W_{(n \times  r)}$。而对于协方差矩阵 $S$, 设某个方向向量为 $w$, 则在对应方向上的方差投影为 $w^{T} (x_i - \mu)(x_{i} - \mu)^{T} w =wSw^{T}$, 即对于$w$ 为特征向量时, 特征值值即为方差, 又参考[[📘ClassNotes/📐Mathmatics/✖️Matrix Theory/5. 特征值估计和对称矩阵的极性#(2) 实对称矩阵 Rayleigh 商的极性|实对称矩阵 Rayleigh 商的极性]]。故**其中第 $v_i$ 个特征向量是特征值相差最大的方向, 此时我们只需要将 $x$ 投影到这些方向上, 即对应主成分方向 $v_i$ 下的坐标为: $(x- \mu) \cdot v_{i}$** , 得到的 r 个主成分为 $y$, 则有公式: 
$$y = W^{T}  (x-\mu)   \qquad  W = (v_{1}, v_{2}, \dots  v_{r})$$
而 <b><mark style="background: transparent; color: orange">PCA 基的变换</mark></b>为:
$$x = W y + \mu \qquad  W =  (v_{1}, v_{2}, \dots  v_{r})$$
下面采用 PCA 降维之后预测 Iris 数据集并绘制图像。

```python
from sklearn.datasets import load_iris  
from sklearn.decomposition import PCA  
from sklearn.preprocessing import StandardScaler  
from sklearn.model_selection import train_test_split, cross_val_score  
from sklearn.naive_bayes import GaussianNB  
from sklearn.metrics import accuracy_score  
import matplotlib.pyplot as plt  
  
X,y = load_iris(return_X_y=True)  
x_train, x_test,y_train, y_test = train_test_split(X,y,test_size=0.35,random_state=42)  
  
pca = PCA(n_components=2).fit(x_train) # 2 components  
print(pca.components_)     # 主成分的特征向量(每一行对应一个主成分， 即对应的 V 矩阵)
print("variance:" ,pca.explained_variance_)         # 查看降维后每个新特征向量上所带的信息量大小（可解释性方差的大小）  
print("variance_ratio:", pca.explained_variance_ratio_)   # 查看降维后每个新特征向量上所带的信息量占总信息量的比例（可解释性方差的百分比）  
  
x_train_new = pca.transform(x_train)
x_test_new = pca.transform(x_test)  
  
gnb = GaussianNB()  
gnb.fit(x_train_new,y_train)  
y_pred = gnb.predict(x_test_new)  
  
fig, axes = plt.subplots(1,2, figsize=(12,5), subplot_kw = {"xticks":[],"yticks":[]})  # subplot_kw 指定绘制顺序
axes[0].scatter(x_test_new[:,0],x_test_new[:,1],c=y_test)  
axes[1].scatter(x_test_new[:,0],x_test_new[:,1],c=y_pred)  
plt.show()  
  
print("predict accuracy:", accuracy_score(y_test,y_pred, normalize=True))
```

> [!caution] PCA 的 n_components 参数
> 1. 为整数时, 则保留相应的维数。
> 2. 如果不写任何值, 则返回 min(X.shape) (由于样本量一般大于特征个数, 相当于没有转换特征, 因此除了绘制累计可解释方差贡献率曲线 `np.cumsum(pca_line.explained_variance_ratio_)` 以外, 一般不采用该方式)
> 3. 可以采用 PCA 用最大似然估计进行自选超参数 `n_components` , 只需指定
> `pca = PCA(n_components = "mle")` 即可 
> 4. 输入 [0,1] 间的浮点数并取 `svd_solver = full` 时, 有 `pca_f = PCA(n_components=0.97,svd_solver="full")` 则降维到保证降维后**总解释性方差贡献率大于n_components指定百分比的信息量**的维数。

另外绘制三维图像也非常简单(不进行预测):
```python
from sklearn.decomposition import PCA  
# unused but required import for doing 3d projections with matplotlib < 3.2  
import mpl_toolkits.mplot3d

fig = plt.figure(1, figsize=(8, 6))  
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)  
  
X_reduced = PCA(n_components=3).fit_transform(iris.data)  
ax.scatter(  
    X_reduced[:, 0],  
    X_reduced[:, 1],  
    X_reduced[:, 2],  
    c=iris.target,  
    s=40, # marker size )
plt.show()
```
### (2) PCA 中的 SVD 
SVD 实际上是<b><mark style="background: transparent; color: blue">不计算协方差矩阵, 直接找出一个新特征向量组成的 n 维空间</mark></b>, 即直接获取右分解向量矩阵 $V^T$, 而 $U$ 和 $\Sigma$ 在 fit 完成之后, 即被舍弃。
而 `print(pca.components_)` 可以直接获取 PCA 中 SVD 分解器的$V$参数。

实际上PCA 的 SVD 求解器是通过 PCA (svd_solver) 控制的, 参数包含:
**“auto”，“full”，“arapck”，“randomized”，默认"auto"**，具体解释如下: 
![[attachments/Pasted image 20240922105941.png|450]]

需要说明 PCA和特征选择的区别: **特征选择后的特征矩阵是可解读的，而PCA降维后的特征矩阵是不可解读的, 即将已经存在的特征进行压缩, 且降维完毕后特征不是原矩阵的任何特征, 而是通过某些方式进行组合起来的，新的特征**.
**重要的是： 如果原特征矩阵是图像，V(k,n)这 个空间矩阵也可以被可视化的话，我们就可以通过两张图来比较，就可以看出新特征空间究竟从原始数据里提取了 什么重要的信息**。例如人脸识别中有较为好的应用。

但是在矩阵分解时, PCA一般在原有特征基础上, 找出能够让信息尽量聚集的新的特征向量。

> [!NOTE] 补充: PCA 的 inverse_transform 部分
> PCA 可以通过 X **右乘所提取的特征矩阵 V** 生成新矩阵 $X_{dr}$ 则让 $X_{dr}$ 右乘 $V(k,n)$ 逆矩阵 $V^{-1}_{(k,n)}$ 即可将 $X_{dr}$ 还原为 $X$, 但舍弃了降维后的部分的信息。

## 三、Kaggle 实战 PCA, SVD 分解与推荐系统
从 Kaggle 上获取推荐系统数据集 [retailrocket](https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset) 进行分析
- Task 1
**When a customer comes to an e-commerce site, he looks for a product with particular properties: price range, vendor, product type and etc. These properties are implicit**, so it's hard to determine them through clicks log.

Try to **create an algorithm which predicts properties of items in "addtocart" event by using data from "view" events for any visitor in the published log**.

- Task 2
Description:
Process of analyzing ecommerce data include very important part of data cleaning. Researchers noticed that in some cases browsing data include up to 40% of abnormal traffic.

Firstly, abnormal users add a lot of noise into data and make recommendation system less effective. In order to increase efficiency of recommendation system, abnormal users should be removed from the raw data.

Secondly, abnormal users add bias to results of split tests, so this type of users should be removed also from split test data.

Goals:
- The main goal is to find abnormal users of e-shop.

Subgoals:
- Generate features
- Build a model
- Create a metric that helps to evaluate quality of the model

### (1) Retailrocket 数据集与基本分析
首先绘制一些简单的图像对整体进行分析:
```python
def __show_data_analysis(self):  
    eve = self.events_data['event']  
    print(eve.value_counts())  
    sns.countplot(x ='event', hue='timestamp', data=self.events_data)  
    plt.show()

# all unique visitors 
all_customers = events['visitorid'].unique()
print("Unique visitors:", all_customers.size)

# all visitors
print('Total visitors:', events['visitorid'].size)
customer_purchased = events[events.transactionid.notnull()].visitorid.unique()
customer_purchased.size

# 提取某个列等于某个值的部分
cart_data  = self.events_data['event'].loc[self.events_data.event=="addtocart"]

# use "isin" to filter the column which satisfy  multiple conditions 
items_new = items.loc[items.property.isin(['categoryid', 'available']), :]
print("items with categoryid and available as propery:", items_new.size)
items_new.head(20)

# 分割数据集的指令示例: 
X_train, X_test, y_train, y_test =  train_test_split(iris_data, iris_target, test_size=0.2, random_state=None)
# 也可以指定一个项目, 如:
train_data, test_data = train_test_split(unique_visitors, test_size=0.2)  
print(train_data.shape, test_data.shape)

#grouping itemid by its event type and creating list of each of them (use groupby method)
grouped = events.groupby('event')['itemid'].apply(list)
```

结果如下:
1. 最多的 10 个 view 对应的 itemid :
![[attachments/Pasted image 20241118124143.png|500]]
2. 最多的 10 个 addtocart 对应的 itemid : 
![[attachments/Pasted image 20241118132455.png|500]]
3.  最多的 10 个  transaction 对应的  itemid:  
![[attachments/Pasted image 20241118132611.png|500]]

推荐系统的思路1 是提取每个客户买的东西, 对于买的物品有多个时, 则对相应的物品添加相关性, 并将相关性较高的推荐给买该物品的客户 (ItemCF) 

需要说明的是, 对于稀疏矩阵的构建, 如果采用一个稀疏矩阵提取包含训练集和测试集的矩阵, 可能会显得比较麻烦, 此处参考[[📘ClassNotes/⌨️Programming/🐍Python/🌟Python 基础部分/5. Scipy 稀疏矩阵相关内容|5. Scipy 稀疏矩阵相关内容]] 进行构建稀疏矩阵; 

循环类别编码方案(例如, 对于一个表格, 其多个属性均需要进行编码):
```python
train_data, test_data = train_test_split(self.events_data, test_size=0.2)  
trans_cat_train = dict()  
trans_cat_test = dict()  
  
# 采用循环编码方案, 对 visitorid 和 itemid 进行类别编码：  
for k in ['visitorid', 'itemid']:  
    cate_enc = LabelEncoder()  
    trans_cat_train[k] = cate_enc.fit_transform(self.events_data[k].values)  
    trans_cat_test[k] = cate_enc.transform(test_data[k].values)  
  
"""
trans_cat_train = {  
    'visitorid': [0, 1, 2, ...],  # 每个 visitorid 被编码成唯一整数  
    'itemid': [0, 1, 2, ...]     # 每个 itemid 被编码成唯一整数  
}  
"""  
cate_enc = LabelEncoder()  
trans_cat_train = cate_enc.fit_transform(train_data.event)  
trans_cat_test = cate_enc.transform(test_data.event)
```

一般而言地, 稀疏矩阵的值应该是 **行为的次数（如 view 或 add_to_cart）**，或者是一个权重分数，而不是类别编码. 实际上应该表示用户对物品的交互强度，通常每个单元格是用户和物品在某行为下的次数或权重，而不是一个固定的类别编号。

另外, <b><mark style="background: transparent; color: orange">实际上对于空的物品 id 以及空的客户 id, 没有必要将整个矩阵压缩为稠密矩阵, 实际上就按照 item id  和  user id 进行直接的建立即可</mark></b>.
**代码逻辑详解**
1. **统计行为次数**  
    `events_view.groupby(['visitorid', 'itemid']).size()` 会统计每个 `(visitorid, itemid)` 组合发生的 `view` 或 `add_to_cart` 次数。    
2. **编码用户和物品**  
    使用之前的 `trans_cat_train` 字典，将 `visitorid` 和 `itemid` 转换为数值编码。
3. **构造稀疏矩阵**
    - 稀疏矩阵的值是统计出的行为次数。
    - 行是 `visitorid`，列是 `itemid`。

### (2) 用户相似度计算方案
显然, 对于 UserCF 矩阵整体, 我们难以直接构造, 往往会采用一些技术减少相似度矩阵的计算量

例如, 对于一个效率极低的算法, 是按照循环进行赋值的, 对于 100000 量级的矩阵几乎没有求解能力, 如下所示:
```python
def get_usrCF(self, sim_mat):  
    """  
    get the  userCF matrix by similarity matrix    """    userCF_mat = lil_matrix((sim_mat.shape[0], sim_mat.shape[0]), dtype=np.float32)    # user x user  
    for i in range(sim_mat.shape[0]):  
        for j in range(sim_mat.shape[0]):  
            if j == i:  
                userCF_mat[i,j] = 1  
                continue  
            # calculate the similarity of user only if the overlap elements is greater than 0  
            idx_i = sim_mat[i, :].nonzero()[1]  
            idx_j = sim_mat[j,: ].nonzero()[1]  
            if np.intersect1d(idx_i, idx_j).size != 0:  
                vec1 = sim_mat[i,:]  
                vec2 = sim_mat[j,:]  
                userCF_mat[i,j] = np.dot(vec1, vec2.T).data/ np.sqrt(np.dot(vec1, vec1.T) * np.dot(vec2,vec2.T).data)  
    return userCF_mat
```

而对于 retail market 的训练数据, 实际的用户量级达到了 140 万以上, 实际建立的稀疏矩阵大小在 140w * 140 w 左右, 因此我们必须考虑更加高效的计算方式:

#### 1. 自适应 KMeans 聚类算法的适用性
同时, 即使是在 140w 情况下进行单循环, 消耗时间也是惊人的, 因此, 我们尽可能减少计算次数, **可以考虑先采用 KMeans  [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/4. KNN算法和KMeans聚类相关算法|4. KNN算法和KMeans聚类相关算法]] 自适应聚类** (参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/4. KNN算法和KMeans聚类相关算法#三、自适应 KMeans 聚类方案|自适应 KMeans 聚类方案]]实现), 将用户分为多个用户组; 然后从组中获取相似度计算结果。

但是, 实际上<mark style="background: transparent; color: red">聚类之后经过结果检验, 绝大多数用户 (约 95%, 均是聚类到类别 0 中</mark>, 这相对于聚类寻优而言, 实际上没有增加多少运算量, 反而增加了实际计算的时间成本, 因此放弃直接自适应聚类的方案), 因此实际上可以考虑直接计算用户相似度矩阵。

需要说明的是, 对于维度极大的矩阵, 我们一般采用<b><mark style="background: transparent; color: blue">降维之后聚类的方式</mark></b>:
- 高维稀疏矩阵（尤其是用户-物品偏好矩阵）通常含有大量的 0，直接在高维空间中聚类会使许多算法（例如 K-Means）难以有效度量相似性，因为在高维稀疏数据中，点之间的欧几里得距离通常会趋于相等。 在高维空间中，距离度量（如欧几里得距离）可能失效，因为数据点的相对距离差异会变小（即所谓的“维度诅咒”）
- PCA 等降维方法提取了矩阵的主要信息（通过捕获主成分），从而将高维稀疏矩阵转化为低维密集矩阵，聚类算法可以在此基础上更好地工作。

#### 2. 稀疏用户相似矩阵 UserCF 的建立和计算
相似度的快速计算方法包括:
1. 采用 scipy.distance 直接计算距离, 适用于矩阵不大的情况
2. 找 sklearn 的 pairwise 库, 其中能够计算较大规模的稀疏矩阵
```python 
from scipy.spatial import distance
from sklearn.metrics.pairwise import cosine_similarity
```
对于 cosine_similarity 计算大规模稀疏矩阵, 对于 30000 左右规模的矩阵效果是非常好的; 而对于 1000000 大小矩阵会分配约 5TB 空间, 无法进行分配.

因此我们采用 PCA 降维和降维之后聚类的方式;
首先, ==**稀疏矩阵是不支持直接 PCA 的, 如果必须使用 PCA，输入需要先转为稠密矩阵**==（这可能会消耗大量内存），然后指定 `svd_solver='arpack'`。

实际上对于稀疏矩阵, 更加方便的方法是 `TruncateSVD` , 即采用:
```python
from sklearn.decomposition import TruncatedSVD 
```
实际上是采用 SVD 分解提取其中的主成分部分, 实际上效率更加高效;

SVD 分解:  通过 `scipy.sparse.linalg.svds` 或 `sklearn.decomposition.TruncatedSVD` 进行分解：
`svds` 是用于计算稀疏矩阵的奇异值分解（Singular Value Decomposition, SVD）的函数，它来自 `scipy.sparse.linalg` 模块。函数的主要参数之一是 `k`，它代表要计算的奇异值的数量。

根据 [TruncatedSVD 简介](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), 其  arpack 是一种朴素的算法, 但是速度很慢, 不适合大规模稀疏矩阵运算过程,  因此往往采用 randomized 算法处理 100w 以上的稀疏矩阵;

根据一般经验, 降维的 `n_components` 选用为 50-300 为合适, 而 `n_components` 越大, 越耗时和消耗计算内存。
一般大规模矩阵选用 50 即可, 而最后将用户聚类为 100 类, 之后只需在类内细分;

#### 3. MiniBatchKMeans 和基于欧氏距离的 UserCF 聚类
在 `MiniBatchKMeans` 中，<mark style="background: transparent; color: red">默认的距离度量是欧几里得距离</mark>。如果要使用余弦距离（cosine distance）进行聚类，可以通过自定义 `MiniBatchKMeans` 的距离度量来实现。遗憾的是，`MiniBatchKMeans` 本身并没有直接支持余弦距离作为距离度量。因此需要我们进行自己实现基于余弦距离的聚类;

但是，由于我们使用的是经过 PCA(TruncatedSVD) 降维之后的矩阵, 其 Item 特征已经稠密, 此时采用欧式距离是更好的选择; 
```python
def __get_pref_mat(self, usr_mat):  
    """  
    get the new preference matrix (new User-Item matrix) based on sparse PCA and Minibatch KMeans clustering    
    this must be called in train process    
    :param usr_mat: usr preference mat (storge the user and item which has operated)    
    :return:  
        model:  the model of the sparse PCA and KMeans (transformed data should use this model)        
        usr_pref_mat: user preference matrix        usr_labels: the cluster labels of the user   
    """
model = TruncatedSVD(n_components=self.features, algorithm='randomized')  
    pca_mat_train = model.fit_transform(usr_mat)  
  
    # MiniBatchKMeans with precomputed cosine distance matrix  
    mb_clusters = MiniBatchKMeans(n_clusters=self.cluster_number, batch_size=self.kmeans_batch_size,  
                                  random_state=42)  
    mb_clusters.fit(pca_mat_train)  # we finally use the cosine distance for fit the model  
    usr_pref_mat = mb_clusters.cluster_centers_  # pref_mat is the cluster center  
    usr_labels = mb_clusters.labels_  
    if self.ouput_info:  
        """ print detailed infomation of the clustering model """  
        print("========= model build succeed =============")  
        show_cluster_counters(usr_labels, show_detailed=False)  
        print("cluster inertia :", mb_clusters.inertia_)  
    return model, usr_pref_mat, usr_labels
```

其中 我们采用一个简单的函数检查每个聚类的结果, 实际上我们只需最后检查聚类结果小于某一阈值(如 = 1) 
```python
def show_cluster_counters(self,labels, cnt_per_row = 3):  
    unique_labels, cnt = np.unique(labels, return_counts=True)  
    idx = 0  
    for (l, c) in zip(unique_labels, cnt):  
        print("num of ",l ,":",c, end=" ")  
        if idx % cnt_per_row == 0:  
            print()  
        idx = idx + 1
    print("signle element clusters number", np.sum(cnt[cnt==1]))
```

参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/5. 近似最邻近算法(kdtree,Faiss,Annoy, ScaNN)|5. 近似最邻近算法(kdtree,Faiss,Annoy, ScaNN)]], 采用如下计算 Transform 之后的每一项的距离, 并聚类到对应的 User 组中.
```python
def __nearest_index(self, mat_from, mat_target, metric = 'euclidean', use_ANN=True):  
    """  
    We use the nearlest Neibour to calculate the nearest index of the matrix,    note: the input are all dense matrix, but the second dim shouldn't too large    :param mat_from: the matrix to be compared    :param mat_target: the matrix to be compared    :param metric: the metric to be used, default is cosine    :param use_ANN: use Approximate Nearest Neighbor Algorithm,           much faster when the matrix to calculate nearlest is large    :return: the index of the nearest matrix  
    """    
    if not use_ANN:  
        # calculate the cosine similarity for each user in the new data  
        dist = distance.cdist(mat_from, mat_target, metric=metric)  
        min_dist_idx = np.argmin(dist, axis=1)  
    else:  
        # Approximate Nearest Neighbor using Faiss  
        if metric == 'cosine':  
            # Normalize the vectors in row direction for cosine similarity  
            mat1_normalized = mat_from / np.linalg.norm(mat_from, axis=1, keepdims=True)  
            mat2_normalized = mat_target / np.linalg.norm(mat_target, axis=1, keepdims=True)  
            index = faiss.IndexFlatIP(mat_target.shape[1])  # Inner Product by line direction  
        elif metric == 'euclidean':  
            mat1_normalized = mat_from  
            mat2_normalized = mat_target  
            index = faiss.IndexFlatL2(mat_target.shape[1])  # L2 distance  
        else:  
            raise ValueError("Currently, only 'cosine' and 'euclidean' metric is supported for ANN.")  
        # Add reference data to the index  
        index.add(mat2_normalized.astype(np.float32))  
  
        # Perform nearest neighbor search  
        _, min_dist_idx = index.search(mat1_normalized.astype(np.float32), 1)  
        min_dist_idx = min_dist_idx.flatten()  
    return min_dist_idx
```

只需要采用如下方法验证:
```python
label_test = model.fit_transform(trans_data_test.visitorid, trans_data_test.itemid, item_max=self.itemid_max)  
label_validation = model.transform(trans_data_test.visitorid, trans_data_test.itemid, use_ANN=True)  
print("validation correction: ", np.sum(label_validation == label_test) / len(label_test)) 
>>> validation correction:  1.0   # 对原始集全部正确聚类
```

最终得到的 cluster_center 作为新的用户偏好矩阵. 即为我们需要的用户分类矩阵.此后只需要对新的数据进行分类.
> [!NOTE] 补充
> 在聚类时, 我们希望方便地计算一个矩阵向量到另一个矩阵向量的所有距离, 此时可采用
> scipy.spatial.distance 中的 `distance.cdist` 函数:
> `dist_matrix = distance.cdist(pref_mat_data, self.pref_mat, metric='cosine')` 
> 欧几里得距离则采用 eculidian
> 这个函数已经高度优化并适用于 3000 左右的中规模矩阵. 另外对于超大规模矩阵，可以考虑近似最邻近算法。

### (3) 结合 UserCF 矩阵和 ItemCF 矩阵的推荐算法优化 
我们上面确定了该问题求解的一般思路: <mark style="background: transparent; color: red">通过 UserCF 矩阵, 降维并筛选出用户所在的分组; </mark> 
但是, 虽然上述方法很方便地筛选出了分组, 有效减少了用户分组的数量, 但是根据新生成的 PCA 降维后的矩阵, 由于物品 Item 交互信息被 PCA 降维并抹去, 无法得知哪一类用户具体喜好哪一类物品. 过滤相似物品的成本, 实际上是很高的; 

但是, 可以通过用户分组缩小我们所求解的物品范围, 从而确定总体架构和求解思路:s

#### 1. **UserCF 稀疏矩阵降维聚类**
2找到相似用户 : 采用余弦相似度或基于稀疏矩阵优化的快速方法, 进行降维和聚类,  将目标用户分为不同的类别, 计目标用户 $(u)$ 的一组相似用户 $( U_{sim}(u) )$, 并通过聚类, 大大减少实际总用户种类数; 
> [!caution] 说明
> 需要说明的是, 在第一阶段聚类时, 聚类中心不要过多, 这会导致很多单个 User 被分为一类, 从而导致之后筛选出现问题; 因此一般 8000 左右的用户, 聚类类别放在 100 左右是合适的; 同时控制聚类内用户数量不要太多. 一般最大聚类中的用户数量 < 500 - 1500 为合适;

#### 2. **用户分组, ItemCF 和近似最邻近算法过滤**
基本的思路对相似用户的交互数据分组, 并在组内过滤其共同物品
我们首先根据 UserCF 的聚类结果, 提取相似用户组, 而<mark style="background: transparent; color: red">仅仅针对相似用户组内的物品, 进行 ItemCF 分析</mark>;

这样做的优点有如下几条: 
- 首先可以**过滤掉大量的其他物品**, 仅在用户组中聚类, 大大提高了 ItemCF 的效率;
- 其中一个问题是, <b><mark style="background: transparent; color: orange">不同的用户组的物品组会有重叠, 但是重新计算这个相似度是合理的;</mark></b> 因为不同用户组往往会有相同的推荐物品, 而根据用户组进行推荐物品实际上就是我们想要的效果。上面的聚类就实现了这一点。
- 上述方法实际上<mark style="background: transparent; color: red">推荐是按照用户组别内进行推荐的</mark>, 而组别内的推荐, 是按照用户购买的物品进行组别内关联推荐的<b><mark style="background: transparent; color: orange">(即一个用户买了某个物品, 则会推荐此用户分组内其他关联性最高的物品)</mark></b>, 这样有效地达到了细化推荐效果。
- 对于训练集中有新物品的情况, 仍然可以通过训练集中其他新物品的关联性寻找用户

由于 ItemCF 部分的矩阵不是非常大, 参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/5. 近似最邻近算法(kdtree,Faiss,Annoy, ScaNN)|5. 近似最邻近算法(kdtree,Faiss,Annoy, ScaNN)]] 进行过滤, (实际上是两层过滤, 而由于第一层的过滤规模在 400-600 左右, 但是实际上仍然是稀疏矩阵); 我们希望直接一层过滤得到 k 个相似物品，但是 Faiss 是不支持稀疏矩阵的, 因此<mark style="background: transparent; color: red">我们可以考虑直接采用 cosine_similarity 余弦相似度计算</mark>(参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/5. 近似最邻近算法(kdtree,Faiss,Annoy, ScaNN)|5. 近似最邻近算法(kdtree,Faiss,Annoy, ScaNN)]]); 另外, 也可以采用 KNN, MiniBatchKMeans 等方法进行过滤得到相应的 itemdict, 为分组内的最相似物品

#### 3. 评估实际计算结果并进行预测
首先, 目前已得到了每个用户组的物品相似矩阵 similar_item_dict, 这个矩阵表征了在用户组内, 和物品 v 最相似的 k 个物品,  但是仍然有问题需要解决:

1. 如何通过数据预测所建立 itemCF 矩阵的正确性, 即如何用训练集衡量 ItemCF 矩阵是否准确
2. 考虑从结合我们在 itemCF 中建立的字典, 从 view 预测 addtocart. 显然每个 addtocart 都会有对应的 View, 直接盲目预测不可行.

我们先解决第一个问题:
首先, 我们将 View 数据分为训练集和测试集, 需要说明的是, 我们这里仅仅采用部分独立物品去建立, 训练时采用 80% 数据集进行训练; 而采用另外 20% 的部分去衡量准确性。

首先考虑, 如果训练中每个用户仅提取其交互物品的一部分, 如果是采用部分做训练集， 肯定会出现有的物品相似度直接找不到的问题， 建立的 ItemCF 是不完整, 不准确的。因此实际上<mark style="background: transparent; color: red">可以采用直接划分的方法去做训练集</mark>, 但如果直接划分训练集和测试集，将训练集中的用户交互物品全都提取肯定是不合理的, 因为这样会导致测试集中可能全都被选过, 没有任何可预测的物品。

我们可以以整个数据集作为训练集, 构建 ItemCF 矩阵，而在评估阶段对用户的交互记录进行分割（部分作为输入，部分作为目标）来验证模型的预测效果，可以有效地解决混杂问题，同时避免因数据稀疏导致的冷启动效应。

注意: test_item_size 不要设置太小, 因为设置太小会使得模型的预测集中在更少的目标上，尽管总体预测结果的数量（`tot_rec_itm_num`）可能不变，但正确命中的数量（`hit_itm_num`）也可能减少。 

最终得到的结果如下: 
预测命中率约为 80%, 而预测的占比为 14.8% 左右
![[attachments/Pasted image 20241124090503.png|500]]

