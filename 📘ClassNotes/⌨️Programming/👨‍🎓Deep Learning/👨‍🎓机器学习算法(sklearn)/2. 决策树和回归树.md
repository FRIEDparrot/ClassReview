## ä¸€ã€å†³ç­–æ ‘ç®—æ³•
å†³ç­–æ ‘æœ€æ—©èµ·æºäº CLS(Concept Learning System) å­¦ä¹ ç³»ç»Ÿ, è€Œ ID3 ç®—æ³•åŸå‹åœ¨1979å¹´æ‰è¢«J.R.Quinlanæå‡ºã€‚å¹¶é€šè¿‡ç®€åŒ– ID3ç®—æ³•, ç¡®ç«‹äº†å†³ç­–æ ‘å­¦ä¹ ç†è®ºã€‚å¼•å…¥èŠ‚ç‚¹ç¼“å†²åŒºå°†ID3å‘å±•ä¸ºID4ç®—æ³•, æœ€åå‘å±•æ”¹è¿›ä¸º C4.5ç®—æ³•, æˆä¸ºæœºå™¨å­¦ä¹ çš„åå¤§æ•°æ®æŒ–æ˜ç®—æ³•ä¹‹ä¸€ã€‚

å¦å¤–ä¸€ä¸ªé‡è¦çš„æœºå™¨å­¦ä¹ ç®—æ³•, <mark style="background: transparent; color: red">ä¹Ÿæ˜¯ID3çš„å¦å¤–ä¸€ä¸ªåˆ†æ”¯, ç§°ä¸ºåˆ†ç±»å†³ç­–æ ‘ç®—æ³•</mark>(Classification and Regression Tree, CARTç®—æ³•), å…¶ä¸­ä¸ C4.5 ä¸åŒçš„æ˜¯, CARTå†³ç­–æ ‘ç®—æ³•ä¸»è¦ç”¨äºé¢„æµ‹ã€‚
> [!NOTE] è¡¥å……: åå¤§æ•°æ®æŒ–æ˜ç®—æ³•
> ![[Pasted image 20221115195839.png|400]]

### (1) å†³ç­–æ ‘åŠå…¶ç®—æ³•æ¡†æ¶
å†³ç­–æ ‘æ˜¯ä¸€ç§åŸºäº if-then ç»“æ„å½¢æˆçš„åˆ†ç±»å­¦ä¹ æ–¹æ³•,  ä»¥æŸç¬”è®°æœ¬ç”µè„‘çš„å®¢æˆ·çš„åˆ†ç±»å’Œé”€å”®æ”¶å…¥è°ƒæŸ¥è¡¨ä¸ºä¾‹:

| è®¡æ•°  | å¹´é¾„  | æ”¶å…¥  | å­¦ç”Ÿ  | ä¿¡èª‰  | æ˜¯å¦è´­ä¹° |
| --- | --- | --- | --- | --- | ---- |
| 64  | é’   | é«˜   | å¦   | è‰¯   | ä¸ä¹°   |
| 64  | é’   | é«˜   | å¦   | ä¼˜   | ä¸ä¹°   |
| 128 | ä¸­   | é«˜   | å¦   | è‰¯   | ä¹°    |
| 60  | è€   | ä¸­   | å¦   | è‰¯   | ä¹°    |
| 64  | è€   | ä½   | æ˜¯   | è‰¯   | ä¹°    |
| 64  | è€   | ä½   | æ˜¯   | ä¼˜   | ä¸ä¹°   |
| 64  | ä¸­   | ä½   | æ˜¯   | ä¼˜   | ä¹°    |
| 128 | é’   | ä¸­   | å¦   | è‰¯   | ä¸ä¹°   |
| 64  | é’   | ä½   | æ˜¯   | è‰¯   | ä¹°    |
| 132 | è€   | ä¸­   | æ˜¯   | è‰¯   | ä¹°    |
| 64  | é’   | ä¸­   | æ˜¯   | ä¼˜   | ä¹°    |
| 32  | ä¸­   | ä¸­   | å¦   | ä¼˜   | ä¹°    |
| 32  | ä¸­   | é«˜   | æ˜¯   | è‰¯   | ä¹°    |
| 64  | è€   | ä¸­   | å¦   | ä¼˜   | ä¸ä¹°   |


é¦–å…ˆ, å†³ç­–æ ‘éƒ¨åˆ†ä¸»è¦åˆ†ä¸ºæ ¹èŠ‚ç‚¹, å¶å­èŠ‚ç‚¹å’Œå†…éƒ¨èŠ‚ç‚¹, <mark style="background: transparent; color: red">å½“æŒ‰ç…§æŸç§æ¡ä»¶è¿›è¡Œåˆ’åˆ†æ—¶, å¦‚æœåˆ’åˆ†åˆ°å­é›†ä¸ºç©º,æˆ–è€…å­é›†ä¸­æ ·æœ¬å·²ç»å½’ä½åŒä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾, åˆ™è¯¥å­é›†æˆä¸ºå¶å­èŠ‚ç‚¹,  å¦åˆ™å¯¹åº”äºå†…éƒ¨èŠ‚ç‚¹ï¼Œ å¯¹äºå†…éƒ¨èŠ‚ç‚¹å‡éœ€è¦é€‰æ‹©ä¸€ä¸ªæ–°çš„ç±»åˆ«æ ‡ç­¾å¯¹è¯¥å­é›†ç»§ç»­è¿›è¡Œåˆ’åˆ†</mark>,ç›´åˆ°å…¨éƒ¨ä¸ºå¶å­èŠ‚ç‚¹ã€‚

ä¾‹å¦‚æŒ‰ç…§å¹´é¾„, å®¹æ˜“å‘ç°ä¸­å¹´å‡æ˜¯è´­ä¹°è¯¥äº§å“çš„ã€‚ä»¥æ­¤ç±»æ¨å¯ä»¥æ„å»ºå¦‚ä¸‹çš„å†³ç­–æ ‘:
![[Excalidraw/2. å†³ç­–æ ‘å’Œå›å½’æ ‘ 2024-09-11 18.13.37|650]]
å…¶ä¸­, å¯ä»¥é€šè¿‡æ•°æ®é™¤ä»¥æ€»ä¸ªæ•° (1024) <mark style="background: transparent; color: red">è·å–åˆ°æ¯ä¸ªèŠ‚ç‚¹å¤„çš„è´­ä¹°æ¦‚ç‡ã€‚</mark> 

å†³ç­–æ ‘ä¸­, ä¸»å‡½æ•°æœ¬è´¨ä¸Šæ˜¯é€šè¿‡é€’å½’å‡½æ•°, æŒ‰ç…§è§„åˆ™ç”Ÿé•¿å‡ºå†³ç­–æ ‘çš„å„ä¸ªåˆ†æ”¯èŠ‚ç‚¹, å…¶ä¸­<b><mark style="background: transparent; color: blue">å†³ç­–æ ‘çš„ä¸åŒå–å†³äºæœ€ä¼˜ç‰¹å¾é€‰æ‹©çš„æ ‡å‡†ä¸Šæœ‰å·®å¼‚ã€‚</mark></b> å…¶ä¸­ <mark style="background: transparent; color: red">ID3 çš„æœ€ä¼˜ç‰¹å¾é€‰æ‹©æ˜¯ä¿¡æ¯å¢ç›Š, C4.5 æ˜¯ä¿¡æ¯å¢ç›Šç‡, CART æ˜¯èŠ‚ç‚¹æ–¹å·®å¤§å°</mark>ã€‚
ä¸€èˆ¬åœ°, <b><mark style="background: transparent; color: blue">é€‰æ‹©æœ€ä¼˜ç‰¹å¾ï¼Œ éœ€è¦éå†æ•´ä¸ªæ•°æ®é›†å¹¶è¯„ä¼°ç‰¹å¾ï¼Œ æ‰¾åˆ°æœ€ä¼˜çš„ç‰¹å¾è¿›è¡Œè¿”å›</mark></b>ã€‚

å…¶ä¸­é¦–å…ˆéœ€è¾“å…¥åˆ†ç±»çš„æ•°æ®é›†å’Œç±»åˆ«æ ‡ç­¾, è€Œå…³é”®æ˜¯<b><mark style="background: transparent; color: blue">æ ¹æ®æŸç§åˆ†ç±»è§„åˆ™æ‰¾åˆ°æœ€ä¼˜çš„åˆ’åˆ†ç‰¹å¾ï¼Œ åˆ›å»ºç‰¹å¾çš„åˆ’åˆ†èŠ‚ç‚¹, å³è®¡ç®—æœ€ä¼˜çš„ç‰¹å¾å­å‡½æ•°</mark></b>, å®ç°æ•°æ®é›†çš„åˆ’åˆ†ã€‚

å†³ç­–æ ‘çš„**åˆ†ç±»å™¨**æ˜¯<mark style="background: transparent; color: red">é€šè¿‡éå†æ•´ä¸ªå†³ç­–æ ‘ï¼Œ ä½¿æµ‹è¯•æ•°æ®æ‰¾åˆ°å†³ç­–æ ‘ä¸­çš„å¶å­èŠ‚ç‚¹å¯¹åº”çš„ç±»åˆ«æ ‡ç­¾ã€‚ åˆ™è¿™ä¸ªæ ‡ç­¾å³ä¸ºè¿”å›çš„ç»“æœã€‚</mark>

### (2) ä¿¡æ¯ç†µæµ‹åº¦
æ•°æ®é›†çš„åˆ’åˆ†æ˜¯å°†æ•°æ®ä»æ— éœ€å˜ä¸ºæœ‰åºçš„è¿‡ç¨‹, å¹¶é‡‡ç”¨ç†µ(entropy)è¡¡é‡æ•°æ®æ— éœ€çš„ç¨‹åº¦ã€‚ è®¾ä¸ç¡®å®šæ€§å‡½æ•°Iä¸ºäº‹ä»¶çš„ä¿¡æ¯é‡,ä¸”ä¸ºäº‹ä»¶ $U$ å‘ç”Ÿæ¦‚ç‡ $p$ çš„å•å‡å‡½æ•°, å¹¶ä¸”æ»¡è¶³ $I(p_1, p_2) = I(p_1) + I(p_2)$ 
$$I(U) = \log_{} \left(\frac{1}{p}\right) =- \log_{} p  \qquad  p = p (y|x)$$
å¯¹äºä¸€ä¸ªä¿¡å·æº, ä¸èƒ½ä»…è€ƒè™‘æŸå•ä¸€æ—¶é—´å‘ç”Ÿçš„ä¸ç¡®å®šæ€§, è€Œéœ€è¦è€ƒè™‘æ‰€æœ‰å¯èƒ½æƒ…å†µçš„å¹³å‡ä¸ç¡®å®šæ€§ã€‚

å½“ä¿¡æºäº‹ä»¶æœ‰ $n$ ç§å–å€¼ $U_1, \dots U_i,\dots ,U_n$ ä¸”ç›¸äº’ç‹¬ç«‹æ—¶,  è®°å…¶å¯¹åº”æ¦‚ç‡ä¸º$p_i$, åˆ™ä¿¡æºçš„å¹³å‡ä¸ç¡®å®šæ€§åº”å½“è®¡ç®—ä¸º:
$$H(U) =   E[- \log_{} p_{i}] = - \sum^{n}_{i=1}  p_{i} \log_{} p_{i}$$
éœ€è¦è¯´æ˜çš„æ˜¯, å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#(5) entropy concept of Information theory|entropy concept of Information theory]], å– 1 "nat" $=  \frac{1}{\ln 2} \approx 1.44bit$ , ç¼–ç è‡³å°‘éœ€è¦ $H[p]$ "nat" è¿›è¡Œç¼–ç ã€‚ <b><mark style="background: transparent; color: blue">è‹¥ä¸Šè¿°å‡½æ•°ä¸­ä¸€èˆ¬å– 2 ä¸ºåº•</mark></b>, åˆ™æˆä¸ºä¸€èˆ¬çš„ bit å•ä½ã€‚
ä¿¡æ¯ç†µæ˜¯ä¸ç¡®å®šæ€§çš„åº¦é‡æ ‡å‡†, ç”¨äºåº¦é‡ç±»åˆ«çš„ä¸ç¡®å®šæ€§ã€‚<mark style="background: transparent; color: red">æŸä¸ªç‰¹å¾åˆ—å‘é‡çš„ä¿¡æ¯ç†µè¶Šå¤§ï¼Œ è¯´æ˜è¯¥å‘é‡çš„ä¸ç¡®å®šæ€§ç¨‹åº¦è¶Šå¤§ï¼Œ åº”å½“ä»è¯¥ç‰¹å¾å‘é‡è¿›è¡Œç€æ‰‹åˆ’åˆ†ã€‚ </mark>

#### 1. æ•´ä¸ªæ•°æ®é›†çš„ä¿¡æ¯ç†µ
ç¬¬ä¸€, <mark style="background: transparent; color: red">é‡‡ç”¨ä¿¡æ¯ç†µåº¦é‡æ ‡ç­¾å¯¹æ ·æœ¬æ•´ä½“çš„ä¸ç¡®å®šæ€§</mark>ã€‚è®¾Sæ˜¯sä¸ªæ•°æ®æ ·æœ¬çš„é›†åˆï¼Œåœ¨ä¸€ä¸ªå¤§åˆ†ç±»(æ˜¯å¦è´­ä¹°)ä¸‹å…±æœ‰ m ä¸ªå°ç±» $C_i$ (è¯¥ç±»åˆ«æ ‡ç­¾å…·æœ‰mä¸ªä¸åŒçš„å€¼, å³ä¹°æˆ–è€…ä¸ä¹°, m = 2), æ­¤æ—¶å¯å–:
$$\Large p_{i} = \frac{s_{i}}{|S|}$$
åˆ™ $p_i$ æ˜¯ä»»æ„æ ·æœ¬çš„å€¼ä¸º $C_i$ çš„æ¦‚ç‡ã€‚åˆ™å¯¹äºç»™å®šçš„æ ·æœ¬, <mark style="background: transparent; color: red">åˆ†ç±»çš„ä¿¡æ¯ç†µä¸ºæŸä¸ªå±äºå„ä¸ªæ ‡ç­¾ä¸åŒå€¼çš„ä¹˜ç§¯</mark>: 
$$\Large\boxed{I(s_{1}, s_{2}, \dots s_{m}) = - \sum^{m}_{i=1} p_{i} \log_{2} (p_{i})}$$
è€ƒè™‘æœ€ç®€å•çš„æƒ…å†µ, æŸä¸ªæ ‡ç­¾ä»…æœ‰1ä¸ªå€¼, æ˜¾ç„¶$p_{i} = 1$ã€‚åˆ™ç†µä¸º0(ä¸éœ€è¦åˆ©ç”¨å…¶ä»–æ ‡ç­¾è¿›è¡Œåˆ†ç±»), <b><mark style="background: transparent; color: blue">è¿™ä¸ªç§°ä¸ºéœ€è¦åˆ†ç±»çš„æ€»ä½“ç†µ, å³æ€»ä½“ä¸ç¡®å®šæ€§ã€‚</mark></b> 

#### 2. èŠ‚ç‚¹ä¿¡æ¯ç†µæœŸæœ›çš„è®¡ç®—
<mark style="background: transparent; color: red">é‡‡ç”¨ä¿¡æ¯ç†µåº¦é‡ç±»åˆ«æ ‡ç­¾å¯¹å„ä¸ªèŠ‚ç‚¹çš„ä¸ç¡®å®šæ€§, å‡è®¾å¯¹äºç‰¹å¾ A(ç±»ä¼¼ä¸Šå›¾ä¸­çš„å¹´é¾„), æœ‰ v ä¸ªä¸åŒçš„å€¼, å¹¶å¯ä»¥é€šè¿‡ A å°† S åˆ’åˆ†ä¸º v ä¸ªå­é›†(S_j), å…¶ä¸­ S çš„å­é›† S_j åœ¨ç‰¹å¾ A ä¸Šå…·æœ‰å€¼ a_j</mark>. æ­¤æ—¶**å¦‚æœé€‰æ‹© A ä½œä¸ºæµ‹è¯•ç‰¹å¾**(å³ç”¨äºåˆ†ç±»Cçš„ç‰¹å¾)ã€‚æ­¤æ—¶, è®¾æœ€ç»ˆéœ€è¦åˆ†çš„ç±»ä¸º $C_i$, è®¾ $s_{ij}$ æ˜¯å­é›† $S_j$ ä¸­çš„ç±» $C_i$ ä¸­çš„æ ·æœ¬æ•°ã€‚åˆ™<b><mark style="background: transparent; color: blue">åˆ©ç”¨ A åˆ’åˆ†æˆçš„å­é›†çš„ç†µçš„æœŸæœ›</mark></b>ç»™å‡ºä¸º(sä¸ºæ ·æœ¬æ€»æ•°):
$$E(A) = \sum^{v}_{j=1}  \frac{s_{1j} + s_{2j} + \dots + s_{mj}}{s} I (s_{1j}, s_{2j},\dots s_{mj})$$
å¯¹äºæƒé‡éƒ¨åˆ†, å¯¹äºCå„ä¸ªåˆ†ç±»ä¸é‡åˆçš„æƒ…å†µå®é™…ä¸Šç­‰äº$a_{j}$ä¸­çš„æ ·æœ¬æ€»æ•°é™¤æ€»çš„æ ·æœ¬æ•°:
$$\text{weight}  =  \frac{s_{1j} + s_{2j} + \dots + s_{mj}}{s}= \frac{|S_{j}|}{s}$$
<b><mark style="background: transparent; color: blue">å…¶ä¸­éœ€è¦æ³¨æ„çš„æ˜¯,</mark></b> å•ç‹¬å­é›†å„è‡ªçš„ç†µ, ä»…è€ƒè™‘æŸä¸ªå­é›† $S_{j}$ æ ·æœ¬ä¸­çš„æˆåŠŸåˆ’åˆ†ä¸ºç±»içš„æ•°é‡:
$$I  (s_{1j}, s_{2j},\dots s_{mj}) =  -\sum^{n}_{i=1} p_{ij}\log_{2} (p_{ij}) \qquad  p_{ij} = \frac{s_{ij}}{|S_{j}|}$$
<mark style="background: transparent; color: red">å‰é¢ä¸€éƒ¨åˆ†ä¸ºç¬¬jä¸ªå­é›†çš„æƒ</mark>,  å½“å­é›†åˆ’åˆ†çº¯åº¦é«˜æ—¶, å‡è®¾ A å¯ä»¥å®Œå…¨åˆ’åˆ†åˆ†ç±» C, åˆ™æ˜¾ç„¶I=0, åŒæ—¶ä¿è¯äº†å½“åˆ’åˆ†æ›´åŠ å‡åŒ€æ—¶, æ‰€å¾—çš„åˆ’åˆ†æœŸæœ›ä»ç„¶æ˜¯è¾ƒå°çš„ã€‚**å¹³å‡èŠ‚ç‚¹ç†µæœ€å°çš„å¯ä»¥ä½œä¸ºæ ¹èŠ‚ç‚¹ä½¿ç”¨**ã€‚

#### 3. ä¿¡æ¯å¢ç›Š
æœ€å, **ä½¿ç”¨ä¿¡æ¯å¢ç›Šæ¥ç¡®å®šå†³ç­–æ ‘çš„åˆ’åˆ†ä¾æ®**ã€‚<mark style="background: transparent; color: red">ä¿¡æ¯å¢ç›Šå®šä¹‰ä¸ºåœ¨å†³ç­–æ ‘æŸä¸ªåˆ†æ”¯ä¸Šæ•´ä¸ªæ•°æ®é›†çš„ä¿¡æ¯ç†µä¸å½“å‰èŠ‚ç‚¹ä¿¡æ¯ç†µçš„å·®å€¼</mark>ã€‚
$$\text{Gain}(A)  =  I (s_{1}, s_{2}\dots  s_{m}) -  E(A) $$
ä¿¡æ¯å¢ç›Šè¡¨ç¤ºäº†<b><mark style="background: transparent; color: blue">å¾—çŸ¥å±æ€§ A ä¹‹å, å¯¼è‡´çš„ç†µå€¼çš„å‹ç¼©æœŸæœ›</mark></b>,  <mark style="background: transparent; color: red">æ­¤æ—¶æˆ‘ä»¬å¯ä»¥å°†æœ€é«˜ä¿¡æ¯å¢ç›Šçš„ç‰¹å¾ä½œä¸º S çš„æµ‹è¯•å±æ€§ï¼Œ å³åˆ›å»ºä»¥æ­¤ä¸ºåˆ†ç±»ä¾æ®çš„èŠ‚ç‚¹</mark>ã€‚è¿™ä¸ªå°±æ˜¯ ID3 å†³ç­–æ ‘çš„ç”Ÿæˆè¿‡ç¨‹ã€‚

å¯¹äºä¸Šè¿°é—®é¢˜, ä¹°çš„å…± 640, ä¸ä¹° 384 äºº, å› æ­¤å¾—åˆ°æ€»çš„ä¿¡æ¯ç†µ:
$$I = I(640,384) = -p1*log2(p1) - p2*log2(p2) = 0.9544$$

### (3) ID3 å†³ç­–æ ‘ç®—æ³•ç¼–ç¨‹å®ä¾‹
ä¾‹å¦‚å¹´é¾„çš„ä¿¡æ¯ç†µä¸º: è®¾è€å¹´ä¸º0, ä¹°ä¸º1 
é¦–å…ˆè®¡ç®—æƒé‡: 
$$p1 = \frac{384}{1024}=0.375\qquad  p2 = \frac{256}{1024} = 0.25,\qquad  p3 = 0.375 $$
$$p11 = \frac{128}{384} = \frac{1}{3} , p21 = \frac{2}{3}  \qquad  I_{1}=-p11*log2(p11)-p21*log2(p21)= 0.9189$$
$$p12 = 1, p22 = 0, I_{2} = 0$$
$$p13 = \frac{128}{384} = \frac{1}{3},p23=\frac{256}{384} = \frac{2}{3}\qquad  I_{3}=0.9189$$
åˆ™å¹³å‡èŠ‚ç‚¹ç†µä¸º:
$$0.375*0.9189 + 0.375 *0.9189 = 0.6891$$
ä¿¡æ¯å¢ç›Šä¸º:
$$\text{Gain} = 0.9544 - 0.6891 = 0.2653$$
åªéœ€ä»¥æ­¤ç±»æ¨ï¼Œè®¡ç®—å‡ºæ‰€æœ‰ç‰¹å¾çš„å¢ç›Š, å–å…¶ä¸­æœ€å¤§çš„ä½œä¸ºæ ¹èŠ‚ç‚¹è¿›è¡Œåˆ†ç±»å³å¯ã€‚
å…·ä½“å†³ç­–æ ‘éƒ¨åˆ†å‚è€ƒ[sklearn å†³ç­–æ ‘éƒ¨åˆ†](https://scikit-learn.org/stable/modules/tree.html)

Iris, Wine å’Œ Breast Cancer å‡å¯ç”¨äºå†³ç­–æ ‘åˆ†ç±»æˆ–è€…å›å½’ç®—æ³•ã€‚

ID3 ç®—æ³•çš„å®ç°: é¦–å…ˆå°†å›¾ç‰‡ç­‰è½¬ä¸º excel è¡¨æ ¼,å†é‡‡ç”¨ pandas è¯»å…¥:
```python 
import pandas as pd  
from pytesseract import image_to_data, image_to_string  
from PIL import Image  
  
data = pd.read_excel("seals_data.xlsx")  
print(data)
```
ç»“æœå¦‚å›¾:
![[attachments/Pasted image 20240912105614.png]]
è½¬åŒ–ä¸ºå†³ç­–æ ‘ç¼–ç : 
![[attachments/Pasted image 20240912113417.png]]
æ­¤æ—¶, 0 ä¸ºä¸è´­ä¹°, 1 ä¸ºè´­ä¹°, åˆ™æŒ‰ç…§å¹´é¾„åˆ†ç»„, é‡‡ç”¨å¦‚ä¸‹ä»£ç :
```python
def calc_entropy_mat(self,counts, decision_arr, target_arr):  
    """  
    calculate the entropy matrix    
    :param counts: the counts array of the current node    
    :param decision_arr: the decision attributes of the current node    
    :param target_arr: target array to be classified of the current node    
    :return:  
    """  
    decisions = np.unique(decision_arr)  
    targets   = np.unique(target_arr)  
  
    cnt_mat = np.zeros((targets.size, decisions.size)) # number of samples of each class for each decision attribute  
    for c in range(targets.size):  
        for d in range(decisions.size):  
            tar = targets[c]  
            dec = decisions[d]  
            cnt_mat[c,d] = np.sum(counts[(decision_arr == dec) & (target_arr == tar)])
```
è®¡ç®—å‡ºå¯¹åº”çš„æ¦‚ç‡çŸ©é˜µå¦‚ä¸‹:
![[attachments/Pasted image 20240912155654.png]]
æˆ‘ä»¬é‡‡ç”¨å¦‚ä¸‹çš„ç»“æ„æ ¼å¼å­˜å‚¨ç›¸åº”çš„çŸ©é˜µ:
ç”±äºæ¯æ¬¡èŠ‚ç‚¹éƒ¨åˆ†åˆ†ç±»å®é™…ä¸Šæ˜¯æ‰¾å­é›†çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬<mark style="background: transparent; color: red">æŠŠæ¯ä¸ªç”¨äºåˆ†ç±»çš„ç‰¹å¾ç§°ä¸ºä¸€ä¸ªdecision, è€Œä¸€ä¸ªdecisionä¸‹çš„ä¸åŒæƒ…å†µç§°ä¸º feature </mark>, æœ€ç»ˆéœ€è¦å½’ç±»çš„ç‰¹å¾ç§°ä¸º targetã€‚
æ¯æ¬¡æˆ‘ä»¬åªéœ€è¦å¾—åˆ° data  çŸ©é˜µ,  
![[Excalidraw/2. å†³ç­–æ ‘å’Œå›å½’æ ‘ 2024-09-12 23.41.10|900]]
å¯»æ‰¾ä¸‹ä¸€ä¸ªæ ‘å»ºç«‹æ‰€éœ€çš„èŠ‚ç‚¹æ—¶, 

æ¯ä¸€æ¬¡æ·»åŠ èŠ‚ç‚¹, åªéœ€è¦ä» features ä¸­æ‰¾å‡ºä¸ä¸º0çš„ç»§ç»­æ·»åŠ å³å¯ã€‚æœ€åä¸€ä¸ªèŠ‚ç‚¹æ˜¯ {target(ä¹°æˆ–ä¸ä¹°) : number } å½¢å¼; å½“ cnt_arr ä»…æœ‰ä¸€ä¸ªéé›¶é¡¹æ—¶. 

å…¶äºŒ, æˆ‘ä»¬æ˜¯åº”å½“ä»¥ decision ä½œä¸ºèŠ‚ç‚¹çš„, è€Œä¸æ˜¯ä»¥ feature ä½œä¸ºèŠ‚ç‚¹ã€‚åŒæ—¶æœ€åçš„å¶å­èŠ‚ç‚¹åŒæ—¶è®°å½•ã€‚æœ€ç»ˆå»ºç«‹çš„æ ‘ç»“æ„å¦‚ä¸‹: (å›¾ä¸­åˆ—å‡ºäº†ä¸‰ç§å…¸å‹çš„ç»“æ„, ç”±äºéœ€è¦å¾—çŸ¥ç»“æœåŠå¯¹åº”çš„æ•°é‡ï¼Œæ‰€ä»¥éœ€è¦å»ºç«‹ä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œè®°å½•)
![[Excalidraw/2. å†³ç­–æ ‘å’Œå›å½’æ ‘ 2024-09-13 00.25.59|650]]

æ­¤æ—¶æˆ‘å¯ä»¥ç¼–å†™å¦‚ä¸‹ç¨‹åºè¿è¡Œå†³ç­–æ ‘:
```python
import numpy  
import numpy as np  
import copy  
import pandas as pd  
from sklearn.preprocessing import LabelEncoder  # encoder labels  
from treelib import Tree, Node  
from sklearn.datasets._base import Bunch  
from dict_to_map import dict2map  
  
class ID3_Tree():  
    """ ID3 decision Tree Algorithm """  
    def __init__(self, counts = None, data = None, target = None, label_encoder = None):  
        if (counts == None or data == None or target == None):  
            self.load_data()  
        else:            self.train_data = Bunch(counts = counts, data = data, target = target)  
            self.label_encoder = label_encoder()  
  
        self.__Init_Tree(self.train_data.counts, self.train_data.data, self.train_data.target)  
        self.build_Tree()  
  
    def to_dict(self, *args, **kwargs):  
        return self.tree  
  
    def load_data (self):  
        """ arrange the data into the correct shapes """  
        data_raw = pd.DataFrame(pd.read_excel("seals_data.xlsx"))  
        label_encoder = LabelEncoder()  
        # eliminate all the white space  
        data_raw = data_raw.map(func=lambda x:x.strip() if isinstance(x, str) else x)  
        data_proceed = pd.DataFrame()  
        data_proceed['è®¡æ•°'] = data_raw.iloc[:, 0]  
        for column in data_raw.columns[1:]:  
            data_proceed[str(column).strip()] = label_encoder.fit_transform(data_raw[column])  
  
        """ split data into 3 part : counts, data and target """  
        counts = np.array(copy.deepcopy(data_proceed.iloc[:,0]))  
        data = np.array(copy.deepcopy(data_proceed.iloc[:,1:-1]))  
        target = np.array(copy.deepcopy(data_proceed.iloc[:,-1]))  
        labels = [str(column).strip() for column in data_raw.columns]  
        self.data_raw = data_raw  
        self.train_data = Bunch(counts = counts, data = data, target = target ,labels = labels) # target = data_raw.iloc[:, 0])  
        self.label_encoder = label_encoder  
  
    def __Init_Tree(self, counts:np.ndarray, data:np.ndarray, target:np.ndarray):  
        self.__check_param(counts, data, target)  
        self.nums   = counts.shape[0]              # number of the type of the samples  
        self.target_num = len(np.unique(target))   # the number of classes (C_i) , i = 1... m  
        self.decision_num = data.shape[1]          # number of decision attributes (D)  
        self.total_num = counts.sum()              # total number of samples (N)  
        self.tree = {}                             # init the tree node  
  
        # calculate the infomation entropy of the entire dataset        targets =  np.unique(target)  
        cls_cnt =  np.array([counts[target == targets[i]].sum() for i in range(targets.size)])  
        cls_prop=  cls_cnt/cls_cnt.sum()  
        self.base_entropy = -np.sum(cls_prop * np.log2(np.where(cls_prop == 0,1e-10, cls_prop)))  
  
    def build_Tree(self):  
        if (self.target_num <= 1):  
            # only 1 class, stop split and return the empty tree  
            self.tree["root"] = self.train_data.target[0]  
            return self.tree  
  
        # initialize the node decision range and target range, we use whole data set to calculate the entropy of root at first  
        dec_range = np.arange(self.decision_num)   # decision attributes  
        tar_range = np.arange(self.nums)           # targets on data  
            # recursive call the calc_entropy_mat function until the class is purely classified.  
        self.tree = self.__build_tree_node(dec_range, tar_range)  
  
    def __check_param(self, counts, data, target):  
        if len(counts.shape)!=1 or len(data.shape)!=2 or len(target.shape)!=1:  
            raise ValueError("The input data is not in the correct shape")  
        elif counts.shape[0]!= data.shape[0] or counts.shape[0] != target.shape[0]:  
            raise ValueError("The input data is not in the correct shape")  
  
    def show(self):  
        map = dict2map(self.tree)  
        tree = Tree.from_map(map)  
        tree.show(line_type="ascii-em", sorting=False)
  
    def __build_tree_node(self, dec_range, tar_range) -> dict:  
        """  
        recursive function,        counts, data, target, node_dec_range, node_tar_range        :param dec_range : decision range (in direction 1 or y)        :param tar_range : target range (in direction 0 or x)        :return: root (name of the root node is defined by decision)  
        """  
        counts = self.train_data.counts[tar_range]  
        data =   self.train_data.data[tar_range][:, dec_range]  
        target = self.train_data.target[tar_range]  
        self.__check_param(counts, data, target)  
  
        gain_list = [self.__get_node_info_gain(counts, data[:, i], target) for i in range(dec_range.size)]  
        best_dec_idx = np.argmax(gain_list)  # best decision (note : relevant to dec_range, not self.train_data)  
  
        best_decision = data[:, best_dec_idx]  
        features = np.unique(best_decision)  
        cnt_mat = self.__get_count_matrix(counts, best_decision, target)  
  
        # use best decision as root node -> delete it from dec_range  
        root_name = "decision" + str(dec_range[best_dec_idx])   # get the location of best decision  
        root = {}  
  
        d2 = numpy.delete(dec_range, best_dec_idx)  # create new dec_range object  
        for i in range(len(features)):  
            feature = features[i]  
            cnt_arr = np.array(cnt_mat[:, i]).squeeze(1)   # change to array and squeeeze to 1 dim  
  
            # record the classification result:            left_tar_idx = np.nonzero(cnt_arr)[0]          # choice leave in this feature  
  
            if (len(left_tar_idx) == 0):  
                raise ValueError("left choices is not zero here!")  
  
            if left_tar_idx.size == 1:  
                # left number can be calculated by cnt_arr[left_choices], t2 = []  
                root["target" + str(left_tar_idx[0]) ] = np.sum(cnt_arr[left_tar_idx])  
            elif d2.size == 0 :  # no available decision left  
                root["dummy"] = np.sum(cnt_arr[left_tar_idx]) # create dummy node  
            else:  
                t2 = tar_range[best_decision == feature]  
                sub_tree = self.__build_tree_node(d2, t2);  
                sub_rootname, sub_root = next(iter(sub_tree.items()))  
                root[sub_rootname] = sub_root  
        return {root_name : root}  
  
    def __get_node_info_gain(self,counts_arr, decision_arr, target_arr):  
        """  
        :param counts_arr: the counts array of the current node        :param decision_arr: the decision attributes of the current node        :param target_arr: target array to be classified of the current node        :return: gain : scalar, infomation gain of the current node  
        """        cnt_mat = self.__get_count_matrix(counts_arr, decision_arr, target_arr)  
        # calculate the information gain of the current node by cnt_mat  
        prob_mat = np.mat(cnt_mat / np.sum(cnt_mat, axis=0))  # calculate probability matrix  
        prob_mat = np.where(prob_mat == 0, 1e-10, prob_mat)   # substitute 0 with 1e-10 to avoid log calculation error  
        node_entropy = -np.sum(np.multiply(prob_mat, np.log2(prob_mat)), axis=0)  
        node_wt = cnt_mat.sum(axis=0) / np.sum(cnt_mat)  
        gain = self.base_entropy - np.multiply(node_wt, node_entropy).sum()  
        return gain  
    def __get_count_matrix(self, counts_arr, decision_arr, target_arr):  
        """  
        calculate the count matrix of the node        :param counts_arr:   the counts array of the current node        :param decision_arr: the decision attributes of the current node        :param target_arr:   target array to be classified of the current node        :return: cnt_mat : np.matrix  
        """        features = np.unique(decision_arr)  
        targets = np.unique(target_arr)  
        cnt_mat = np.array([  
            [np.sum(counts_arr[(decision_arr == dec) & (target_arr == tar)]) for dec in features]  
            for tar in targets  
        ])  
        return np.matrix(cnt_mat)  
  
  
if __name__ == "__main__":  
    tree = ID3_Tree()  
    print(tree.to_dict())  
    tree.show()
```

å…¶ä¸­ dict2map  å‡½æ•°å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸPython/ğŸŒŸPython åŸºç¡€éƒ¨åˆ†/2. Python åŸºæœ¬æ•°æ®ç»“æ„å’Œå¯è§†åŒ–æ–¹æ³•#(1) Python ä¸­çš„æ ‘ç»“æ„|2. Python åŸºæœ¬æ•°æ®ç»“æ„å’Œå¯è§†åŒ–æ–¹æ³•]], å¯ä»¥è¾“å‡ºå¦‚ä¸‹çš„æ ‘ç»“æ„:
```python
{'decision0': {'target1': 256, 'decision3': {'target0': 128, 'target1': 256}, 'decision2': {'target0': 256, 'target1': 128}}}
decision0
â• â•â• decision0-target1 : 256
â• â•â• decision0-decision3
â•‘   â• â•â• decision0-decision3-target0 : 128
â•‘   â•šâ•â• decision0-decision3-target1 : 256
â•šâ•â• decision0-decision2
    â• â•â• decision0-decision2-target0 : 256
    â•šâ•â• decision0-decision2-target1 : 128
```
æ³¨æ„åº”å½“è®¾ç½® Tree çš„ sorting = False æ¥ä¿è¯è¾“å‡ºæ ‘æ˜¯ç›¸åº”çš„å†³ç­–éƒ¨åˆ†, ä¸æ›´æ”¹featureç›¸å¯¹é¡ºåºã€‚

å¯¹åº”çš„ï¼Œå†³ç­–æ ‘åˆ†ç±»çš„ä»£ç ä¹Ÿç›¸å¯¹ç®€å•, åªéœ€è¦æŒ‰ç…§ç›¸åº”çš„ç±»è¿›è¡Œåˆ†ç±»å³å¯;

### (4) C4.5 ç®—æ³•
å¯¹äº C4.5 ç®—æ³•, <b><mark style="background: transparent; color: blue">åŸºæœ¬çš„ç¨‹åºç»“æ„ä¸ ID3 ç›¸åŒ, è€ŒèŠ‚ç‚¹çš„åˆ’åˆ†æ ‡å‡†ä¸Šåšäº†æ”¹è¿›</mark></b>ã€‚å³é‡‡ç”¨ä¿¡æ¯å¢ç›Šç‡ä»£æ›¿ä¿¡æ¯å¢ç›Šã€‚
$$\text{GainRatio}(S,A) = \frac{\text{Gain}(S, A)}{\text{SplitInfo}(S,A)}$$
å…¶ä¸­, SplitInfo(S,A) ä»£è¡¨äº†æŒ‰ç…§ç‰¹å¾ A åˆ’åˆ†Sçš„å¹¿åº¦å’Œå‡åŒ€æ€§ã€‚è®¡ç®—ä¸º:
$$\text{SplitInfo(S,A)} =  - \sum^{c}_{i=1} \frac{|s_{i}|}{|S|} \log_{2} \left(\frac{|S|}{|s_{i}|}\right)$$
å…¶ä¸­ s_j ä¸º A çš„ C ä¸ªä¸åŒçš„å€¼æ„æˆçš„æ ·æœ¬å­é›†ã€‚

å¯¹äºä¸Šè¿°é—®é¢˜, é‡‡ç”¨ C4.5ç®—æ³•è·å–å…¶å†³ç­–æ ‘, éœ€è¦ `pip install c45-decision-tree` è·å– c4.5 çš„å†³ç­–æ ‘åº“ã€‚
å¦å¤–ï¼Œä¸ºäº†ä¿®æ”¹ä¸­æ–‡ä¹±ç é—®é¢˜, è¿›å…¥C45Classifierå‡½æ•°æºç å¹¶æ·»åŠ  `dot.attr(encoding='utf-8')  # Ensure UTF-8 encoding` å’Œ `fontname="SimHei"` éƒ¨åˆ†: 
```python
def generate_tree_diagram(self, graphviz, filename):  
    # Menghasilkan diagram pohon keputusan menggunakan modul graphviz  
    dot = graphviz.Digraph()  
    def build_tree(node, parent_node=None, edge_label=None):  
        if isinstance(node, _DecisionNode):  
            current_node_label = str(node.attribute)  
  
            dot.node(str(id(node)), label=current_node_label)  
            if parent_node:  
                dot.edge(str(id(parent_node)), str(id(node)), label=edge_label, fontname="SimHei")  
  
            for value, child_node in node.children.items():  
                build_tree(child_node, node, value)  
        elif isinstance(node, _LeafNode):  
            current_node_label = f"Class: {node.label}, Weight: {node.weight}"  
            dot.node(str(id(node)), label=current_node_label, shape="box", fontname="SimHei")  
  
            if parent_node:  
                dot.edge(str(id(parent_node)), str(id(node)), label=edge_label, fontname="SimHei")  
  
    build_tree(self.tree)  
    dot.format = 'png'  
    dot.attr(encoding='utf-8')  # Ensure UTF-8 encoding  
    return dot.render(filename, view=False)
```

ç„¶ååªéœ€è¦å¦‚ä¸‹ä»£ç å°±å¯ä»¥æ˜¾ç¤ºå†³ç­–æ ‘äº†: 
```python
from C45 import C45Classifier  
import pandas as pd  
import graphviz  
  
data_raw = pd.DataFrame(pd.read_excel("scripts/seals_data.xlsx"))  
data_raw = data_raw.map(func=lambda x:x.strip() if isinstance(x, str) else x)  
  
counts = data_raw.iloc[:,0]  
data   = data_raw.iloc[:,1:-1]  
target = data_raw.iloc[:,-1]  
  
data_new = []  
target_new = []  
for i in range(counts.size):  
    for j in range(counts[i]):  
        data_new.append(list(data.iloc[i]))  
        target_new.append(target.iloc[i])  
  
model = C45Classifier()  
model.fit(data_new, target_new)  
tree_diagram = model.generate_tree_diagram(graphviz, "tree_diagram")  
graphviz.view(tree_diagram)
print(model.predict([['è€', 'ä½', 'æ˜¯', 'è‰¯']])) # ä¹°
```

![[attachments/tree_diagram.png]]


æ­¤å¤–, sklearn ä¸­ç»™å‡ºäº†å†³ç­–æ ‘åˆ†ç±»å’Œå›å½’æ¨¡å—, ä½†æ˜¯ä¸€èˆ¬ç”¨äºè¿ç»­æ•°æ®: 
```python
from sklearn.tree import DecisionTreeClassifier 
from sklearn.tree import DecisionTreeRegressor 
```

### (5) CART å›å½’æ ‘ç®—æ³• 
CART æ˜¯ä¸€ç§é€šè¿‡å†³ç­–æ ‘æ–¹æ³•å®ç°å›å½’çš„ç®—æ³•, å¯ä»¥ç”¨äºåˆ†ç±»å’Œé¢„æµ‹ã€‚
é¦–å…ˆ, åˆ›å»ºå›å½’æ¨¡å‹æ—¶, æ ·æœ¬å–å€¼åŒ…æ‹¬è§‚å¯Ÿå€¼å’Œè¾“å‡ºå€¼ã€‚ ä¸”è§‚å¯Ÿå€¼å’Œè¾“å‡ºå€¼å‡ä¸ºè¿ç»­çš„ã€‚<b><mark style="background: transparent; color: blue">CART é‡‡ç”¨äº†æœ€å°å‰©ä½™æ–¹å·®åŠæ³•åˆ¤å®šå›å½’æ ‘çš„æœ€ä¼˜åˆ’åˆ†</mark></b>(Squared Residuals Minimization)ã€‚**å°†æ•°æ®é›†è¿›è¡Œåˆ‡åˆ†ä¹‹å, é€šè¿‡çº¿æ€§å›å½’æŠ€æœ¯è¿›è¡Œå»ºæ¨¡**ã€‚
åŸºæœ¬æ€æƒ³æ˜¯, **å†³ç­–æ ‘å°†æ•°æ®é›†åˆ†æˆå­æ¨¡å‹æ•°æ®ï¼Œ å¹¶åˆ©ç”¨çº¿æ€§å›å½’æŠ€æœ¯æ¥è¿›è¡Œå»ºæ¨¡ï¼Œ å¦‚æœåˆ‡åˆ†åçš„å­é›†éš¾ä»¥æ‹Ÿåˆï¼Œ åˆ™ç»§ç»­åˆ‡åˆ†ã€‚è¯¥çº¿æ€§å›å½’æ¨¡å‹ä¹Ÿè¢«ç§°ä¸ºæ¨¡å‹æ ‘**ã€‚

CART ç®—æ³•çš„ä¸»è¦æµç¨‹:
1. å†³ç­–æ ‘ä¸»å‡½æ•°: ä¸»å‡½æ•°æ˜¯é€’å½’å‡½æ•°, ä¸”åŠŸèƒ½æ˜¯æ ¹æ® CART è§„åˆ™ç”Ÿé•¿å‡ºå„ä¸ªèŠ‚ç‚¹, å¹¶æ ¹æ®ç»ˆæ­¢æ¡ä»¶ç»“æŸç®—æ³•ã€‚
	1. é€šè¿‡æœ€å°å‰©ä½™æ–¹å·®ç¡®å®šæœ€ä½³åˆ’åˆ†, å¹¶åˆ›å»ºå¯¹åº”çš„**åˆ’åˆ†èŠ‚ç‚¹**(æœ€å°æ–¹å·®å­å‡½æ•°)ã€‚
	2. åœ¨åˆ’åˆ†èŠ‚ç‚¹å¤„, å°†æ•°æ®é›†è¿›è¡ŒäºŒåˆ† 
	3. æ ¹æ®äºŒåˆ†ç»“æœæ„å»ºæ–°çš„å·¦å³èŠ‚ç‚¹, ä½œä¸ºæ–°çš„ä¸¤ä¸ªåˆ†æ”¯
	4. æ£€éªŒé€’å½’ç»ˆæ­¢æ¡ä»¶, ä¸‹ä¸€æ¬¡èŠ‚ç‚¹è¾“å…¥å˜ä¸ºå…¶åŒ…å«çš„ç›¸åº”æ•°æ®é›†å’Œç±»æ ‡ç­¾ã€‚
2. é‡‡ç”¨æœ€å°å‰©ä½™æ–¹å·®å‡½æ•°è®¡ç®—æ•°æ®çš„æœ€ä¼˜åˆ’åˆ†æ–¹å·®ï¼Œ åˆ’åˆ†åˆ—å’Œåˆ’åˆ†å€¼ã€‚
3. äºŒåˆ†æ•°æ®é›†
4. å‰ªæç­–ç•¥  


åˆ’åˆ†åŸç†æ˜¯, <mark style="background: transparent; color: red">æ¯æ¬¡åˆ’åˆ†å–æŸä¸ª feature åˆ—æ‰€æœ‰çš„å¤§äº value çš„æ ·æœ¬è¡Œ</mark>ã€‚åœ¨**æ¯ä¸ªç‰¹å¾featureä¸Š**è¿›è¡ŒäºŒåˆ†æ•°æ®é›†ã€‚ç„¶åè®¡ç®—åˆ’åˆ†ä¹‹åçš„æ€»æ–¹å·®(æ–¹å·®å’Œ)ï¼Œ å¹¶**å’Œåˆ’åˆ†ä¹‹å‰è¿›è¡Œæ¯”è¾ƒ**ã€‚æœ€ç»ˆç¡®å®šæœ€ä¼˜åˆ’åˆ†ã€‚

#### 1. æ¨¡å‹æ ‘
CART é¢„æµ‹å®é™…ä¸Šæ˜¯åˆ©ç”¨åˆ†æ®µçš„çº¿æ€§å‡½æ•°ä½œä¸ºå¶å­èŠ‚ç‚¹å»æ‹ŸåˆåŸå§‹æ•°æ®ã€‚è€Œå…¶ä¸­çš„<mark style="background: transparent; color: red">æ¯ä¸ªçº¿æ€§å‡½æ•°éƒ½è¢«ç§°ä¸ºä¸€é¢—æ¨¡å‹æ ‘</mark>ã€‚

- ä¸€èˆ¬æ ·æœ¬æ€»ä½“çš„é‡å¤ä¸å¾ˆé«˜, ä½†æ˜¯**å±€éƒ¨æ¨¡å¼ä¸€èˆ¬ä¼šç»å¸¸é‡å¤** 
- æ¨¡å‹ç»™å‡ºäº†æ•°æ®çš„èŒƒå›´ã€‚ 
- ä¼ ç»Ÿçš„å›å½’æ–¹æ³•åŒ…å«çš„ä¿¡æ¯éƒ½ä¸å¦‚æ¨¡å‹æ ‘ä¸°å¯Œã€‚ä»è€Œæ¨¡å‹æ ‘çš„å‡†ç¡®åº¦æ›´é«˜ã€‚

#### 2. å‰ªææ–¹æ³•
ä¸€èˆ¬åœ°, ä¸ºäº†<mark style="background: transparent; color: red">é¿å…è¿‡æ‹Ÿåˆ</mark>, é¢„æµ‹æ ‘é‡‡ç”¨äº†å‰ªæçš„æ–¹æ³•ã€‚å‰ªæåŒ…æ‹¬<mark style="background: transparent; color: red"></mark><b><mark style="background: transparent; color: blue">å…ˆå‰ªæå’Œåå‰ªæ</mark></b>ï¼Œå…ˆå‰ªæä¸€èˆ¬æ–¹æ³•æ˜¯å…ˆç»™å®šä¸€ä¸ªé¢„å®šä¹‰çš„åˆ’åˆ†é˜ˆå€¼, è€Œå½“å­é›†åˆ’åˆ†ä½äºè¿™ä¸ªé˜ˆå€¼ä¹‹åï¼Œå­é›†åˆ’åˆ†ç»ˆæ­¢ã€‚å¦å¤–ä¹ŸåŒ…å«é¢„å‰ªæéƒ¨åˆ†; é¢„å‰ªæä¸ä¼šç”Ÿæˆæ•´ä¸ªå†³ç­–æ ‘ï¼Œ ä¸”ç®—æ³•ç®€å•, é€‚åˆå¤§ä¼°æ‘¸é—®é¢˜çš„ç²—ç•¥ä¼°è®¡ã€‚

åå‰ªææ˜¯åœ¨å®Œå…¨ç”Ÿæˆçš„å†³ç­–æ ‘ä¸­, å»æ‰ä¸å…·æœ‰ä»£è¡¨æ€§çš„å­æ ‘å¹¶ä½¿ç”¨å¶èŠ‚ç‚¹ä»£æ›¿ã€‚<mark style="background: transparent; color: red">ä¸€èˆ¬æ–¹æ³•æ˜¯é€’å½’ä¼°ç®—å†…éƒ¨èŠ‚ç‚¹çš„è¯¯åˆ¤ç‡ï¼Œä½äºæŸä¸ªå€¼æ—¶, å¯å°†å…¶è®¾ç½®ä¸ºå¶å­èŠ‚ç‚¹</mark>ã€‚
å¯¹äºæŸä¸ªå¶å­èŠ‚ç‚¹,  è¦†ç›– $N$ ä¸ªæ ·æœ¬, å‡è®¾å…¶ä¸­æœ‰ $E$ ä¸ªé”™è¯¯ã€‚åˆ™å–é”™è¯¯ç‡è®¡ç®—ä¸º:
$$\frac{E + 0.5}{N}$$
å…¶ä¸­ 0.5 ä¸ºæƒ©ç½šå› å­ã€‚è€ƒè™‘æŸå­æ ‘, é”™è¯¯åˆ†ç±»æ ·æœ¬è®°ä¸º1, æ­£ç¡®åˆ†ç±»æ ·æœ¬è®°ä¸º0ï¼ŒåŒæ—¶å‡è®¾æ ‘çš„**å›ºæœ‰è¯¯åˆ¤ç‡**ä¸º$e$, åˆ™å¯ä»¥æ ¹æ®[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/ğŸ£Probability Theory/é‡è¦å®šç†/å¸¸ç”¨çš„æ¦‚ç‡åˆ†å¸ƒåŠæ•°å­¦æœŸæœ›å’Œæ–¹å·®|å¸¸ç”¨çš„æ¦‚ç‡åˆ†å¸ƒåŠæ•°å­¦æœŸæœ›å’Œæ–¹å·®]](äºŒé¡¹åˆ†å¸ƒ)ä¼°è®¡å‡ºå‡å€¼å’Œæ ‡å‡†å·®: 
$$E = e \times  N \qquad  \text{var} = \sigma  = \sqrt{N e(1 - e)}$$
å¯¹åº”çš„å¶å­ç»“ç‚¹, è¯¯åˆ¤ç‡ä¸º: 
$$e = \frac{E + 0.5}{N}\qquad  E(\text{leaf err cnt}) = N \times e $$
å‰ªææ ‡å‡†å–ä¸º:
$$E(\text{sub tree err})  -   var(\text{sub tree err} )  > E (\text{leaf err cnt})$$
ä¸€èˆ¬ç”±äºCARTæ˜¯äºŒå‰æ ‘ï¼Œæ‰€ä»¥ä¸€èˆ¬åªéœ€è¦é‡‡ç”¨ .left å’Œ .right æ¥å­˜å‚¨å¯¹åº”çš„èŠ‚ç‚¹å°±è¡Œäº†ã€‚æ•´ä½“ç±»ä¼¼äºäºŒå‰æ ‘ã€‚åªéœ€è¦è®¡ç®—æ–¹å·®ã€‚(å®é™…ä¸Šåªéœ€è¦åˆ©ç”¨ä¸€ä¸ªSetå­é›†åˆ†åˆ«è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ–¹å·®å³å¯), å½“æ»¡è¶³æ¡ä»¶æ—¶, åˆ™åˆå¹¶(ä½¿ç”¨å·¦å³å­æ ‘å‡å€¼ä»£æ›¿å­æ ‘)

> [!NOTE] è¯´æ˜
> åœ¨æœ€å¼€å§‹çš„æ ‘ä¸­, ä»…æœ‰å¶å­èŠ‚ç‚¹æœ‰æ„ä¹‰ï¼Œ æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä»»æ„é‡‡ç”¨ä¸­é—´èŠ‚ç‚¹è¿›è¡Œåˆå¹¶å¶å­èŠ‚ç‚¹ã€‚è¿™å°±æ˜¯å‰ªæã€‚å‰ªæä¹‹åä¸­é—´èŠ‚ç‚¹çš„å€¼(ç”¨äºåˆ†å‰²)æˆä¸ºä¸¤ä¸ªå¶å­ç»“ç‚¹å€¼çš„å‡å€¼ã€‚

#### 3. CART å›å½’æ ‘çš„å»ºç«‹ 
```python title:
import sklearn  
from sklearn import tree  
import numpy as np  
import matplotlib.pyplot as plt  
  
x = np.linspace(0, 10, 100)  
y_raw  = np.sin(x)  
y_rand = np.random.rand(100)  
y = y_raw + y_rand * 0.5  
  
fig, ax = plt.subplots()  
clf = tree.DecisionTreeRegressor(max_depth=4) # max layer of the regression tree  
clf.fit(x.reshape(-1, 1), y_raw.reshape(-1, 1))  
y_pred = clf.predict(x.reshape(-1, 1))  
  
plt.scatter(x, y, c='b', marker='.')  
plt.plot(x, y_pred, c='r', linewidth=2)  
plt.show()
```

æœ€ç®€å•çš„ä»£ç å¦‚ä¸Šæ‰€ç¤º, å…¶ä¸­ max_depth è¶Šæ·±, æ‹Ÿåˆè¶Šç²¾ç¡®ã€‚
![[attachments/Pasted image 20240914000254.png]]


