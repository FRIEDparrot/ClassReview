#### 1. softmax函数
softmax函数可以进行归一化，根据三个数值的相对大小,将其变为和为1的概率
$$S_{ij} = \frac{e^{z_i}}{\sum_k e^{z_k}}$$
-  𝑆𝑖是经过softmax的类别概率输出
-  𝑧_𝑘是神经元的输出

不同的值反映出取不同的部分的概率

#### 2. 交叉熵损失函数
交叉熵的定义是根据正确分类数量定义的损失函数, 参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/⚓Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#(3) Loss Function|Loss Function]] 部分, 定义为:
$$\boxed{\Large C = -\underset{i}{\sum} y_{i} \log_{}  \hat{y}_{i}}$$
一般<mark style="background: transparent; color: red">回归问题采用 MSE, MEAE 等等作为损失函数,  分类采用 crossEntropy 作为 Loss </mark>

交叉熵损失函数和Softmax**经常一起使用**，特别是在分类任务中，这种组合被称为“Softmax-交叉熵”。这种搭配非常常见，并且理论上和实践上都得到了广泛应用。

有时, softmax 和交叉熵会进行合并计算, 即有:
$$\text{Softmax: } \hat{y}_j = \frac{\exp(z_j)}{\sum_{k=1}^q \exp(z_k)}$$
$$\text{Cross-entropy: } l(y, \hat{y}) = -\sum_{j=1}^q y_j \log(\hat{y}_{j})$$
需要注意的是, python 中, `torch.nn.CrossEntropyLoss` 已经将 **Softmax** 和 **交叉熵** 损失函数整合在了一起，因此 **不需要在网络最后一层显式添加 Softmax**。直接将 **logits**（未经过Softmax的网络输出）传入即可。
因此采用交叉熵 `nn.CrossEntropyLoss` 时, 不需要 softmax 作为最后一层使用, 而采用  nn.Linear 等函数。

实际上, 如果进行[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/⚓Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#(4) Softmax and Cross-Entropy loss Relation|联合推导]], 则容易得到 :
$$\Large\boxed{l(y, \hat{y}) =  \log\sum^{q}_{k=1} \exp (o_{k}) - \sum^{q}_{j=1} y_{j} o_{j}}$$
并有:
$$\frac{\partial l(y, \hat{y})}{\partial o_{j}} = \text{softmax}(o_{j})- y_{j}$$
