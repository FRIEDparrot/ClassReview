参考文章:
1. https://blog.csdn.net/scott198510/article/details/124779155
2. https://blog.csdn.net/arcers/article/details/88732639
3. https://blog.csdn.net/weixin_45962068/article/details/118410646
4. https://www.cnblogs.com/pinard/p/6638955.html
### (1) 相关概念
首先我们给出基础的状态转移图: 
![[Excalidraw/1. 马尔科夫MCS抽样(MCMC) 2024-12-09 18.41.42|300]]
马尔科夫链需要满足马尔科夫性(转移概率与以前的状态无关); 即<mark style="background: transparent; color: red">对于确定的当前状态</mark>, 转移矩阵总是相同的， 并定义<mark style="background: transparent; color: red">状态转移矩阵</mark>如下:
$$P_{ij}(m , m+n) = P\left\{ X_{m+n}= a_{i} | X_{m} = a_{i}\right\}$$
状态转移矩阵P描述了<mark style="background: transparent; color: red">从列中状态转移到行中状态的概率</mark>， 因而显然;
$$\sum^{n}_{j=1} P_{ij}(m, m+n) = 1$$
由于在马尔科夫链中, ==状态转移概率矩阵$P_{ij}(m,m+n)$是仅和$i,j, n$有关系， 而和$m$无关的==， 因此我们将其简记为$P_{ij}(n)$;
满足C-K 方程 (其中 P (n) 为 n 步时的状态转移矩阵， 即只需要通过第一步进行矩阵连乘即可): 
$$P(n) = P(1) P(n-1) = P(1) P(1) \dots P(1) = P(1)^{n}$$
此时实际上<b><mark style="background: transparent; color: orange">最终的抽样结果收敛于极限分布</mark></b>$\boldsymbol{\pi} = (\pi_{1}, \pi_{2}, \dots )$ :  
$$\boxed{\lim_{n \rightarrow +\infty} P_{ij}(n) \lambda  = \pi_{j}}$$
**其中$\lambda$为任意的向量**且不依赖于$\pi_i$; 显然， 当马氏链收敛时， 则经过 n 步抽样之后得到的 $\pi_{j}$ 为概率分布:
$$\sum^{n}_{j=1} \pi_{j} = 1$$
假设马氏链的极限分布为:
$$P_{ij}(m) > 0, i,j  = 1,2,\dots, N$$

设 $a_{i}$ 的全集为 $S$, 则取 $p_{ij}^{(n)}$ 为第 n 步转移概率, 而状态转移矩阵为 $P$, 一步为 $P^{(S)}$, 则有<mark style="background: transparent; color: red">如下性质</mark>:
$$P^{(n)} = P P^{(n-1)} = P^{n}$$
计算转移概率的通用公式(C-K方程):
$$p_{ij}^{(n)}  = \sum_{k \in  S} p_{ik} ^{(S)} p_{kj}^{(n-1 )} \tag{1}$$
第 $n$ 步转移概率完全由一步转移概率确定: 
$$p_{ij}^{(n)} = \sum_{k_{1}\in S} \sum_{k_{2}\in S}\dots  \sum_{k_{n-1} \in S} p_{ik_{1}} p_{k_{1}k_{2}} \dots  p_{{n-1} j};  p_{ik} ^{(S)} \tag{2}$$

### (2) 马氏链的平稳分布和细致平稳条件
设转移矩阵为$P$, **某个概率分布**为$\pi (x)$ , 当满足以下条件时, 则<b><mark style="background: transparent; color: orange">称为平稳分布</mark></b>:  
$$\boldsymbol{\pi} P = \boldsymbol{\pi}$$
<b><mark style="background: transparent; color: red">此时, 我们可以取分布pi为所需要最终抽样的目标分布， 则此时经过P的若干次变换之后, 最终可以得到</mark></b>$\pi(x)$ <b><mark style="background: transparent; color: red">中进行采样的部分</mark></b>; 
此时, 为了对所有的$\pi$ 均满足要求, 则有:  
$$\sum^{n}_{j=1}P (j, i  ) \pi(j)= \pi(i)\tag{1}$$
而我们定义马尔科夫链的<b><mark style="background: transparent; color: blue">细致平稳条件</mark></b>如下: 
$$\pi(i) P_{ij}  = \pi(j) P_{ji} \qquad \forall i,j \tag{2}$$
> [!caution] 说明
> 细致平稳条件是平稳分布的充分条件， 不是必要条件; 
> 可以通过式子两边同时对i求和证明中间的式子成立: 即
> $$\sum^{n}_{i=1} P(i,j) \pi(i) = \sum^{n}_{i=1} P(j,i) \pi(j)$$
> 此时, 提出右边的$\pi(j)$, 则显然右边和为1, 则得到: 
> $$ \sum^{n}_{i=1} P(i,j) \pi(i) = \pi(j)$$
> 该式的形式与(1)一致

细致平稳条件的构造参考[MCMC方法与示例](https://blog.csdn.net/arcers/article/details/88732639), 即引入**矩阵$Q'$使得** 
$$p(i)Q'(i,j)=p(j)Q'(j,i)\tag{3}$$
其中取$Q' = q(i,j) p_{j} q(j,i)$, 显然上式是成立的;   此时即满足细致平稳条件, 可以进行抽样。
我们取$\alpha_{i,j} =  p_{j}q(j,i)$, 即有$Q' = Q\alpha(i,j)$, 由于原先$p(i) Q(i,j) \neq p(j)Q(j,i)$ , 则$\alpha (i,j)$实际上是一个变换, **得到满足细致平稳条件的状态转移矩阵**; 

> [!NOTE] 说明
> 引入$\alpha(i,j)$实际上是一个从原先状态转移矩阵$Q$到新状态转移矩阵$Q'$的变换, 其目的是最终能够满足平稳分布条件从而保证最终抽样得到的分布是$\pi$, 即**最终抽样的$\pi$均从目标分布中获取**; 而Q只是定义了一个初始抽样$\pi$到另一个抽样$\pi$的转移; 例如$\pi = \{ var1 = 1.5, var2 = 3.0 \}$, 则转移后, $\pi$ 会成为一个新的值, 而对应又有新的状态转移矩阵; 

## (3) MCMC 马尔科夫链蒙特卡洛算法
MCMC方法解决的是给定一个平稳分布$\pi$ , 如何得到状态转移矩阵$P$和获取平稳分布对应的样本。
首先我们**任选一个状态转移矩阵**为$Q_{0}$, 并<mark style="background: transparent; color: red">给定一个已知平稳分布</mark>$\pi(x)$;
首先用$X$ 表示状态向量， 而$Q_0$为状态转移矩阵,  则$X_{0} = x_{0}$, 而<mark style="background: transparent; color: red">初始时， 样本点从状态集中的获取实际上是随机的</mark>; 
采样过程如下:
1. $t$时刻有$X_{t} = x_{t}$, 从$Q(x|x_t)$中<mark style="background: transparent; color: red">通过抽样获取得到</mark>样本$y$, 计算对应状态转移矩阵为$Q_{t}= q(y|x_{t})$; 其中$y$也叫==备选状态==, 是<mark style="background: transparent; color: red">从建议分布中抽取得到的</mark>(一般选择为以$x_t$为中心的n维均匀分布或者等距分布以保证下一个跳转范围不过大); 
2. 从$U(0,1)$中抽取样本点$u$来计算转移概率;
3. 当有$u < \alpha(x_{t}, y) = p(y) * q(x_{t}|y)$时, 则==接收转移, 并取$x_{t+1} = y$, 否则拒绝转移==$x_{t+1} = x_{t}$, 其中$\alpha(x_{t}, y)$是从不满足细致平稳条件到满足细致平稳条件的变换(<mark style="background: transparent; color: red">注意, 转移过程中使用的是条件概率的值</mark>); 
4. <mark style="background: transparent; color: red">注意不同状态下的状态转移矩阵并不是相等的</mark>; 当转移到状态$y$时， 则状态转移矩阵更新为$Q_{t+1} = q(x|y)$, 否则保持原先的状态转移矩阵$Q_{t+1} = Q_{t} = q(x|x_{t})$; )

![[attachments/Pasted image 20240310103923.png|600]]

> [!NOTE] 为何采用$\alpha(i,j)$作为转移概率
> 由于$\alpha(i,j) = \pi(j) \cdot q(i,j)$, 其中包含了给出的转移概率$q(i,j)$和对应的分布概率$\pi(j)$ 
> 另外， 如果矩阵是对称矩阵， 则M-H方法的概率为$\frac{\pi(i)}{\pi(j)}$, 显然状态转移概率还是在$\pi$中概率大的部分， 转移的概率就大, 因此， 经过大量转移之后，落在平稳分布$\pi$概率大的部分的样本就多。<mark style="background: transparent; color: red">最终的样本实际上是所有抽样出的</mark>$\pi(i)$的<mark style="background: transparent; color: red">集总</mark>

`````ad-bug
title: $\alpha$ 作为转移概率的合理性
collapse: open
`````

例如以正态分布为平稳分布， 每一次的$\pi(i)$其实就是一个值, 根据平稳分布转移到下一个, 最终所有的样本$\pi(1), \pi(2)\dots \pi(n)$为获得的拟合平稳分布的值。
![[attachments/Pasted image 20240310134745.png]]
参考 https://www.cnblogs.com/pinard/p/6638955.html 
> [!cite] 补充说明
> 虽然<mark style="background: transparent; color: red">状态转移矩阵在状态转移过程中并不总是相同的</mark>,  但是由于状态转移矩阵$Q'$都是利用$\pi$构造的以$\pi$为平稳分布, 且$\alpha$都是以$\pi$构造的转移概率。因此对于不同状态下的初值，显然最终收敛到的分布$\pi$的分布，即最终迭代结果**收敛于给定的平稳分布**$\pi$， 这样就产生了对应分布的样本集; 
(())

## (4) Metropolis-Hastings 采样方法 
我们考虑到$\alpha(i,j) < 1$且通常较小, 跳转可能性较低, 因此可以通过**放大跳转率**$\alpha(i,j)$获取到: 在M-H方法中, 如果取: 
$$\alpha(i,j) =\frac{ p(j)q(j,i)}{p(i) q(i,j)}\qquad  \alpha(j,i) = 1$$
此时细致平稳条件(3)仍然成立, 即代入之后有: 
$$p(i) \cdot  \frac{p(j) q(j,i)}{p(i) q(i,j)} \cdot   q(i,j) = p(j) q(j,i)$$
由于概率是不可能大于1的, 则将其推广得到:
$$\alpha(i,j) = \min\left(\frac{ p(j)q(j,i)}{p(i) q(i,j)}, 1\right)$$
## (5) Gibbs方法(坐标轮换采样方法)
**单变量的转移**过程如下:
![[attachments/Pasted image 20240310134745.png]] 
参考[cnblogs.com/pinard/p/6645766.html](https://www.cnblogs.com/pinard/p/6645766.html), 我们考虑有多个变量的一个$\pi$值， 用于拟合联合分布;
以二维的$\pi$向量为例，定义$\pi (x_{1}, x_{2})$为二维联合的数据分布; 对应的n个点表示$\pi (x_{1}^{i}, x_{2}^{i})$;

我们仅考虑从$A(x_{1}^{(1)}, x_{2}^{(1)})$变化到$B(x_1^{(1)},x_2^{(2)})$ 的情况时:  首先， 用$\pi (x_{2}^{(2)}| x_{1}^{(1)})$表示$x_{1} = x_{1}^{(1)}$时(在这一列内)， 取到$x_2^{(2)}$的条件概率; 而同时计边缘概率密度为:
$$\pi(x_{1}^{(1)}) = \int_{\alpha}^{\beta}\pi (x_{1}, x)dx$$
则有: 我们配凑下式右边的形式(两式右边相等), 则分别合并前两个得到: 
![[attachments/Pasted image 20240310142405.png|600]]
因此有关系:
$$\Large\boxed{\pi(x_{1}^{(1)}, x_{2}^{(2)}) \pi(x_{2}^{(1)}| x_{1}^{(1)})=\pi (x_{1}^{(1)}, x_{2}^{(1)}) \pi (x_{2}^{(2)}|x_{1}^{(1)})}\tag{5.1}$$
得到:
$$\pi (A) \pi (x_{2}^{(2)}|x_{1}^{(1)})=\pi(B) \pi (x_{2}^{(1)}|x_{1}^{(1)})$$
同样地, 从$A(x_{1}^{(1)}, x_{2}^{(1)})$到$C(x_1^{(2)}, x_2^{(1)})$ 有关系:
$$\pi(A) \pi (x_{1}^{(2)}|x_{2}^{(1)}) =  \pi(C) \pi(x_{1}^{(1)}|x_{2}^{(1)})$$
![[attachments/Pasted image 20240310143810.png|800]]
即<mark style="background: transparent; color: red">构造为如上图的状态转移矩阵之后</mark>,  则有关系:
$$\pi(E) P(E\rightarrow F) = \pi(F) P(F\rightarrow E)$$
其方法就是每一次只取一个坐标轴， 并每一次轮换坐标轴进行垂直采样;  此时, 我们只要按照此条件概率分布$\pi(x_{2}|x_{1}^{(1)})$为状态转移概率时， 显然<b><mark style="background: transparent; color: blue">两点之间的转移满足细致平稳条件(自动满足)</mark></b>, 此时, ==状态转移概率必定为1==, 即每一次抽样都会被接受。
![[attachments/Pasted image 20240310144427.png]]
此时, 我们可以以$\pi(x_{2}^{(2)}| x_{1}^{(1)})$为<mark style="background: transparent; color: red">状态转移概率</mark>, 而每一次采样实际上是
1. 从$P(x_{2}|x_{1})$<mark style="background: transparent; color: red">条件概率</mark>采样$x_2$, 而再从$P(x_1|x_2)$条件概率采样$x_1$， 但是需要注意的是,  不是从“建议分布”中随机抽取的, 而是从条件概率分布中进行按概率进行抽样的。
2. 相比于一维情况的抽样， 实际上是将<mark style="background: transparent; color: red">建议分布抽样和概率拒绝方法</mark>改成了直接在<mark style="background: transparent; color: red">对应的条件概率分布下进行抽样</mark>， 这个区别主要是由于==一维下状态转移矩阵不能确定，因此随机初始化了一个==;  而二维下状态转移矩阵确定了， 即可直接**按照转移概率抽样**。
![[attachments/Pasted image 20240310151354.png|800]]
