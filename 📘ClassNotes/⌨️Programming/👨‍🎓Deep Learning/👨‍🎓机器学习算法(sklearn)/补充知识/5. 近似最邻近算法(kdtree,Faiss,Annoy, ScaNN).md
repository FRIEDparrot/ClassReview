除此之外, SVD 分解, KNN 也是常用的算法;

对于搜索矩阵中的最邻近向量, 建立余弦相似度矩阵等等操作, 除了挨个计算出余弦等距离 (其中 KDTree 是最近邻部分的内容, 可以参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/1.机器学习算法和文本分类挖掘(Naive Bayes)|1.机器学习算法和文本分类挖掘(Naive Bayes)]])
1. 在 400-500 的矩阵规模下，近似最近邻（Approximate Nearest Neighbor, ANN）算法可以显著提高效率，尤其是需要频繁查询时。以下是适合你的需求的几种方法：

### 方法 1: 使用 Scikit-learn 的 KDTree
**适合场景**：中小规模数据（如 400-500 行矩阵），且查询次数较多。
首先如果是稀疏矩阵, 则需要先转换为稠密矩阵

#### 示例代码
```python
from sklearn.neighbors import KDTree
import numpy as np

# 假设矩阵为 data_matrix，维度为 (400, 500)
data_matrix = np.random.rand(400, 500)  # 示例数据
query_vector = np.random.rand(500)     # 查询向量

# 创建 KDTree
tree = KDTree(data_matrix)

# 查询最近邻（返回最近邻的索引和距离）
dist, ind = tree.query(query_vector.reshape(1, -1), k=1)  # k=1 表示最近一个
print(f"最近邻索引: {ind[0][0]}, 距离: {dist[0][0]}")
```

**优点**:
- 对于固定数据集，多次查询速度很快。
- 高维数据（如 500 列）也能处理，但可能性能略受影响。
**缺点**：
- 如果矩阵规模增长显著（>1000行），效率会下降。


### 方法 2: 使用 Faiss
[Facebook AI Similarity Search (Faiss)](https://github.com/facebookresearch/faiss) 是一款高效的最近邻搜索库，专为中高维大数据设计。

但是，需要说明, Faiss 设计目标是高效处理 **密集向量（dense vectors）**，通过 SIMD（单指令多数据）和 GPU 加速来优化性能。本身并不完全支持稀疏矩阵直接作为输入。

#### 示例代码
```python
import faiss
import numpy as np

# 假设矩阵为 data_matrix，维度为 (400, 500)
data_matrix = np.random.rand(400, 500).astype('float32')
query_vector = np.random.rand(500).astype('float32')

# 构建索引（L2 距离）
index = faiss.IndexFlatL2(data_matrix.shape[1])  # 支持欧几里得距离
index.add(data_matrix)  # 添加数据

# 查询最近邻
D, I = index.search(query_vector.reshape(1, -1), 1)  # 查询最近1个
print(f"最近邻索引: {I[0][0]}, 距离: {D[0][0]}")
```

**优点**：
- 针对高维数据的高效实现。
- 支持 GPU 加速（需安装 GPU 版本）。

**缺点**:
- 需要安装额外的库。
- 如果数据集非常小（<100 行），优势不明显。

其中, 如果按行寻找最小距离, 参考[Faiss API](https://github.com/facebookresearch/faiss/wiki/getting-started), 采用: (其中d是对应形状的维数, 也是距离向量的长度), 因此按行向量, 则用 `shape[1]`, faiss 默认输入中, 默认每一行是一个向量(item);


对于余弦相似度矩阵和<mark style="background: transparent; color: red">按列寻找最小距离</mark>, 可以采用:
```python
"""
- indices (numpy.ndarray): Indices of the k nearest neighbors for each column. 
- distances (numpy.ndarray): Distances of the k nearest neighbors for each column.
"""
if metric == "cosine":  
    itemCF_mat = ui_mat /sp.linalg.norm(ui_mat, axis=0).reshape((1,-1))  # normalize by user 
    index = faiss.IndexFlatIP(itemCF_mat.shape[0])   # calculate InnerProduct in column direction  
elif metric == "euclidean": 
    # Use L2 index for Euclidean distance  
    index = faiss.IndexFlatL2(itemCF_mat.shape[0])  # L2 distance  
else:  
    raise ValueError("metric must be 'cosine' or 'euclidean'")
# Add item features to the index
index.add(itemCF_mat.T)  # Faiss expects items as rows (transpose input)
# Calculate the k nearest neighbors for each item
distances, indices = index.search(itemCF_mat.T, k)
if metric == "cosine":
    # Convert inner product back to cosine distance  
    distances = 1 - distances    
return distances, indices
```


```python
index = faiss.IndexFlatL2(itemCF_mat.shape[1])   #  build the index 
index.add(itemCF_mat)
index.d 
distance, indices = index.search(itemCF_mat, k)
```

注意事项
- 如果是高维稀疏数据，使用 `faiss` 的近似索引（如 `IndexIVFFlat`）可能更高效。

另外, 显然 np.linalg.norm 不适用于稀疏矩阵, 因此我们一般采用如下方法进行归一化:
如果你的矩阵是稀疏矩阵，例如用 `csr_matrix` 表示，可以使用 scipy.sparse 中的专用模块, 而不是采用 numpy 部分, 以下说明了如何将 ui_map 归一化为 item_CF 矩阵。
```python
import scipy.sparse as sp
import scipy.sparse.linalg 
itemCF_mat = ui_mat / sp.linalg.norm(ui_mat, axis=0).reshape((1,-1)) 
```
其中 norm 没有 keepDims 参数(与 numpy 不同)，需要手动 reshape

### 方法 3: 使用 Approximate Methods in Annoy
[Annoy](https://github.com/spotify/annoy) 是一种基于随机投影树（Random Projection Tree）的 ANN 方法，适合频繁查询。
#### 示例代码
```python
from annoy import AnnoyIndex
import numpy as np

# 假设矩阵为 data_matrix，维度为 (400, 500)
data_matrix = np.random.rand(400, 500).astype('float32')
query_vector = np.random.rand(500).astype('float32')

# 构建 Annoy 索引
f = data_matrix.shape[1]  # 特征维度
index = AnnoyIndex(f, 'euclidean')  # 欧几里得距离
for i, vec in enumerate(data_matrix):
    index.add_item(i, vec)
index.build(10)  # 构建索引，10 个树

# 查询最近邻
nearest_index = index.get_nns_by_vector(query_vector, 1, include_distances=True)
print(f"最近邻索引: {nearest_index[0][0]}, 距离: {nearest_index[1][0]}")
```

**优点**：

- 快速、简单的近似搜索。
- 数据规模增长时仍保持较高效率。

**缺点**：

- 近似结果，可能略有误差。

---

### 方法选择建议

1. **数据规模和查询次数较少**（<500次）：优先使用 `scikit-learn.KDTree`。
2. **高维数据（如 500 维）且查询频繁**：使用 `Faiss`。
3. **高效近似方法且容易实现**：使用 `Annoy`。

如果你的查询次数较多，优先考虑 **Faiss** 或 **Annoy**，它们在高维数据的场景下具有出色的性能表现。



#### 1. 使用 `ScaNN`（稀疏矩阵支持更好）
[ScaNN](https://github.com/google-research/google-research/tree/master/scann) 是 Google 开发的高效近邻搜索库，支持稀疏矩阵并且内存利用率更高。

```bash
pip install scann
```

以下是一个使用 ScaNN 的示例：

```python
import numpy as np
import tensorflow as tf
import scann

# 假设 mat1 和 mat2 是稀疏矩阵
mat1 = sp.random(100000, 50, density=0.01, format='csr')
mat2 = sp.random(100, 50, density=0.01, format='csr')

# 将稀疏矩阵转换为稠密矩阵
mat1_dense = mat1.toarray()
mat2_dense = mat2.toarray()

# Normalize for cosine similarity
mat1_dense = mat1_dense / np.linalg.norm(mat1_dense, axis=1, keepdims=True)
mat2_dense = mat2_dense / np.linalg.norm(mat2_dense, axis=1, keepdims=True)

# 创建 ScaNN 索引
searcher = scann.scann_ops_pybind.builder(mat1_dense, 10, "dot_product").build()

# 查询最近邻
nearest_indices, _ = searcher.search_batched(mat2_dense)
print(nearest_indices)
```

#### 2. 直接在稀疏矩阵上计算
如果数据规模适中，也可以直接使用稀疏矩阵进行计算。以下是基于 `scipy` 的稀疏矩阵余弦相似度搜索: 

需要说明的时, cos_similarity 对于稀疏矩阵有很好的支持, 当其他求解距离的点不多时, 例如
对于大小为 100000 验证集, 而仅需要求解距离的第二个矩阵为 100, 我们可以直接利用余弦相似度计算, 但是一般地, 得到的 100 已经是 TruncatedSVD 得到的, 显然不能用稀疏矩阵方法.

用法是 cos_similarity(target, origin), 也可以 `scipy.spatial.distance.cdist`, 这个也可以求解欧式距离 (sklearn.metrics.pairwise.pairwise_distances)


> [!CAUTION] 注意
> 需要说明的是, 由于 `cosine_similarity` 或 `pairwise_distances` 的返回值是稠密矩阵，当数据规模较大（例如上百万的物品）时，稠密矩阵会导致内存不足问题。因此我们需要指定 `dense_output=False` 参数，此时得到的 `similarity_sparse` 是一个稀疏矩阵（例如 `scipy.sparse.csr_matrix`）。

```python
from sklearn.metrics.pairwise import cosine_similarity
import scipy.sparse as sp

# 假设 mat1 和 mat2 是稀疏矩阵
mat1 = sp.random(100000, 50, density=0.01, format='csr')
mat2 = sp.random(100, 50, density=0.01, format='csr')

# 直接计算余弦相似度
cos_sim = cosine_similarity(mat2, mat1 , dense_output=False)  # shape (100, 100000)
dist_mat = 1 - cosine_similarity(mat1, dense_output=False)  # cos distance 

nearest_indices = np.argmax(cos_sim, axis=1)
print(nearest_indices)
```


此外也可以如下计算:
```python
from scipy.sparse import linalg

# 对物品-用户矩阵按列进行归一化
ui_mat_norm = ui_mat_t.multiply(1 / linalg.norm(ui_mat_t, axis=0))
# 余弦相似度的稀疏计算（点积代替）
similarity_sparse = ui_mat_norm @ ui_mat_norm.T
```

### 方法对比

| 方法                      | 优点                                               | 缺点                                  |
|---------------------------|----------------------------------------------------|---------------------------------------|
| **Faiss + SVD 降维**      | 高效处理大规模数据，适合稠密矩阵                   | SVD 会导致精度损失                   |
| **ScaNN**                 | 支持稀疏矩阵，高效准确                             | 需要安装新库，配置稍复杂              |
| **直接计算（`scipy`）**   | 实现简单，适合中小规模稀疏矩阵                     | 对于大规模矩阵，内存和计算开销较大    |

如果矩阵 `mat1` 和 `mat2` 足够稀疏并且规模较大，建议使用 **`ScaNN`**，因为它能很好地支持稀疏矩阵并且速度很快。如果需要与 `Faiss` 集成，可以通过 **SVD 降维** 将稀疏数据嵌入到低维密集空间，再用 `Faiss` 搜索。


## 二、寻找前 k 个最小余弦距离
### (1) 三种方法及其时间性能比较
对于 python 中求解余弦最邻近点, 一般有 n 个距离向量, 而我们很容易通过 cos_similarity 等计算出其距离矩阵; 此时我们需要找出其中最小的几个.
```python
if metric == "cosine":  
    # normalize the matrix  
    ui_mat_norm = ui_mat.T / sp.linalg.norm(ui_mat.T, axis=1).reshape(-1, 1)  
    dist_mat = 1 - cosine_similarity(ui_mat_norm)  # calcuate the new item CF matrix  
```

其中求解出的 dist_mat 是与其他所有向量的余弦距离
```python
tm = time.time()  
top_k_indices = []  
for row in dist_mat:  
    small_idx =  heapq.nsmallest(k, range(len(row)), key=row.__getitem__)  
    top_k_indices.append(small_idx)  
print("method1:", time.time() - tm)  
  
tm = time.time()  
top_k_indices = np.argsort(dist_mat)[:, :k]  
print("method2:", time.time() - tm)  
  
tm = time.time()  
top_k_indices = np.argpartition(dist_mat, k, axis=1)[:, :k]  
print("method3:", time.time() - tm)
```

对于 (702, 702) 的 dist_mat, 设 k = 5, 则具体时间性能如下:
```python
method1: 0.08102059364318848
method2: 0.004978656768798828
method3: 0.004000663757324219
```

### (2) 基于 sparse_dot_topn 的 topk 相似度提取
需要说明的是, 对于稀疏矩阵中提取最相似元素, 方法一是遍历行提取, 方法二是采用社区库: `sparse_dot_topn` 是专为稀疏矩阵设计的工具，用于计算稀疏矩阵的点积并提取 Top-k。尽管主要用于稀疏点积，但其 Top-k 功能也可以应用于相似度矩阵中。示例代码如下: 
```python
from sparse_dot_topn import awesome_cossim_topn

def compute_top_k_sparse(similarity_sparse, k):
    """
    使用 sparse_dot_topn 计算稀疏矩阵的 Top-k
    """
    top_k_sparse = awesome_cossim_topn(similarity_sparse, similarity_sparse, k, 0.0)
    return top_k_sparse

# 示例调用
top_k_similarity = compute_top_k_sparse(similarity_sparse, k=10)
```

### 4. **并行化优化**
在稀疏矩阵上进行逐行处理时，`for` 循环不可避免，但可以通过并行化显著加速。例如，可以将每个分块或每行处理分配到多个线程或进程。

#### 多线程或多进程代码（适用于大型矩阵）：
```python
from joblib import Parallel, delayed

def top_k_row(row, k):
    row_data = row.data
    row_indices = row.indices
    if len(row_data) > k:
        top_k_idx = np.argpartition(-row_data, k)[:k]
        top_k_idx_sorted = top_k_idx[np.argsort(-row_data[top_k_idx])]
    else:
        top_k_idx_sorted = np.arange(len(row_data))
    return row_data[top_k_idx_sorted], row_indices[top_k_idx_sorted]

def top_k_sparse_parallel(similarity_sparse, k, n_jobs=4):
    similarity_csr = csr_matrix(similarity_sparse)
    results = Parallel(n_jobs=n_jobs)(
        delayed(top_k_row)(similarity_csr.getrow(i), k) for i in range(similarity_csr.shape[0])
    )
    
    top_k_data, top_k_rows, top_k_cols = [], [], []
    for i, (data, indices) in enumerate(results):
        top_k_data.extend(data)
        top_k_rows.extend([i] * len(data))
        top_k_cols.extend(indices)
    
    return csr_matrix((top_k_data, (top_k_rows, top_k_cols)), shape=similarity_csr.shape)

# 示例调用
top_k_similarity = top_k_sparse_parallel(similarity_sparse, k=10, n_jobs=8)
```
**并行化（方法 4）**：在多核 CPU 系统上非常高效。