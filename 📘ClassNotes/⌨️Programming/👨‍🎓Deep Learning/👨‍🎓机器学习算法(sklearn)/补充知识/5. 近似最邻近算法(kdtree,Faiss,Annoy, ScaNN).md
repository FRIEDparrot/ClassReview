é™¤æ­¤ä¹‹å¤–, SVD åˆ†è§£, KNN ä¹Ÿæ˜¯å¸¸ç”¨çš„ç®—æ³•;

å¯¹äºæœç´¢çŸ©é˜µä¸­çš„æœ€é‚»è¿‘å‘é‡, å»ºç«‹ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µç­‰ç­‰æ“ä½œ, é™¤äº†æŒ¨ä¸ªè®¡ç®—å‡ºä½™å¼¦ç­‰è·ç¦» (å…¶ä¸­ KDTree æ˜¯æœ€è¿‘é‚»éƒ¨åˆ†çš„å†…å®¹, å¯ä»¥å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/1.æœºå™¨å­¦ä¹ ç®—æ³•å’Œæ–‡æœ¬åˆ†ç±»æŒ–æ˜(Naive Bayes)|1.æœºå™¨å­¦ä¹ ç®—æ³•å’Œæ–‡æœ¬åˆ†ç±»æŒ–æ˜(Naive Bayes)]])
1. åœ¨ 400-500 çš„çŸ©é˜µè§„æ¨¡ä¸‹ï¼Œè¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆApproximate Nearest Neighbor, ANNï¼‰ç®—æ³•å¯ä»¥æ˜¾è‘—æé«˜æ•ˆç‡ï¼Œå°¤å…¶æ˜¯éœ€è¦é¢‘ç¹æŸ¥è¯¢æ—¶ã€‚ä»¥ä¸‹æ˜¯é€‚åˆä½ çš„éœ€æ±‚çš„å‡ ç§æ–¹æ³•ï¼š

### æ–¹æ³• 1: ä½¿ç”¨ Scikit-learn çš„ KDTree
**é€‚åˆåœºæ™¯**ï¼šä¸­å°è§„æ¨¡æ•°æ®ï¼ˆå¦‚ 400-500 è¡ŒçŸ©é˜µï¼‰ï¼Œä¸”æŸ¥è¯¢æ¬¡æ•°è¾ƒå¤šã€‚
é¦–å…ˆå¦‚æœæ˜¯ç¨€ç–çŸ©é˜µ, åˆ™éœ€è¦å…ˆè½¬æ¢ä¸ºç¨ å¯†çŸ©é˜µ

#### ç¤ºä¾‹ä»£ç 
```python
from sklearn.neighbors import KDTree
import numpy as np

# å‡è®¾çŸ©é˜µä¸º data_matrixï¼Œç»´åº¦ä¸º (400, 500)
data_matrix = np.random.rand(400, 500)  # ç¤ºä¾‹æ•°æ®
query_vector = np.random.rand(500)     # æŸ¥è¯¢å‘é‡

# åˆ›å»º KDTree
tree = KDTree(data_matrix)

# æŸ¥è¯¢æœ€è¿‘é‚»ï¼ˆè¿”å›æœ€è¿‘é‚»çš„ç´¢å¼•å’Œè·ç¦»ï¼‰
dist, ind = tree.query(query_vector.reshape(1, -1), k=1)  # k=1 è¡¨ç¤ºæœ€è¿‘ä¸€ä¸ª
print(f"æœ€è¿‘é‚»ç´¢å¼•: {ind[0][0]}, è·ç¦»: {dist[0][0]}")
```

**ä¼˜ç‚¹**:
- å¯¹äºå›ºå®šæ•°æ®é›†ï¼Œå¤šæ¬¡æŸ¥è¯¢é€Ÿåº¦å¾ˆå¿«ã€‚
- é«˜ç»´æ•°æ®ï¼ˆå¦‚ 500 åˆ—ï¼‰ä¹Ÿèƒ½å¤„ç†ï¼Œä½†å¯èƒ½æ€§èƒ½ç•¥å—å½±å“ã€‚
**ç¼ºç‚¹**ï¼š
- å¦‚æœçŸ©é˜µè§„æ¨¡å¢é•¿æ˜¾è‘—ï¼ˆ>1000è¡Œï¼‰ï¼Œæ•ˆç‡ä¼šä¸‹é™ã€‚


### æ–¹æ³• 2: ä½¿ç”¨ Faiss
[Facebook AI Similarity Search (Faiss)](https://github.com/facebookresearch/faiss) æ˜¯ä¸€æ¬¾é«˜æ•ˆçš„æœ€è¿‘é‚»æœç´¢åº“ï¼Œä¸“ä¸ºä¸­é«˜ç»´å¤§æ•°æ®è®¾è®¡ã€‚

ä½†æ˜¯ï¼Œéœ€è¦è¯´æ˜, Faiss è®¾è®¡ç›®æ ‡æ˜¯é«˜æ•ˆå¤„ç† **å¯†é›†å‘é‡ï¼ˆdense vectorsï¼‰**ï¼Œé€šè¿‡ SIMDï¼ˆå•æŒ‡ä»¤å¤šæ•°æ®ï¼‰å’Œ GPU åŠ é€Ÿæ¥ä¼˜åŒ–æ€§èƒ½ã€‚æœ¬èº«å¹¶ä¸å®Œå…¨æ”¯æŒç¨€ç–çŸ©é˜µç›´æ¥ä½œä¸ºè¾“å…¥ã€‚

#### ç¤ºä¾‹ä»£ç 
```python
import faiss
import numpy as np

# å‡è®¾çŸ©é˜µä¸º data_matrixï¼Œç»´åº¦ä¸º (400, 500)
data_matrix = np.random.rand(400, 500).astype('float32')
query_vector = np.random.rand(500).astype('float32')

# æ„å»ºç´¢å¼•ï¼ˆL2 è·ç¦»ï¼‰
index = faiss.IndexFlatL2(data_matrix.shape[1])  # æ”¯æŒæ¬§å‡ é‡Œå¾—è·ç¦»
index.add(data_matrix)  # æ·»åŠ æ•°æ®

# æŸ¥è¯¢æœ€è¿‘é‚»
D, I = index.search(query_vector.reshape(1, -1), 1)  # æŸ¥è¯¢æœ€è¿‘1ä¸ª
print(f"æœ€è¿‘é‚»ç´¢å¼•: {I[0][0]}, è·ç¦»: {D[0][0]}")
```

**ä¼˜ç‚¹**ï¼š
- é’ˆå¯¹é«˜ç»´æ•°æ®çš„é«˜æ•ˆå®ç°ã€‚
- æ”¯æŒ GPU åŠ é€Ÿï¼ˆéœ€å®‰è£… GPU ç‰ˆæœ¬ï¼‰ã€‚

**ç¼ºç‚¹**:
- éœ€è¦å®‰è£…é¢å¤–çš„åº“ã€‚
- å¦‚æœæ•°æ®é›†éå¸¸å°ï¼ˆ<100 è¡Œï¼‰ï¼Œä¼˜åŠ¿ä¸æ˜æ˜¾ã€‚

å…¶ä¸­, å¦‚æœæŒ‰è¡Œå¯»æ‰¾æœ€å°è·ç¦», å‚è€ƒ[Faiss API](https://github.com/facebookresearch/faiss/wiki/getting-started), é‡‡ç”¨: (å…¶ä¸­dæ˜¯å¯¹åº”å½¢çŠ¶çš„ç»´æ•°, ä¹Ÿæ˜¯è·ç¦»å‘é‡çš„é•¿åº¦), å› æ­¤æŒ‰è¡Œå‘é‡, åˆ™ç”¨ `shape[1]`, faiss é»˜è®¤è¾“å…¥ä¸­, é»˜è®¤æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå‘é‡(item);


å¯¹äºä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µå’Œ<mark style="background: transparent; color: red">æŒ‰åˆ—å¯»æ‰¾æœ€å°è·ç¦»</mark>, å¯ä»¥é‡‡ç”¨:
```python
"""
- indices (numpy.ndarray): Indices of the k nearest neighbors for each column. 
- distances (numpy.ndarray): Distances of the k nearest neighbors for each column.
"""
if metric == "cosine":  
    itemCF_mat = ui_mat /sp.linalg.norm(ui_mat, axis=0).reshape((1,-1))  # normalize by user 
    index = faiss.IndexFlatIP(itemCF_mat.shape[0])   # calculate InnerProduct in column direction  
elif metric == "euclidean": 
    # Use L2 index for Euclidean distance  
    index = faiss.IndexFlatL2(itemCF_mat.shape[0])  # L2 distance  
else:  
    raise ValueError("metric must be 'cosine' or 'euclidean'")
# Add item features to the index
index.add(itemCF_mat.T)  # Faiss expects items as rows (transpose input)
# Calculate the k nearest neighbors for each item
distances, indices = index.search(itemCF_mat.T, k)
if metric == "cosine":
    # Convert inner product back to cosine distance  
    distances = 1 - distances    
return distances, indices
```


```python
index = faiss.IndexFlatL2(itemCF_mat.shape[1])   #  build the index 
index.add(itemCF_mat)
index.d 
distance, indices = index.search(itemCF_mat, k)
```

æ³¨æ„äº‹é¡¹
- å¦‚æœæ˜¯é«˜ç»´ç¨€ç–æ•°æ®ï¼Œä½¿ç”¨ `faiss` çš„è¿‘ä¼¼ç´¢å¼•ï¼ˆå¦‚ `IndexIVFFlat`ï¼‰å¯èƒ½æ›´é«˜æ•ˆã€‚

å¦å¤–, æ˜¾ç„¶ np.linalg.norm ä¸é€‚ç”¨äºç¨€ç–çŸ©é˜µ, å› æ­¤æˆ‘ä»¬ä¸€èˆ¬é‡‡ç”¨å¦‚ä¸‹æ–¹æ³•è¿›è¡Œå½’ä¸€åŒ–:
å¦‚æœä½ çš„çŸ©é˜µæ˜¯ç¨€ç–çŸ©é˜µï¼Œä¾‹å¦‚ç”¨ `csr_matrix` è¡¨ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ scipy.sparse ä¸­çš„ä¸“ç”¨æ¨¡å—, è€Œä¸æ˜¯é‡‡ç”¨ numpy éƒ¨åˆ†, ä»¥ä¸‹è¯´æ˜äº†å¦‚ä½•å°† ui_map å½’ä¸€åŒ–ä¸º item_CF çŸ©é˜µã€‚
```python
import scipy.sparse as sp
import scipy.sparse.linalg 
itemCF_mat = ui_mat / sp.linalg.norm(ui_mat, axis=0).reshape((1,-1)) 
```
å…¶ä¸­ norm æ²¡æœ‰ keepDims å‚æ•°(ä¸ numpy ä¸åŒ)ï¼Œéœ€è¦æ‰‹åŠ¨ reshape

### æ–¹æ³• 3: ä½¿ç”¨ Approximate Methods in Annoy
[Annoy](https://github.com/spotify/annoy) æ˜¯ä¸€ç§åŸºäºéšæœºæŠ•å½±æ ‘ï¼ˆRandom Projection Treeï¼‰çš„ ANN æ–¹æ³•ï¼Œé€‚åˆé¢‘ç¹æŸ¥è¯¢ã€‚
#### ç¤ºä¾‹ä»£ç 
```python
from annoy import AnnoyIndex
import numpy as np

# å‡è®¾çŸ©é˜µä¸º data_matrixï¼Œç»´åº¦ä¸º (400, 500)
data_matrix = np.random.rand(400, 500).astype('float32')
query_vector = np.random.rand(500).astype('float32')

# æ„å»º Annoy ç´¢å¼•
f = data_matrix.shape[1]  # ç‰¹å¾ç»´åº¦
index = AnnoyIndex(f, 'euclidean')  # æ¬§å‡ é‡Œå¾—è·ç¦»
for i, vec in enumerate(data_matrix):
    index.add_item(i, vec)
index.build(10)  # æ„å»ºç´¢å¼•ï¼Œ10 ä¸ªæ ‘

# æŸ¥è¯¢æœ€è¿‘é‚»
nearest_index = index.get_nns_by_vector(query_vector, 1, include_distances=True)
print(f"æœ€è¿‘é‚»ç´¢å¼•: {nearest_index[0][0]}, è·ç¦»: {nearest_index[1][0]}")
```

**ä¼˜ç‚¹**ï¼š

- å¿«é€Ÿã€ç®€å•çš„è¿‘ä¼¼æœç´¢ã€‚
- æ•°æ®è§„æ¨¡å¢é•¿æ—¶ä»ä¿æŒè¾ƒé«˜æ•ˆç‡ã€‚

**ç¼ºç‚¹**ï¼š

- è¿‘ä¼¼ç»“æœï¼Œå¯èƒ½ç•¥æœ‰è¯¯å·®ã€‚

---

### æ–¹æ³•é€‰æ‹©å»ºè®®

1. **æ•°æ®è§„æ¨¡å’ŒæŸ¥è¯¢æ¬¡æ•°è¾ƒå°‘**ï¼ˆ<500æ¬¡ï¼‰ï¼šä¼˜å…ˆä½¿ç”¨ `scikit-learn.KDTree`ã€‚
2. **é«˜ç»´æ•°æ®ï¼ˆå¦‚ 500 ç»´ï¼‰ä¸”æŸ¥è¯¢é¢‘ç¹**ï¼šä½¿ç”¨ `Faiss`ã€‚
3. **é«˜æ•ˆè¿‘ä¼¼æ–¹æ³•ä¸”å®¹æ˜“å®ç°**ï¼šä½¿ç”¨ `Annoy`ã€‚

å¦‚æœä½ çš„æŸ¥è¯¢æ¬¡æ•°è¾ƒå¤šï¼Œä¼˜å…ˆè€ƒè™‘ **Faiss** æˆ– **Annoy**ï¼Œå®ƒä»¬åœ¨é«˜ç»´æ•°æ®çš„åœºæ™¯ä¸‹å…·æœ‰å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚



#### 1. ä½¿ç”¨ `ScaNN`ï¼ˆç¨€ç–çŸ©é˜µæ”¯æŒæ›´å¥½ï¼‰
[ScaNN](https://github.com/google-research/google-research/tree/master/scann) æ˜¯ Google å¼€å‘çš„é«˜æ•ˆè¿‘é‚»æœç´¢åº“ï¼Œæ”¯æŒç¨€ç–çŸ©é˜µå¹¶ä¸”å†…å­˜åˆ©ç”¨ç‡æ›´é«˜ã€‚

```bash
pip install scann
```

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ ScaNN çš„ç¤ºä¾‹ï¼š

```python
import numpy as np
import tensorflow as tf
import scann

# å‡è®¾ mat1 å’Œ mat2 æ˜¯ç¨€ç–çŸ©é˜µ
mat1 = sp.random(100000, 50, density=0.01, format='csr')
mat2 = sp.random(100, 50, density=0.01, format='csr')

# å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºç¨ å¯†çŸ©é˜µ
mat1_dense = mat1.toarray()
mat2_dense = mat2.toarray()

# Normalize for cosine similarity
mat1_dense = mat1_dense / np.linalg.norm(mat1_dense, axis=1, keepdims=True)
mat2_dense = mat2_dense / np.linalg.norm(mat2_dense, axis=1, keepdims=True)

# åˆ›å»º ScaNN ç´¢å¼•
searcher = scann.scann_ops_pybind.builder(mat1_dense, 10, "dot_product").build()

# æŸ¥è¯¢æœ€è¿‘é‚»
nearest_indices, _ = searcher.search_batched(mat2_dense)
print(nearest_indices)
```

#### 2. ç›´æ¥åœ¨ç¨€ç–çŸ©é˜µä¸Šè®¡ç®—
å¦‚æœæ•°æ®è§„æ¨¡é€‚ä¸­ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ç¨€ç–çŸ©é˜µè¿›è¡Œè®¡ç®—ã€‚ä»¥ä¸‹æ˜¯åŸºäº `scipy` çš„ç¨€ç–çŸ©é˜µä½™å¼¦ç›¸ä¼¼åº¦æœç´¢: 

éœ€è¦è¯´æ˜çš„æ—¶, cos_similarity å¯¹äºç¨€ç–çŸ©é˜µæœ‰å¾ˆå¥½çš„æ”¯æŒ, å½“å…¶ä»–æ±‚è§£è·ç¦»çš„ç‚¹ä¸å¤šæ—¶, ä¾‹å¦‚
å¯¹äºå¤§å°ä¸º 100000 éªŒè¯é›†, è€Œä»…éœ€è¦æ±‚è§£è·ç¦»çš„ç¬¬äºŒä¸ªçŸ©é˜µä¸º 100, æˆ‘ä»¬å¯ä»¥ç›´æ¥åˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—, ä½†æ˜¯ä¸€èˆ¬åœ°, å¾—åˆ°çš„ 100 å·²ç»æ˜¯ TruncatedSVD å¾—åˆ°çš„, æ˜¾ç„¶ä¸èƒ½ç”¨ç¨€ç–çŸ©é˜µæ–¹æ³•.

ç”¨æ³•æ˜¯ cos_similarity(target, origin), ä¹Ÿå¯ä»¥ `scipy.spatial.distance.cdist`, è¿™ä¸ªä¹Ÿå¯ä»¥æ±‚è§£æ¬§å¼è·ç¦» (sklearn.metrics.pairwise.pairwise_distances)


> [!CAUTION] æ³¨æ„
> éœ€è¦è¯´æ˜çš„æ˜¯, ç”±äº `cosine_similarity` æˆ– `pairwise_distances` çš„è¿”å›å€¼æ˜¯ç¨ å¯†çŸ©é˜µï¼Œå½“æ•°æ®è§„æ¨¡è¾ƒå¤§ï¼ˆä¾‹å¦‚ä¸Šç™¾ä¸‡çš„ç‰©å“ï¼‰æ—¶ï¼Œç¨ å¯†çŸ©é˜µä¼šå¯¼è‡´å†…å­˜ä¸è¶³é—®é¢˜ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦æŒ‡å®š `dense_output=False` å‚æ•°ï¼Œæ­¤æ—¶å¾—åˆ°çš„ `similarity_sparse` æ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼ˆä¾‹å¦‚ `scipy.sparse.csr_matrix`ï¼‰ã€‚

```python
from sklearn.metrics.pairwise import cosine_similarity
import scipy.sparse as sp

# å‡è®¾ mat1 å’Œ mat2 æ˜¯ç¨€ç–çŸ©é˜µ
mat1 = sp.random(100000, 50, density=0.01, format='csr')
mat2 = sp.random(100, 50, density=0.01, format='csr')

# ç›´æ¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
cos_sim = cosine_similarity(mat2, mat1 , dense_output=False)  # shape (100, 100000)
dist_mat = 1 - cosine_similarity(mat1, dense_output=False)  # cos distance 

nearest_indices = np.argmax(cos_sim, axis=1)
print(nearest_indices)
```


æ­¤å¤–ä¹Ÿå¯ä»¥å¦‚ä¸‹è®¡ç®—:
```python
from scipy.sparse import linalg

# å¯¹ç‰©å“-ç”¨æˆ·çŸ©é˜µæŒ‰åˆ—è¿›è¡Œå½’ä¸€åŒ–
ui_mat_norm = ui_mat_t.multiply(1 / linalg.norm(ui_mat_t, axis=0))
# ä½™å¼¦ç›¸ä¼¼åº¦çš„ç¨€ç–è®¡ç®—ï¼ˆç‚¹ç§¯ä»£æ›¿ï¼‰
similarity_sparse = ui_mat_norm @ ui_mat_norm.T
```

### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³•                      | ä¼˜ç‚¹                                               | ç¼ºç‚¹                                  |
|---------------------------|----------------------------------------------------|---------------------------------------|
| **Faiss + SVD é™ç»´**      | é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼Œé€‚åˆç¨ å¯†çŸ©é˜µ                   | SVD ä¼šå¯¼è‡´ç²¾åº¦æŸå¤±                   |
| **ScaNN**                 | æ”¯æŒç¨€ç–çŸ©é˜µï¼Œé«˜æ•ˆå‡†ç¡®                             | éœ€è¦å®‰è£…æ–°åº“ï¼Œé…ç½®ç¨å¤æ‚              |
| **ç›´æ¥è®¡ç®—ï¼ˆ`scipy`ï¼‰**   | å®ç°ç®€å•ï¼Œé€‚åˆä¸­å°è§„æ¨¡ç¨€ç–çŸ©é˜µ                     | å¯¹äºå¤§è§„æ¨¡çŸ©é˜µï¼Œå†…å­˜å’Œè®¡ç®—å¼€é”€è¾ƒå¤§    |

å¦‚æœçŸ©é˜µ `mat1` å’Œ `mat2` è¶³å¤Ÿç¨€ç–å¹¶ä¸”è§„æ¨¡è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨ **`ScaNN`**ï¼Œå› ä¸ºå®ƒèƒ½å¾ˆå¥½åœ°æ”¯æŒç¨€ç–çŸ©é˜µå¹¶ä¸”é€Ÿåº¦å¾ˆå¿«ã€‚å¦‚æœéœ€è¦ä¸ `Faiss` é›†æˆï¼Œå¯ä»¥é€šè¿‡ **SVD é™ç»´** å°†ç¨€ç–æ•°æ®åµŒå…¥åˆ°ä½ç»´å¯†é›†ç©ºé—´ï¼Œå†ç”¨ `Faiss` æœç´¢ã€‚


## äºŒã€å¯»æ‰¾å‰ k ä¸ªæœ€å°ä½™å¼¦è·ç¦»
### (1) ä¸‰ç§æ–¹æ³•åŠå…¶æ—¶é—´æ€§èƒ½æ¯”è¾ƒ
å¯¹äº python ä¸­æ±‚è§£ä½™å¼¦æœ€é‚»è¿‘ç‚¹, ä¸€èˆ¬æœ‰ n ä¸ªè·ç¦»å‘é‡, è€Œæˆ‘ä»¬å¾ˆå®¹æ˜“é€šè¿‡ cos_similarity ç­‰è®¡ç®—å‡ºå…¶è·ç¦»çŸ©é˜µ; æ­¤æ—¶æˆ‘ä»¬éœ€è¦æ‰¾å‡ºå…¶ä¸­æœ€å°çš„å‡ ä¸ª.
```python
if metric == "cosine":  
    # normalize the matrix  
    ui_mat_norm = ui_mat.T / sp.linalg.norm(ui_mat.T, axis=1).reshape(-1, 1)  
    dist_mat = 1 - cosine_similarity(ui_mat_norm)  # calcuate the new item CF matrix  
```

å…¶ä¸­æ±‚è§£å‡ºçš„ dist_mat æ˜¯ä¸å…¶ä»–æ‰€æœ‰å‘é‡çš„ä½™å¼¦è·ç¦»
```python
tm = time.time()  
top_k_indices = []  
for row in dist_mat:  
    small_idx =  heapq.nsmallest(k, range(len(row)), key=row.__getitem__)  
    top_k_indices.append(small_idx)  
print("method1:", time.time() - tm)  
  
tm = time.time()  
top_k_indices = np.argsort(dist_mat)[:, :k]  
print("method2:", time.time() - tm)  
  
tm = time.time()  
top_k_indices = np.argpartition(dist_mat, k, axis=1)[:, :k]  
print("method3:", time.time() - tm)
```

å¯¹äº (702, 702) çš„ dist_mat, è®¾ k = 5, åˆ™å…·ä½“æ—¶é—´æ€§èƒ½å¦‚ä¸‹:
```python
method1: 0.08102059364318848
method2: 0.004978656768798828
method3: 0.004000663757324219
```

### (2) åŸºäº sparse_dot_topn çš„ topk ç›¸ä¼¼åº¦æå–
éœ€è¦è¯´æ˜çš„æ˜¯, å¯¹äºç¨€ç–çŸ©é˜µä¸­æå–æœ€ç›¸ä¼¼å…ƒç´ , æ–¹æ³•ä¸€æ˜¯éå†è¡Œæå–, æ–¹æ³•äºŒæ˜¯é‡‡ç”¨ç¤¾åŒºåº“: `sparse_dot_topn` æ˜¯ä¸“ä¸ºç¨€ç–çŸ©é˜µè®¾è®¡çš„å·¥å…·ï¼Œç”¨äºè®¡ç®—ç¨€ç–çŸ©é˜µçš„ç‚¹ç§¯å¹¶æå– Top-kã€‚å°½ç®¡ä¸»è¦ç”¨äºç¨€ç–ç‚¹ç§¯ï¼Œä½†å…¶ Top-k åŠŸèƒ½ä¹Ÿå¯ä»¥åº”ç”¨äºç›¸ä¼¼åº¦çŸ©é˜µä¸­ã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹: 
```python
from sparse_dot_topn import awesome_cossim_topn

def compute_top_k_sparse(similarity_sparse, k):
    """
    ä½¿ç”¨ sparse_dot_topn è®¡ç®—ç¨€ç–çŸ©é˜µçš„ Top-k
    """
    top_k_sparse = awesome_cossim_topn(similarity_sparse, similarity_sparse, k, 0.0)
    return top_k_sparse

# ç¤ºä¾‹è°ƒç”¨
top_k_similarity = compute_top_k_sparse(similarity_sparse, k=10)
```

### 4. **å¹¶è¡ŒåŒ–ä¼˜åŒ–**
åœ¨ç¨€ç–çŸ©é˜µä¸Šè¿›è¡Œé€è¡Œå¤„ç†æ—¶ï¼Œ`for` å¾ªç¯ä¸å¯é¿å…ï¼Œä½†å¯ä»¥é€šè¿‡å¹¶è¡ŒåŒ–æ˜¾è‘—åŠ é€Ÿã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†æ¯ä¸ªåˆ†å—æˆ–æ¯è¡Œå¤„ç†åˆ†é…åˆ°å¤šä¸ªçº¿ç¨‹æˆ–è¿›ç¨‹ã€‚

#### å¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹ä»£ç ï¼ˆé€‚ç”¨äºå¤§å‹çŸ©é˜µï¼‰ï¼š
```python
from joblib import Parallel, delayed

def top_k_row(row, k):
    row_data = row.data
    row_indices = row.indices
    if len(row_data) > k:
        top_k_idx = np.argpartition(-row_data, k)[:k]
        top_k_idx_sorted = top_k_idx[np.argsort(-row_data[top_k_idx])]
    else:
        top_k_idx_sorted = np.arange(len(row_data))
    return row_data[top_k_idx_sorted], row_indices[top_k_idx_sorted]

def top_k_sparse_parallel(similarity_sparse, k, n_jobs=4):
    similarity_csr = csr_matrix(similarity_sparse)
    results = Parallel(n_jobs=n_jobs)(
        delayed(top_k_row)(similarity_csr.getrow(i), k) for i in range(similarity_csr.shape[0])
    )
    
    top_k_data, top_k_rows, top_k_cols = [], [], []
    for i, (data, indices) in enumerate(results):
        top_k_data.extend(data)
        top_k_rows.extend([i] * len(data))
        top_k_cols.extend(indices)
    
    return csr_matrix((top_k_data, (top_k_rows, top_k_cols)), shape=similarity_csr.shape)

# ç¤ºä¾‹è°ƒç”¨
top_k_similarity = top_k_sparse_parallel(similarity_sparse, k=10, n_jobs=8)
```
**å¹¶è¡ŒåŒ–ï¼ˆæ–¹æ³• 4ï¼‰**ï¼šåœ¨å¤šæ ¸ CPU ç³»ç»Ÿä¸Šéå¸¸é«˜æ•ˆã€‚