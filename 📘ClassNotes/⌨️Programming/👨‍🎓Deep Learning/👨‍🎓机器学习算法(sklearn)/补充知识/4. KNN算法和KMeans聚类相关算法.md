### 一、KNN算法
KNN(K-Nearest Neighbor Classification)算法, 也叫K最邻近分类算法, 是一种常用的识别，分类算法。
KNN算法要求分成的类别是已知的, 且训练样本点的类别均已知。

基本过程如下:
1. **对于每个点, 求解数据点到其余各个训练样本点的距离d**。
2. **求解对应的最邻近的K个训练样本点**。
3. 判断样本点中最多的点的类别， 并将该点与其他归为一类。 
优点是简单方便, 但是对噪声数据比较敏感(k不同，可以起到一定抗噪声作用)

需要说明的是, KNN算法<mark style="background: transparent; color: red">可以采用不同的距离度量公式</mark>, 就文本分类, **一般采用夹角余弦**(即和原点的夹角余弦值)作为距离公式。
$$\cos \theta = \frac{A\cdot B}{|A| |B|}$$
```python 
from numpy import linalg as la
def dist_cos(vector1, vector2):
	return dot(vector1, vector2)/(la.norm(vector1) * la.norm(vector2))
```

### 二、K-Means算法(K均值算法)
聚类要求： <mark style="background: transparent; color: red">类内距离小，类间距离大, 目标是未知</mark> 
1. 随机取K个点作为初始的聚类中心，代表各个聚类(一般选取数据点(注意是不同的数据点)，有时随机)
2. 计算求解各个样本相对于各个数据的距离。**并将点归类到与其距离最近的点**
3. 重新求解新类别的均值，再次调整中心位置(调整中心位置的过程:<mark style="background: transparent; color: red">采用子集的均值进行更新</mark>)
4. **重新求解，迭代至中心点近似不动为止**

简单示例参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/3. 推荐系统和需求搜寻算法(CF,PCA,SVD)#(1) 物品数据的 KMeans 聚类|KMeans 聚类]] 

> [!hint] 补充
> KMeans 适合处理球状分布的数据, 对于聚类结果是密集的,  且类间的区别比较明显时, KMeans 效果较好, 同时处理大数据集的效果较好。

缺点: 
1. 随机选取中心点, 容易导致迭代次数大或者陷入局部最优
2. 必须事先给出 k 的个数
3. 对于异常数据是敏感的 

- K-Means算法没有训练样本数据，因此为无监督学习
- 影响: 初始中心，输入数据K的选择，距离的度量
- 改进：二分K-Means算法--> <mark style="background: transparent; color: red">避免收敛于局部的问题</mark>   划分的标准是最大限度降低SSE值(最小化误差平方和)

对于二分 KMeans 算法, 主要是将所有点作为一个簇, 并将簇一分为二， <mark style="background: transparent; color: red">选择能够最大限度降低聚类代价的函数(误差平方和)的簇划分为两个簇</mark>,不断进行直到簇的数量等于给定数目 $k$ 为止。每一次选择**误差平方和最大的簇进行划分**。

以下是一个**二分KMeans的示例算法**: 
每一次采用 KMeans(n_cluster=2), 每一次采用 sse_error 作为划分最大的簇的标准。
```python 
import numpy as np  
from sklearn.cluster import KMeans  
from sklearn.datasets import load_iris  
import matplotlib.pyplot as plt  
  
def sse_error(cluster):  
    center = np.mean(cluster, axis=0)  
    sse = np.sum((cluster - center) ** 2)  
    return sse  
  
def bisecting_kmeans(X, k):  
    clusters = [X]  
    while len(clusters) < k:  
        largest_cluster = max(clusters, key=sse_error)  # sse error  
        clusters.remove(largest_cluster)  
        kmeans = KMeans(n_clusters=2).fit(largest_cluster)  
        clusters.append(largest_cluster[kmeans.labels_ == 0])  
        clusters.append(largest_cluster[kmeans.labels_ == 1])  
    return clusters  
  
# 加载数据集  
iris = load_iris()  
X = iris.data  
  
# 设定簇的数量  
k = 3  
clusters = bisecting_kmeans(X, k)  
  
# 可视化结果  
colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']  
for i, cluster in enumerate(clusters):  
    plt.scatter(cluster[:, 0], cluster[:, 1], c=colors[i % len(colors)])  
plt.title("Bisecting KMeans Clustering")  
plt.show()
```
示例结果如下:
![[attachments/Pasted image 20240919110305.png|200]]
每个部分仍然采用 kmeans 随机生成聚类中心点, 最终得到相应的聚类结果。二分KMeans算法能够避免收敛于局部的问题。

## 三、自适应 KMeans 聚类方案
### (1) 基本概念
#### 1. **肘部法则 (Elbow Method)**
- 计算不同聚类数 \( K \) 下的 **总聚类内误差平方和 (Within-Cluster Sum of Squares, WCSS)**。
此时, WCSS 定义为:
$$WCSS = \sum_{i = 1}^{k} \sum_{x\in  C_{i}} ||x -  \mu_{i}||^{2}  $$
其中:
- k 是聚类的数量。
- $C_i$​ 是第 i 个聚类。
- $\mu_i$​ 是第 i 个聚类的质心。
一般需要找到拐点处的 $K$, 即增加聚类数后收益变小的位置。这个可以利用二分或者其他寻优算法计算。

#### 2. **轮廓系数 (Silhouette Coefficient)**
- 衡量每个点与其所属簇的紧密程度以及与其他簇的分离程度，值范围为 \([-1, 1]\)：
  $$S = \frac{b - a}{\max(a, b)}$$
  - \( a \)：样本点与同簇其他点的平均距离。
  - \( b \)：样本点与最近簇点的平均距离。
- 选择具有最高平均轮廓系数的 \( K \)。

#### 3. **Gap Statistic**
- 比较实际数据的聚类性能和随机数据的聚类性能。
- 随机数据生成后计算其 WCSS，对比找到优化 \( K \)。

#### 优化点
1. **数据降维**：
   - 如果数据维度较高，可先用 **PCA** 降维，提高聚类效率和效果。

2. **分布检测**：
   - 检查数据分布，适配是否需要非球状数据的聚类方法（如 DBSCAN 或 Spectral Clustering）。

3. **Gap Statistic 实现**：
   - 如果需要更严谨的评估，可引入 Gap Statistic 作为补充评估标准，提升准确性。

以下代码实现了肘部法则和轮廓系数自动评估最佳聚类数, 假设数据是一个二维 `ndarray`：
```python
from sklearn.datasets import make_blobs
# 生成样本数据
data, _ = make_blobs(n_samples=1000, centers=5, cluster_std=1.0, random_state=42)
# 自动寻找最佳聚类数
optimal_k = find_optimal_clusters(data, max_k=10)
# 使用最佳聚类数进行 KMeans 聚类
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
labels = kmeans.fit_predict(data)
```

以下定义寻优的 find_optimal_clusters 函数:
```python fold
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

def find_optimal_clusters(data, max_k=10):
    """
    自动选择最佳聚类数
    :param data: 数据 (ndarray)
    :param max_k: 最大聚类数
    :return: 最佳聚类数 K
    """
    wcss = []
    silhouette_scores = []

    for k in range(2, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(data)

        # WCSS (Within-Cluster Sum of Squares)
        wcss.append(kmeans.inertia_)

        # 计算轮廓系数
        silhouette_scores.append(silhouette_score(data, labels))

    # 绘制评估曲线
    plt.figure(figsize=(12, 6))

    # 肘部法则
    plt.subplot(1, 2, 1)
    plt.plot(range(2, max_k + 1), wcss, marker='o')
    plt.title('Elbow Method')
    plt.xlabel('Number of Clusters')
    plt.ylabel('WCSS')

    # 轮廓系数
    plt.subplot(1, 2, 2)
    plt.plot(range(2, max_k + 1), silhouette_scores, marker='o')
    plt.title('Silhouette Scores')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Silhouette Score')

    plt.tight_layout()
    plt.show()

    # 返回最佳聚类数
    optimal_k = 2 + np.argmax(silhouette_scores)
    print(f"Optimal number of clusters: {optimal_k}")
    return optimal_k
```

### (2) 超大数据集的 MiniBatchKMeans 聚类
对于数据集量达到 10w 甚至 100w 的规模情况下, 可以采用 MiniBatch KMeans 聚类提高聚类效率

MiniBatch_KMeans 的使用非常简单, 如下:
```python
from sklearn.cluster import MiniBatchKMeans

mb_Kmeans  =  MiniBatchKMeans(n_clusters= min_n_clusters, batch_size=batch_size, random_state=42)  
mb_Kmeans.fit(data)  # train the model  
labels = mb_Kmeans.labels_
```

其中 <b><mark style="background: transparent; color: orange">labels_ 表示得到的每个数据的聚类结果, cluster_centers_ 属性为聚类中心的坐标</mark></b>，这两个是最重要的聚类属性。
需要说明的是, MiniBatch_KMeans 是按行进行聚类的, 即不同行分为不同的类;

```python
import matplotlib.pyplot as plt

plt.scatter(data[:, 0], data[:, 1], c=cluster_labels, cmap='viridis', s=10)
plt.scatter(mb_kmeans.cluster_centers_[:, 0], mb_kmeans.cluster_centers_[:, 1], c='red', marker='x')
plt.title('Clustering Results')
plt.show()
```

### (3) 通过寻优方法获取最优分类数
对于不同的 $k$ 值, 实际的聚类结果具有不同的 WCSS, 可以采用多种方法求解:
例如采用:
```python
from scipy.optimize import dual_annealing   # 模拟退火算法
```
方法 2 是基于网格搜索 + 随机采样的自适应寻优办法, 即对于不定函数大小的部分, 先网格搜索, 再搜索最优值, 如下所示:
```python fold title:网格搜索+细化搜索
def selfAdapt_KMeans(data,  
                     min_n_clusters,  
                     max_n_clusters,  
                     batch_size = 1000,  
                     grid_number = 20):  
    """  
    Use the  MiniBatchKMeans to cluster the user data    :return:  
    """    # use the gradient method for speed-up the optimizing process    if grid_number > (max_n_clusters - min_n_clusters + 1):  
        grid_number = max_n_clusters - min_n_clusters + 1  
    cluster_array = np.unique(np.linspace(min_n_clusters, max_n_clusters, grid_number, dtype=np.int32))  
    stp = np.int32(np.ceil((max_n_clusters - min_n_clusters)/(2. * grid_number)))  
    # initialize as -1  
    best_inertia = np.inf  
    inertia_arr  = np.zeros(max_n_clusters - min_n_clusters + 1, dtype=np.float32) - 1  
    for n_cluster in cluster_array:  
        mb_kmeans  =  MiniBatchKMeans(n_clusters= n_cluster, batch_size=batch_size, random_state=42)  
        mb_kmeans.fit(data)  # train the model  
        # calcuate the WCSS, the smaller the better        inertia = mb_kmeans.inertia_  
        inertia_arr[n_cluster - min_n_clusters] = inertia  
        if inertia < best_inertia:  
            best_inertia = inertia  
            best_cst = n_cluster  
        print("cluster test result :", n_cluster, inertia)  
  
    # search the index range  
    idx_start = max(min_n_clusters, best_cst - stp)  
    idx_stop = min(best_cst + stp, max_n_clusters)  
    for n_cluster in range(idx_start, idx_stop):  
        if inertia_arr[n_cluster - min_n_clusters] >= 0:  
            continue  
        # not calculated  
        mb_kmeans  =  MiniBatchKMeans(n_clusters= n_cluster, batch_size=batch_size, random_state=42)  
        mb_kmeans.fit(data)  # train the model  
        inertia = mb_kmeans.inertia_  
        inertia_arr[n_cluster - min_n_clusters] = inertia  
        if (inertia <  best_inertia):  
            best_inertia = inertia  
            best_cst = n_cluster  
        print(n_cluster, inertia)  
    return best_cst
```

还有一种办法是基于  Bayes 的黑盒寻优算法, 这也是一种**离散寻优方法**, 具体如下:
```python title:Bayes黑盒高效寻优算法
def Bayesian_Clustering(data,  
                        min_n_clusters,  
                        max_n_clusters,  
                        batch_size = 1000,  
                        n_calls = 20):  
    def intertia_function(k):  
        mb_kmeans = MiniBatchKMeans(n_clusters=k[0], batch_size=batch_size, random_state=42)  
        mb_kmeans.fit(data)  # train the model  
        # calcuate the WCSS, the smaller the better        inertia = mb_kmeans.inertia_  
        return inertia  
    # 贝叶斯优化  
    result = gp_minimize(intertia_function,  
                         [(min_n_clusters, max_n_clusters)],  
                          n_calls=n_calls,  
                         random_state=42)  
    print(f"Minimum at x = {result.x[0]:.2f}, value = {result.fun:.2f}")  
    return result.x[0]
```

