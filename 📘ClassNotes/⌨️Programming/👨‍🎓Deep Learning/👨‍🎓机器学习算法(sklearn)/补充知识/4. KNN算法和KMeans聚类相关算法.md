### ä¸€ã€KNNç®—æ³•
KNN(K-Nearest Neighbor Classification)ç®—æ³•, ä¹Ÿå«Kæœ€é‚»è¿‘åˆ†ç±»ç®—æ³•, æ˜¯ä¸€ç§å¸¸ç”¨çš„è¯†åˆ«ï¼Œåˆ†ç±»ç®—æ³•ã€‚
KNNç®—æ³•è¦æ±‚åˆ†æˆçš„ç±»åˆ«æ˜¯å·²çŸ¥çš„, ä¸”è®­ç»ƒæ ·æœ¬ç‚¹çš„ç±»åˆ«å‡å·²çŸ¥ã€‚

åŸºæœ¬è¿‡ç¨‹å¦‚ä¸‹:
1. **å¯¹äºæ¯ä¸ªç‚¹, æ±‚è§£æ•°æ®ç‚¹åˆ°å…¶ä½™å„ä¸ªè®­ç»ƒæ ·æœ¬ç‚¹çš„è·ç¦»d**ã€‚
2. **æ±‚è§£å¯¹åº”çš„æœ€é‚»è¿‘çš„Kä¸ªè®­ç»ƒæ ·æœ¬ç‚¹**ã€‚
3. åˆ¤æ–­æ ·æœ¬ç‚¹ä¸­æœ€å¤šçš„ç‚¹çš„ç±»åˆ«ï¼Œ å¹¶å°†è¯¥ç‚¹ä¸å…¶ä»–å½’ä¸ºä¸€ç±»ã€‚ 
ä¼˜ç‚¹æ˜¯ç®€å•æ–¹ä¾¿, ä½†æ˜¯å¯¹å™ªå£°æ•°æ®æ¯”è¾ƒæ•æ„Ÿ(kä¸åŒï¼Œå¯ä»¥èµ·åˆ°ä¸€å®šæŠ—å™ªå£°ä½œç”¨)

éœ€è¦è¯´æ˜çš„æ˜¯, KNNç®—æ³•<mark style="background: transparent; color: red">å¯ä»¥é‡‡ç”¨ä¸åŒçš„è·ç¦»åº¦é‡å…¬å¼</mark>, å°±æ–‡æœ¬åˆ†ç±», **ä¸€èˆ¬é‡‡ç”¨å¤¹è§’ä½™å¼¦**(å³å’ŒåŸç‚¹çš„å¤¹è§’ä½™å¼¦å€¼)ä½œä¸ºè·ç¦»å…¬å¼ã€‚
$$\cos \theta = \frac{A\cdot B}{|A| |B|}$$
```python 
from numpy import linalg as la
def dist_cos(vector1, vector2):
	return dot(vector1, vector2)/(la.norm(vector1) * la.norm(vector2))
```

### äºŒã€K-Meansç®—æ³•(Kå‡å€¼ç®—æ³•)
èšç±»è¦æ±‚ï¼š <mark style="background: transparent; color: red">ç±»å†…è·ç¦»å°ï¼Œç±»é—´è·ç¦»å¤§, ç›®æ ‡æ˜¯æœªçŸ¥</mark> 
1. éšæœºå–Kä¸ªç‚¹ä½œä¸ºåˆå§‹çš„èšç±»ä¸­å¿ƒï¼Œä»£è¡¨å„ä¸ªèšç±»(ä¸€èˆ¬é€‰å–æ•°æ®ç‚¹(æ³¨æ„æ˜¯ä¸åŒçš„æ•°æ®ç‚¹)ï¼Œæœ‰æ—¶éšæœº)
2. è®¡ç®—æ±‚è§£å„ä¸ªæ ·æœ¬ç›¸å¯¹äºå„ä¸ªæ•°æ®çš„è·ç¦»ã€‚**å¹¶å°†ç‚¹å½’ç±»åˆ°ä¸å…¶è·ç¦»æœ€è¿‘çš„ç‚¹**
3. é‡æ–°æ±‚è§£æ–°ç±»åˆ«çš„å‡å€¼ï¼Œå†æ¬¡è°ƒæ•´ä¸­å¿ƒä½ç½®(è°ƒæ•´ä¸­å¿ƒä½ç½®çš„è¿‡ç¨‹:<mark style="background: transparent; color: red">é‡‡ç”¨å­é›†çš„å‡å€¼è¿›è¡Œæ›´æ–°</mark>)
4. **é‡æ–°æ±‚è§£ï¼Œè¿­ä»£è‡³ä¸­å¿ƒç‚¹è¿‘ä¼¼ä¸åŠ¨ä¸ºæ­¢**

ç®€å•ç¤ºä¾‹å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/3. æ¨èç³»ç»Ÿå’Œéœ€æ±‚æœå¯»ç®—æ³•(CF,PCA,SVD)#(1) ç‰©å“æ•°æ®çš„ KMeans èšç±»|KMeans èšç±»]] 

> [!hint] è¡¥å……
> KMeans é€‚åˆå¤„ç†çƒçŠ¶åˆ†å¸ƒçš„æ•°æ®, å¯¹äºèšç±»ç»“æœæ˜¯å¯†é›†çš„,  ä¸”ç±»é—´çš„åŒºåˆ«æ¯”è¾ƒæ˜æ˜¾æ—¶, KMeans æ•ˆæœè¾ƒå¥½, åŒæ—¶å¤„ç†å¤§æ•°æ®é›†çš„æ•ˆæœè¾ƒå¥½ã€‚

ç¼ºç‚¹: 
1. éšæœºé€‰å–ä¸­å¿ƒç‚¹, å®¹æ˜“å¯¼è‡´è¿­ä»£æ¬¡æ•°å¤§æˆ–è€…é™·å…¥å±€éƒ¨æœ€ä¼˜
2. å¿…é¡»äº‹å…ˆç»™å‡º k çš„ä¸ªæ•°
3. å¯¹äºå¼‚å¸¸æ•°æ®æ˜¯æ•æ„Ÿçš„ 

- K-Meansç®—æ³•æ²¡æœ‰è®­ç»ƒæ ·æœ¬æ•°æ®ï¼Œå› æ­¤ä¸ºæ— ç›‘ç£å­¦ä¹ 
- å½±å“: åˆå§‹ä¸­å¿ƒï¼Œè¾“å…¥æ•°æ®Kçš„é€‰æ‹©ï¼Œè·ç¦»çš„åº¦é‡
- æ”¹è¿›ï¼šäºŒåˆ†K-Meansç®—æ³•--> <mark style="background: transparent; color: red">é¿å…æ”¶æ•›äºå±€éƒ¨çš„é—®é¢˜</mark>   åˆ’åˆ†çš„æ ‡å‡†æ˜¯æœ€å¤§é™åº¦é™ä½SSEå€¼(æœ€å°åŒ–è¯¯å·®å¹³æ–¹å’Œ)

å¯¹äºäºŒåˆ† KMeans ç®—æ³•, ä¸»è¦æ˜¯å°†æ‰€æœ‰ç‚¹ä½œä¸ºä¸€ä¸ªç°‡, å¹¶å°†ç°‡ä¸€åˆ†ä¸ºäºŒï¼Œ <mark style="background: transparent; color: red">é€‰æ‹©èƒ½å¤Ÿæœ€å¤§é™åº¦é™ä½èšç±»ä»£ä»·çš„å‡½æ•°(è¯¯å·®å¹³æ–¹å’Œ)çš„ç°‡åˆ’åˆ†ä¸ºä¸¤ä¸ªç°‡</mark>,ä¸æ–­è¿›è¡Œç›´åˆ°ç°‡çš„æ•°é‡ç­‰äºç»™å®šæ•°ç›® $k$ ä¸ºæ­¢ã€‚æ¯ä¸€æ¬¡é€‰æ‹©**è¯¯å·®å¹³æ–¹å’Œæœ€å¤§çš„ç°‡è¿›è¡Œåˆ’åˆ†**ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ª**äºŒåˆ†KMeansçš„ç¤ºä¾‹ç®—æ³•**: 
æ¯ä¸€æ¬¡é‡‡ç”¨ KMeans(n_cluster=2), æ¯ä¸€æ¬¡é‡‡ç”¨ sse_error ä½œä¸ºåˆ’åˆ†æœ€å¤§çš„ç°‡çš„æ ‡å‡†ã€‚
```python 
import numpy as np  
from sklearn.cluster import KMeans  
from sklearn.datasets import load_iris  
import matplotlib.pyplot as plt  
  
def sse_error(cluster):  
    center = np.mean(cluster, axis=0)  
    sse = np.sum((cluster - center) ** 2)  
    return sse  
  
def bisecting_kmeans(X, k):  
    clusters = [X]  
    while len(clusters) < k:  
        largest_cluster = max(clusters, key=sse_error)  # sse error  
        clusters.remove(largest_cluster)  
        kmeans = KMeans(n_clusters=2).fit(largest_cluster)  
        clusters.append(largest_cluster[kmeans.labels_ == 0])  
        clusters.append(largest_cluster[kmeans.labels_ == 1])  
    return clusters  
  
# åŠ è½½æ•°æ®é›†  
iris = load_iris()  
X = iris.data  
  
# è®¾å®šç°‡çš„æ•°é‡  
k = 3  
clusters = bisecting_kmeans(X, k)  
  
# å¯è§†åŒ–ç»“æœ  
colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']  
for i, cluster in enumerate(clusters):  
    plt.scatter(cluster[:, 0], cluster[:, 1], c=colors[i % len(colors)])  
plt.title("Bisecting KMeans Clustering")  
plt.show()
```
ç¤ºä¾‹ç»“æœå¦‚ä¸‹:
![[attachments/Pasted image 20240919110305.png|200]]
æ¯ä¸ªéƒ¨åˆ†ä»ç„¶é‡‡ç”¨ kmeans éšæœºç”Ÿæˆèšç±»ä¸­å¿ƒç‚¹, æœ€ç»ˆå¾—åˆ°ç›¸åº”çš„èšç±»ç»“æœã€‚äºŒåˆ†KMeansç®—æ³•èƒ½å¤Ÿé¿å…æ”¶æ•›äºå±€éƒ¨çš„é—®é¢˜ã€‚

## ä¸‰ã€è‡ªé€‚åº” KMeans èšç±»æ–¹æ¡ˆ
### (1) åŸºæœ¬æ¦‚å¿µ
#### 1. **è‚˜éƒ¨æ³•åˆ™ (Elbow Method)**
- è®¡ç®—ä¸åŒèšç±»æ•° \( K \) ä¸‹çš„ **æ€»èšç±»å†…è¯¯å·®å¹³æ–¹å’Œ (Within-Cluster Sum of Squares, WCSS)**ã€‚
æ­¤æ—¶, WCSS å®šä¹‰ä¸º:
$$WCSS = \sum_{i = 1}^{k} \sum_{x\in  C_{i}} ||x -  \mu_{i}||^{2}  $$
å…¶ä¸­:
- k æ˜¯èšç±»çš„æ•°é‡ã€‚
- $C_i$â€‹ æ˜¯ç¬¬ i ä¸ªèšç±»ã€‚
- $\mu_i$â€‹ æ˜¯ç¬¬ i ä¸ªèšç±»çš„è´¨å¿ƒã€‚
ä¸€èˆ¬éœ€è¦æ‰¾åˆ°æ‹ç‚¹å¤„çš„ $K$, å³å¢åŠ èšç±»æ•°åæ”¶ç›Šå˜å°çš„ä½ç½®ã€‚è¿™ä¸ªå¯ä»¥åˆ©ç”¨äºŒåˆ†æˆ–è€…å…¶ä»–å¯»ä¼˜ç®—æ³•è®¡ç®—ã€‚

#### 2. **è½®å»“ç³»æ•° (Silhouette Coefficient)**
- è¡¡é‡æ¯ä¸ªç‚¹ä¸å…¶æ‰€å±ç°‡çš„ç´§å¯†ç¨‹åº¦ä»¥åŠä¸å…¶ä»–ç°‡çš„åˆ†ç¦»ç¨‹åº¦ï¼Œå€¼èŒƒå›´ä¸º \([-1, 1]\)ï¼š
  $$S = \frac{b - a}{\max(a, b)}$$
  - \( a \)ï¼šæ ·æœ¬ç‚¹ä¸åŒç°‡å…¶ä»–ç‚¹çš„å¹³å‡è·ç¦»ã€‚
  - \( b \)ï¼šæ ·æœ¬ç‚¹ä¸æœ€è¿‘ç°‡ç‚¹çš„å¹³å‡è·ç¦»ã€‚
- é€‰æ‹©å…·æœ‰æœ€é«˜å¹³å‡è½®å»“ç³»æ•°çš„ \( K \)ã€‚

#### 3. **Gap Statistic**
- æ¯”è¾ƒå®é™…æ•°æ®çš„èšç±»æ€§èƒ½å’Œéšæœºæ•°æ®çš„èšç±»æ€§èƒ½ã€‚
- éšæœºæ•°æ®ç”Ÿæˆåè®¡ç®—å…¶ WCSSï¼Œå¯¹æ¯”æ‰¾åˆ°ä¼˜åŒ– \( K \)ã€‚

#### ä¼˜åŒ–ç‚¹
1. **æ•°æ®é™ç»´**ï¼š
   - å¦‚æœæ•°æ®ç»´åº¦è¾ƒé«˜ï¼Œå¯å…ˆç”¨ **PCA** é™ç»´ï¼Œæé«˜èšç±»æ•ˆç‡å’Œæ•ˆæœã€‚

2. **åˆ†å¸ƒæ£€æµ‹**ï¼š
   - æ£€æŸ¥æ•°æ®åˆ†å¸ƒï¼Œé€‚é…æ˜¯å¦éœ€è¦éçƒçŠ¶æ•°æ®çš„èšç±»æ–¹æ³•ï¼ˆå¦‚ DBSCAN æˆ– Spectral Clusteringï¼‰ã€‚

3. **Gap Statistic å®ç°**ï¼š
   - å¦‚æœéœ€è¦æ›´ä¸¥è°¨çš„è¯„ä¼°ï¼Œå¯å¼•å…¥ Gap Statistic ä½œä¸ºè¡¥å……è¯„ä¼°æ ‡å‡†ï¼Œæå‡å‡†ç¡®æ€§ã€‚

ä»¥ä¸‹ä»£ç å®ç°äº†è‚˜éƒ¨æ³•åˆ™å’Œè½®å»“ç³»æ•°è‡ªåŠ¨è¯„ä¼°æœ€ä½³èšç±»æ•°, å‡è®¾æ•°æ®æ˜¯ä¸€ä¸ªäºŒç»´ `ndarray`ï¼š
```python
from sklearn.datasets import make_blobs
# ç”Ÿæˆæ ·æœ¬æ•°æ®
data, _ = make_blobs(n_samples=1000, centers=5, cluster_std=1.0, random_state=42)
# è‡ªåŠ¨å¯»æ‰¾æœ€ä½³èšç±»æ•°
optimal_k = find_optimal_clusters(data, max_k=10)
# ä½¿ç”¨æœ€ä½³èšç±»æ•°è¿›è¡Œ KMeans èšç±»
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
labels = kmeans.fit_predict(data)
```

ä»¥ä¸‹å®šä¹‰å¯»ä¼˜çš„ find_optimal_clusters å‡½æ•°:
```python fold
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

def find_optimal_clusters(data, max_k=10):
    """
    è‡ªåŠ¨é€‰æ‹©æœ€ä½³èšç±»æ•°
    :param data: æ•°æ® (ndarray)
    :param max_k: æœ€å¤§èšç±»æ•°
    :return: æœ€ä½³èšç±»æ•° K
    """
    wcss = []
    silhouette_scores = []

    for k in range(2, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(data)

        # WCSS (Within-Cluster Sum of Squares)
        wcss.append(kmeans.inertia_)

        # è®¡ç®—è½®å»“ç³»æ•°
        silhouette_scores.append(silhouette_score(data, labels))

    # ç»˜åˆ¶è¯„ä¼°æ›²çº¿
    plt.figure(figsize=(12, 6))

    # è‚˜éƒ¨æ³•åˆ™
    plt.subplot(1, 2, 1)
    plt.plot(range(2, max_k + 1), wcss, marker='o')
    plt.title('Elbow Method')
    plt.xlabel('Number of Clusters')
    plt.ylabel('WCSS')

    # è½®å»“ç³»æ•°
    plt.subplot(1, 2, 2)
    plt.plot(range(2, max_k + 1), silhouette_scores, marker='o')
    plt.title('Silhouette Scores')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Silhouette Score')

    plt.tight_layout()
    plt.show()

    # è¿”å›æœ€ä½³èšç±»æ•°
    optimal_k = 2 + np.argmax(silhouette_scores)
    print(f"Optimal number of clusters: {optimal_k}")
    return optimal_k
```

### (2) è¶…å¤§æ•°æ®é›†çš„ MiniBatchKMeans èšç±»
å¯¹äºæ•°æ®é›†é‡è¾¾åˆ° 10w ç”šè‡³ 100w çš„è§„æ¨¡æƒ…å†µä¸‹, å¯ä»¥é‡‡ç”¨ MiniBatch KMeans èšç±»æé«˜èšç±»æ•ˆç‡

MiniBatch_KMeans çš„ä½¿ç”¨éå¸¸ç®€å•, å¦‚ä¸‹:
```python
from sklearn.cluster import MiniBatchKMeans

mb_Kmeans  =  MiniBatchKMeans(n_clusters= min_n_clusters, batch_size=batch_size, random_state=42)  
mb_Kmeans.fit(data)  # train the model  
labels = mb_Kmeans.labels_
```

å…¶ä¸­ <b><mark style="background: transparent; color: orange">labels_ è¡¨ç¤ºå¾—åˆ°çš„æ¯ä¸ªæ•°æ®çš„èšç±»ç»“æœ, cluster_centers_ å±æ€§ä¸ºèšç±»ä¸­å¿ƒçš„åæ ‡</mark></b>ï¼Œè¿™ä¸¤ä¸ªæ˜¯æœ€é‡è¦çš„èšç±»å±æ€§ã€‚
éœ€è¦è¯´æ˜çš„æ˜¯, MiniBatch_KMeans æ˜¯æŒ‰è¡Œè¿›è¡Œèšç±»çš„, å³ä¸åŒè¡Œåˆ†ä¸ºä¸åŒçš„ç±»;

```python
import matplotlib.pyplot as plt

plt.scatter(data[:, 0], data[:, 1], c=cluster_labels, cmap='viridis', s=10)
plt.scatter(mb_kmeans.cluster_centers_[:, 0], mb_kmeans.cluster_centers_[:, 1], c='red', marker='x')
plt.title('Clustering Results')
plt.show()
```

### (3) é€šè¿‡å¯»ä¼˜æ–¹æ³•è·å–æœ€ä¼˜åˆ†ç±»æ•°
å¯¹äºä¸åŒçš„ $k$ å€¼, å®é™…çš„èšç±»ç»“æœå…·æœ‰ä¸åŒçš„ WCSS, å¯ä»¥é‡‡ç”¨å¤šç§æ–¹æ³•æ±‚è§£:
ä¾‹å¦‚é‡‡ç”¨:
```python
from scipy.optimize import dual_annealing   # æ¨¡æ‹Ÿé€€ç«ç®—æ³•
```
æ–¹æ³• 2 æ˜¯åŸºäºç½‘æ ¼æœç´¢ + éšæœºé‡‡æ ·çš„è‡ªé€‚åº”å¯»ä¼˜åŠæ³•, å³å¯¹äºä¸å®šå‡½æ•°å¤§å°çš„éƒ¨åˆ†, å…ˆç½‘æ ¼æœç´¢, å†æœç´¢æœ€ä¼˜å€¼, å¦‚ä¸‹æ‰€ç¤º:
```python fold title:ç½‘æ ¼æœç´¢+ç»†åŒ–æœç´¢
def selfAdapt_KMeans(data,  
                     min_n_clusters,  
                     max_n_clusters,  
                     batch_size = 1000,  
                     grid_number = 20):  
    """  
    Use the  MiniBatchKMeans to cluster the user data    :return:  
    """    # use the gradient method for speed-up the optimizing process    if grid_number > (max_n_clusters - min_n_clusters + 1):  
        grid_number = max_n_clusters - min_n_clusters + 1  
    cluster_array = np.unique(np.linspace(min_n_clusters, max_n_clusters, grid_number, dtype=np.int32))  
    stp = np.int32(np.ceil((max_n_clusters - min_n_clusters)/(2. * grid_number)))  
    # initialize as -1  
    best_inertia = np.inf  
    inertia_arr  = np.zeros(max_n_clusters - min_n_clusters + 1, dtype=np.float32) - 1  
    for n_cluster in cluster_array:  
        mb_kmeans  =  MiniBatchKMeans(n_clusters= n_cluster, batch_size=batch_size, random_state=42)  
        mb_kmeans.fit(data)  # train the model  
        # calcuate the WCSS, the smaller the better        inertia = mb_kmeans.inertia_  
        inertia_arr[n_cluster - min_n_clusters] = inertia  
        if inertia < best_inertia:  
            best_inertia = inertia  
            best_cst = n_cluster  
        print("cluster test result :", n_cluster, inertia)  
  
    # search the index range  
    idx_start = max(min_n_clusters, best_cst - stp)  
    idx_stop = min(best_cst + stp, max_n_clusters)  
    for n_cluster in range(idx_start, idx_stop):  
        if inertia_arr[n_cluster - min_n_clusters] >= 0:  
            continue  
        # not calculated  
        mb_kmeans  =  MiniBatchKMeans(n_clusters= n_cluster, batch_size=batch_size, random_state=42)  
        mb_kmeans.fit(data)  # train the model  
        inertia = mb_kmeans.inertia_  
        inertia_arr[n_cluster - min_n_clusters] = inertia  
        if (inertia <  best_inertia):  
            best_inertia = inertia  
            best_cst = n_cluster  
        print(n_cluster, inertia)  
    return best_cst
```

è¿˜æœ‰ä¸€ç§åŠæ³•æ˜¯åŸºäº  Bayes çš„é»‘ç›’å¯»ä¼˜ç®—æ³•, è¿™ä¹Ÿæ˜¯ä¸€ç§**ç¦»æ•£å¯»ä¼˜æ–¹æ³•**, å…·ä½“å¦‚ä¸‹:
```python title:Bayesé»‘ç›’é«˜æ•ˆå¯»ä¼˜ç®—æ³•
def Bayesian_Clustering(data,  
                        min_n_clusters,  
                        max_n_clusters,  
                        batch_size = 1000,  
                        n_calls = 20):  
    def intertia_function(k):  
        mb_kmeans = MiniBatchKMeans(n_clusters=k[0], batch_size=batch_size, random_state=42)  
        mb_kmeans.fit(data)  # train the model  
        # calcuate the WCSS, the smaller the better        inertia = mb_kmeans.inertia_  
        return inertia  
    # è´å¶æ–¯ä¼˜åŒ–  
    result = gp_minimize(intertia_function,  
                         [(min_n_clusters, max_n_clusters)],  
                          n_calls=n_calls,  
                         random_state=42)  
    print(f"Minimum at x = {result.x[0]:.2f}, value = {result.fun:.2f}")  
    return result.x[0]
```

