## 一、基本介绍
支持向量机分为 
- 线性支持向量机 
- 非线性支持向量机 - 核方法 

参考 [支持向量机通俗导论](https://blog.csdn.net/v_JULY_v/article/details/7624837) 和 [超平面方程的由来详解](https://blog.csdn.net/dengheCSDN/article/details/77313758) 
预备知识:  **VC 维,  经验风险和结构风险**参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/🚧结构可靠性设计基础/第八章 响应面方法和支持向量机|响应面方法和支持向量机]] 部分; 
### (1) Logistic 函数
支持向量机(Suppported Vector Machine, SVM), 是一种分类算法,其一般目标是在空间中找到一个超平面(最大边缘超平面, Maximal Margin Hyperplane), 并表示方程为
![[attachments/Pasted image 20240327152229.png|300]]
$$w^{T} x + b = 0$$
其中$w = (w_1;w_2;....;w_m)$为垂直于超平面的法向量,$b$为位移量, $w$决定决策边界方向，$b$决定决策边界与原点之间的距离。决策边界由参数$\omega,b$确定

我们首先介绍Logistic函数, 即将数据从$(-\infty, +\infty)$ 映射到(0,1)的函数，其形式如下:
$$\boxed {h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1 + e^{-\theta^{T}x}}}$$
显然,当$\theta^{T}x > 0$时,$h_{\theta}(x) >0.5$, 获得的图像如下图:
![[attachments/Pasted image 20240327152635.png|400]]
其中上式中的$\theta^{T} x$即对应一个对输入参数(n维)的线性变换:
$$\theta^{T} x = \theta_{0}x_{0} + \theta_{1} x_{1}  + \theta_{2}x_{2} + \dots  + \theta_{n}x_{n}\qquad  (x_{0} = 1)$$
另外其中$x_1,\dots x_n$代表了多个不同训练样本值（x_0=1）也可以使用不同的基函数进行代替. 
对应的分类目的, 就是求解对应的超参数$\theta_{1}, \theta_{2}, \dots$,使得训练数据中对应的 y=1 部分, $\theta^{T} x >>0$, 而 y=0 部分, $\theta^{T} x << 0$，  

1. 使用Logistic函数或者Sigmoid函数，将数据映射到（0,1）上
2. Logistic回归的目的是学习出一个（0,1）分类模型
3. 通过Logistic函数映射，将自变量映射到（0,1）上，映射后的值被认为是属于 y = 1 的概率。
4. Logistic回归模型目标是学习得到$\theta$使得正例的特征$\theta^T x$远大于0,负例的特征远小于0，则得到的大于0和小于0的部分可以进行分类(注意在SVM中，是使用-1和1来作为标签的)

判别公式为:
$$\begin{matrix} 
P(y=1,x;\theta) = h_\theta(x) \\
P(y=0,x;\theta) = 1 - h_\theta(x)
\end{matrix}$$

### (2)最大边缘超平面
Maximal Margin Hyperplane
![[attachments/Pasted image 20240327154353.png|300]]
由于将训练样本点分开的超平面很多，所以往往必须有最大边缘超平面作为决策边界。我们考虑和每个超平面平行的一对仍然可以将样本点分开的超平面,而其中两平面间距称为边缘(margin)

最大边缘超平面即距离最长的一组超平面(B1), 而我们往往以其为分类依据, 以获得更好的泛化误差。这种方法所对应的即为**线性支持向量机方法**。（同时适用于线性可分和线性不可分）

另外，数据可能需要非线性最优超平面进行分类。对应方法为**非线性支持向量机方法** -> 此时往往使用**核方法**获得不同的非线性支持向量机特性。
![[attachments/Pasted image 20240327154556.png|300]]

> [!NOTE] 核方法原理
> 核方法实际上是基于Mercer核展开定义通过内积函数定义的非线性变换将输入空间映射到高位特征空间， 并在高维空间中寻找相应的关系。

![[Excalidraw/2.SVM支持向量机 2024-03-30 10.49.21]]
## 二、线性支持向量机
### (1) 问题引入
对于线性超平面, **分类决策边界可以写成**(其中w, b未知):
$$w^{T}  x + b  =0$$
决策边界由参数w,b共同决定。首先假设决策编辑将训练样本($x_1, x_2,\dots$)正确分类， 则显然有
$$\begin{cases}
w^{T} x_{i} + b \geq  \varepsilon \qquad y_{i} = 1 \\
w^{T} x_{i} + b \leq  \varepsilon \qquad y_{i} = -1
\end{cases}$$
此时，**显然按照比例调整(扩大)决策边界的参数w,b(可以按比例调整)可得到**: 
$$\begin{cases}
w^{T} x_{i} + b \geq  1 \qquad y_{i} = 1 \\
w^{T} x_{i} + b \leq  -1 \qquad y_{i} = -1
\end{cases} \tag{8.1.1}$$
为了计算两个超平面之间的边缘$\gamma$, 则在两个**边缘超平面上各取一个样本点**，使得:
$$w^{T} x_{1} + b = 1 \qquad  w^{T}x_{2} +b =-1$$
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/第八章 向量代数和空间解析几何|第八章 向量代数和空间解析几何]], 此时容易说明**超平面距离显然计算为**：
$$\boxed{\gamma =\frac{2}{||w||}\qquad  ||w|| = w_{1}^{2} + w_{2}^{2} + \dots + w_{n}^{2}}$$
只需寻找对应的参数w,b使得 $\gamma$ 最大即可, 可以转换为下面的二次目标函数优化问题:
$$\text{argmax} \frac{2}{||w||} \leftrightarrows \text{argmin}\frac{1}{2}||w||^{2}  \tag{8.1.2}$$
### (2) 优化求解目标推导和分类器函数
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/👆重要定理/拉格朗日乘数法求条件极值|拉格朗日乘数法求条件极值]], 使用拉格朗日乘子$\alpha_{i} \geq  0$, 条件是**点均在边界上**，即对点$(x_i,y_i)$, 有:
$$\boxed{\forall  i , \qquad  s.t.  y_{i}(w^{T} x_{i} + b) \geq  1}\tag{8. 2.1}$$
其中 $y_i = 1$ 或 $y_i = -1$ 为**预测的分类值**(8.1.1) 
显然 $y_{i}(w^{T} x_{i} + b) -1 \geq 0$, 获得**拉格朗日函数($\alpha_i \geq 0$)**:
$$\boxed{\Large  L(w, b, \alpha) = \frac{1}{2}w^T  w- \sum^{n}_{i=1} \alpha_{i}(y_{i}(w^{T} x_{i} + b) -1 )} \tag{8.2.2}$$
其中 $\alpha_i$ 为<mark style="background: transparent; color: red">拉格朗日乘子</mark>, 我们取$L(w,b, \alpha)$==对$w$和$b$的偏导为0==，使**上述最小**,获得:
$$||w|| =\sum^{n}_{i=1}  \alpha_{i} y_{i}x_{i}\qquad  \sum^{n}_{i=1} a_{i}y_{i}= 0 \tag{8.2.3}$$
此时代入式$(8.2.2)$ 使其最小，得到优化问题为 : **使下面的式子最小**(由于||w||二次项有负号导致的)
$$L(w, b, \alpha) = -\frac{1}{2}\sum^{n}_{i=1} \sum^{n}_{j=1} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} + \sum^{n}_{i=1} \alpha_{i}$$
变形得到最终优化问题
$$\Large\boxed{ L (w, \alpha, b)  = \min_{\alpha} \frac{1}{2}\sum^{n}_{i=1} \sum^{n}_{j=1}  \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} -\sum^{n}_{i=1} \alpha_{i} } \tag{8.2.4}$$
**只需求解使上述参数最小的$\alpha$即可**, 其中, 约束条件为:
$$\boxed{\Large \alpha  \geq  0,\qquad  \sum_{i = 1}^{n}\alpha_{i} y_{i}  = 0} \tag{8.2.5}$$
而对应的<b><mark style="background: transparent; color: blue">分类器函数为 </mark></b> 
$$\Large \boxed{f(x) =  w^{T}x + b  =  \left(\sum_{i = 1}^{n}\alpha_{i} y_{i}   \right) ^{T} x_{i} + b = \sum_{i = 1}^{n}   \alpha_{i}y_{i} <x_{i}, x> + b }$$
其中我们一般**以核函数代替向量内积** 

### (3) 对偶优化与 KKT 条件
主要参考: [支持向量机](https://zhuanlan.zhihu.com/p/76609851) 以及 [文章](https://zhuanlan.zhihu.com/p/77750026)
#### 1. 对偶优化
问题:求解**一般约束条件**:
$$g_{i}(x) \leq 0, h_{j}(x) = 0$$
下的$f(x)$极小值
我们参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/👆重要定理/拉格朗日乘数法求条件极值|拉格朗日乘数法求条件极值]], 可使用下式:
$$L(x, \alpha, \beta) =  f(x) + \sum^{m}_{i=1} \alpha_{i}g_{i}(x) + \sum^{k}_{j=1} \beta_{j} h_{j}(x)$$
我们**只需要取不同的x值让这个式子最小即可** 
$$\min f(x) \rightarrow   \min_{x}  L (x, \alpha, \beta)$$
显然，**这个式子对于任何的$\alpha, \beta$ 都需要取到最小值, 我们不放设$\alpha， \beta$让右侧的式子取到最大值**，则问题转化为:
$$\boxed{\min f(x) \rightarrow \min_{x} \max_{\alpha \geq 0, \beta} L (x, \alpha, \beta)\overset{转化为对偶问题}{\longrightarrow}  \max_{\alpha \geq 0, \beta}\min_{x} L (x, \alpha, \beta)} \tag{8.3.1}$$
上式即为==对偶优化问题的转换式==。

#### 2. KKT 条件 
KKT条件(**Karush-Kuhn-Tucker 条件**)
对于上式中, 求解$\max_{\alpha , \beta\geq 0}\min_{x} L (x, \alpha, \beta)$, 其中(显然前面一项是取小的, 而后面两项是使得取更小值的,但是由于$\alpha$存在使得最终为0)可以化为无条件极值;
$$\max_{\alpha, \beta \geq  0}  L(x, \alpha, \beta) =  f(x) + \sum^{m}_{i=1} \alpha_{i}g_{i}(x) + \sum^{k}_{j=1} \beta_{j} h_{j}(x)\qquad (g_{i} \leq   0, h_{j} = 0)$$
为了保证==对偶优化问题和初始的问题具有相同的解==， 参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/推导部分/对偶优化一致解条件推导过程.pdf|对偶优化一致解条件推导过程.pdf]] 和 [KKT条件详解](https://zhuanlan.zhihu.com/p/26514613) 得到一致解条件(KKT条件)，KKT 条件给出了判断解 $x^*$ 是否为最优解的<b><mark style="background: transparent; color: blue">必要条件</mark></b>: 
$$\Large \begin{cases}
\left.\frac{\partial L(x ,\alpha^{*}, \beta^{*})}{\partial x}  \right|= 0 ,  \\
\alpha_{i}^{*} g_{i}(x^{*}) = 0  \\
h_{j} (x^{*}) = 0,  g_{i} (x^{*}) \leq  0 \\
\alpha_{i} (x^{*})  \geq  0
\end{cases}\tag{8.3.2}$$
最优解必然满足上述所有条件， 实际上我们也是考虑此时对应的 x 值。

显然, 上面的条件将 $\max_{\alpha， \beta\geq 0}  L(x, \alpha, \beta)$ 转换为仅含 $x$ 的部分, 我们参考需要得到的 

### (4) 带松弛变量的SVM方法
首先, 我们对于线性可分的样本 
$$D = \left\{(x_{1}, y_{1}), \dots , (x_{l}, y_{l}) \right\},  \quad  y_{i}  \in \{-1 , +1\}$$
假设线性超平面为:
$$\Large\boxed{w \cdot  X + b  = 0}\tag{8.4.1}$$
分类目标是最大化超平面距离 $\gamma$, 即最小化$\frac{1}{2} ||w||^{2}$, 同时, <b><mark style="background: transparent; color: blue">考虑到一些样本不能够被正确分类, 引入松弛变量</mark></b>$\xi_{i}$ 从而可以**有效地解决某些情况下的数据偏斜问题**, 参考 [[#(2) 优化求解目标推导和分类器函数|式8.2.1]], 有:
$$\boxed{\Large  y_{i} (w \cdot  \varphi(x_{i})  +  b) \geq   1 -  \xi_{i} } \tag{8.4.2}$$
此时最小化问题中, 增加一个惩罚项 $C \sum_{i = 1}^{l} \xi_i$, 有目标:
$$\Large \boxed{\text{arg}\min_{\alpha}  \frac{1}{2}||w||^{2}  + C \sum^{l}_{i=1} \xi_{i}\tag{8.4.3}}$$ $$\Large \boxed{s.t. \quad  g(x) = -( y(w \cdot  \varphi (x_{i} )+ b ) - 1  + \xi_{i}) \leq  0 \qquad  \xi_{i} \geq 0\tag{8.4.4}}$$
此时， 引入拉格朗日函数, 得到下面的**拉格朗日函数**(寻求L最小值), 其中**最后一项是由于$\xi_{i} \geq 0$构造的**$-\gamma_{i}\xi_{i} \leq  0$: (**相对于 8.2.2**)
$$L(w, b, \xi , \alpha, \gamma) = \frac{1}{2}||w||^{2} + C \sum^{l}_{i=1} \xi_{i} - \sum^{l}_{i=1} \alpha_{i} \left\{y_{i} [w \cdot  \varphi (x_{i}) + b \right\}  - 1 + \xi_{i}) - \sum^{l}_{i=1} \gamma_{i} \xi_{i}$$
代入[[#(4) 对偶优化|KKT条件]], 构造并经过[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/推导部分/支持向量机SVM分类与回归算法推导过程.pdf|支持向量机SVM分类与回归算法推导过程]], 将上式化为**类似(8.2.4)的目标函数如下**:
$$\Large\boxed{\min_{\alpha \geq  0}  \frac{1}{2}\sum^{l}_{i=1} \sum^{l}_{j=1} \alpha_{i}\alpha_{j} y_{i} y_{j}  \mathcal{K}(x_{i}, x_{j}) -  \sum^{l}_{i=1} \alpha_{i}  \quad \left(\sum^{n}_{i=1} \alpha_{i} y_{i} = 0,  \alpha_{i}\in  [0, C]\right)}\tag{8.4.5}$$
这个仍然是**仅仅含有 $\alpha$ 项的最优化公式形式**, $\mathcal{K}$表示核函数(实际上用来代替上述中的 $\varphi(x_i)$)。而支持向量的==**分类预测函数**==可以计算为:
$$\Large \boxed{g_{SVM }(x) = \text{sgn}\left(\sum^{l}_{i=1} \alpha_{i} y_{i}\mathcal{K}(x_{i}, x_{j}) + b\right)}\tag{8.4.6}$$

> [!NOTE] 说明
> 其中对于每个变量都对应一个$\xi_{i}$, 描述不满足约束$\geq 1$的程度。而$C$表示**正则化常数**。取$C = \infty$时,则约束变为所有的点都必须明确分类 $y_{i}(w \cdot \varphi(x_{i}) + b)\geq 1$, 而C取有限值时, 一些样本点则可以不满足约束。

补充: **通过符号进行二分类的实现**.
其中b值可以利用下式的任一分量式求解:
$$y_{i}(w \varphi(x_{i}) + b) = 1\qquad  \text{when}  \quad \alpha_{i} \in (0,C)$$
我们**一般做法是计算出每一个$\alpha\in (0,C)$分量对应的 b 之后,取平均值**。
此外, 求解获取到的大部分$\alpha_{i} = 0$,  不为零对应的样本的是支持向量。

### (5) SMO 求解 SVM 拉格朗日乘子
#### 1. 二变量优化步骤推导
整体求解过程参考[SMO 论文部分](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/smo-book.pdf) 
我们无论是在 [[#(2) 优化求解目标推导和分类器函数]] 这一节得到的**最终优化函数 (8.2.4)**, 还是 [[#(4) 带松弛变量的SVM方法]] 这一节得到的**最终优化函数 (8.4.6), 实际上都是将函数转化为了**求解<mark style="background: transparent; color: red">含有拉格朗日参数</mark> $\alpha$ 的最小值问题, 而求解 $\alpha$ 的常用方法，如 **SMO(Sequential Minimal Optimization)算法**(序列最小优化算法), 同时所需内存是线性的, 是一种<b><mark style="background: transparent; color: orange">有效的二次规划算法, 且针对线性 SVM 数据稀疏时,  则性能更加优越</mark></b>。而相对于 PCG Chunking 算法, SMO 性能有显著的提升。

首先介绍坐标上升算法:
坐标上升算法参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/6. 最小二乘, RBF, 岭回归和 Lasso 回归#(3) Lasso 回归模型|Lasso 回归模型坐标下降算法]], 实际上是每一次沿着一个坐标轴寻找最小值, 每次选一个变量进行一维最优化(一维求导=0), 直到终点即可。而 SMO 算法的基本思想与之类似, 每次选择尽量少的 $\alpha$ 进行优化。

首先对公式 (8.4.5): 
$$\Large \psi (\alpha) = \min_{\alpha \geq  0}  \frac{1}{2}\sum^{l}_{i=1} \sum^{l}_{j=1} \alpha_{i}\alpha_{j}  y_{i} y_{j}  \mathcal{K}(x_{i}, x_{j}) -  \sum^{l}_{i=1} \alpha_{i}  \quad \left(\sum^{n}_{i=1} \alpha_{i} y_{i} = 0,  \alpha_{i}\in  [0, C]\right)\tag{8.5.1}$$
上述公式是一个凸二次优化问题, SOM 算法一次选取两个 $\alpha_{i}, \alpha_{j}$ 做优化, 我们将其表示为 $\alpha_1, \alpha_2$。如果 $\alpha_1$ 能够用 $\alpha_2$ 表示出, 再代回上述 $\psi$ 中, 则 $\psi$ 变为 $\alpha_2$ 的函数. 即分解多参数的二次规划问题为多个子问题, 而每个问题为 2 参数;

$f(x)  = w^{T} + b$
![[Excalidraw/8. SVM支持向量机 2024-11-26 18.17.02|250]]
需要说明的是, 每次选取 2 个参数的原因是: $\alpha_1, \alpha_2$ 并非独立, **需要满足 [[#(3) 对偶优化与 KKT 条件|KKT条件]] 才能获得一致最优解**。这样才能保证满足最优解约束。

==**仅有所有点均满足如下的三个条件时, 得到正确的解**==:
1. $\alpha = 0,  \rightarrow y_{i}f(x_{i}) \geq 1$, 正常分类 
2. $\alpha\in (0, C) \rightarrow  y_{i} f(x_{i}) = 1$ 支持向量, 在边界上 
3. $\alpha = C \rightarrow   y_{i}  f(x_{i}) \leq  1$  
根据 KKT 条件, 最优解必须满足上述的三个条件, 

我们将 SVM 的优化问题进行展开, 仅考虑其中有 $\alpha_1, \alpha_2$ 两项, 有:
$$\psi (\alpha_{1},  \alpha_{2})=  \frac{1}{2} (\alpha_{1}^{2} K_{11} + 2\alpha_{1} \alpha_{2} y_{1}y_{2} K_{12 } + \alpha_{2}^{2} K_{22}) - (\alpha_{1}  +  \alpha_{2})  + y_{1} \alpha_{1}\sum_{i = 3}^{l}\alpha_{i}  y_{i} K_{1i } + \ y_{2}\alpha_{2}\sum_{i = 3}^{l } \alpha_{i}  K_{2i}  y_{i}+ C $$
此时目标转化为求解$\psi$ 的最小值, 又根据 $\sum_{i = 1}^{n} \alpha_{i}  y_{i} = 0$, 则有:
$$\text{arg}\min \psi (\alpha_{1}, \alpha_{2}) \quad  \alpha_{1}  y_{1} +  \alpha_{2} y_{2} = \zeta = - \sum_{i = 3}^{n} \alpha_{i} y_{i}$$
其中第二式两端乘 $y_1$ 得到(显然 $y_i y_i =1$):
$$\alpha_{1} + \alpha_{2}y_{1} y_{2} = \zeta y_{1}$$
得到关系:
$$\boxed{\Large \alpha_{1}  = (\zeta  -  \alpha_{2}y_{2}) y_{1}} \tag{8.5.2}$$
这个关系式就是 $\alpha_1$ 和 $\alpha_2$ 的限制关系条件, 有我们取 
$$v_{1}= \sum_{i = 3}^{n} \alpha_{i}y_{i} K_{i,1},\quad   v_{2} = \sum_{i = 3}^{n} \alpha_{i}y_{i} K_{i,2}$$
代入上式有: 
$$\psi (\alpha_{1}, \alpha_{2} )=  \frac{1}{2} (\alpha_{1}^{2} K_{11} + 2\alpha_{1} \alpha_{2} K_{12 } +  \alpha_{2}^{2} K_{22}) - (\alpha_{1} + \alpha_{2}) + \alpha_{1} y_{1} v_{1} +\alpha_{2}  y_{2} v_{2}  + C \tag{8.5.3}$$
对 (8.5.3) 代入 (8.5.2) 并求解 $\psi$ 极值, 通过[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/推导部分/SMO 算法的最优化条件计算.pdf|SMO 算法的最优化条件计算.pdf]] 推导得到如下公式: 
$$\boxed{\Large \alpha^{new}_{2} = \alpha_{2}^{old } +  y_{2}\frac{E_{2} - E_{1}}{\eta}\tag{8.5.3}}$$
其中 <b><mark style="background: transparent; color: orange">E 为在第 i 个样本上的偏差</mark></b>, 而如果样本 $x$ 中有两个输入相同, 则 $\eta = 0$ :
$$\eta = (K_{11} - 2K_{12}  + K_{22}),\quad  E_{1} = f(x_{1}) - y_{1},\quad  E_{2} = f(x_{2}) - y_{2}$$
而考虑到 $0 \leq \alpha_{i} \leq  C$ 的约束, 实际上的 $\alpha$ 取值范围仅能为方形, 而约束条件为:
$$\Large\boxed{\alpha_{1} y_{1} + \alpha_{2}  y_{2} =  \zeta}$$
对于上述约束,仅有 $y_1, y_2$ 同号和 $y_1, y_2$ 异号两种情况, 实际上仅对应如下两种约束类型:
![[Excalidraw/8. SVM支持向量机 2024-11-27 13.16.51|350]]
对于 (8.5.3)式, 如果 $\alpha$ 不在 $0 - C$ 范围内, 必然需要修剪, 我们定义修剪的上下界为: 

1. 当 $y_1 \neq  y_2$ 时, 上下界约束为(其中 $\alpha_2^{old} - \alpha_1^{old}$) 这一项是为了保证之后和为 $\zeta$ 条件不变 实际上是范围域内优化;
$$\Large \boxed{L =  \max (0,  0 +  \alpha_{2}^{old} - \alpha_{1}^{old}) \qquad  H = \min (C, C + \alpha_{2}^{old} - \alpha_{1}^{old})}\tag{8.5.4}$$
2. 当 $y_1 = y_2$ 时, 上下界约束为:
$$\Large\boxed{L  = \max  (0 , \alpha_{1}^{old} +  \alpha_{2}^{old }- C)\qquad  H = \min(C,  \alpha_{1}^{old} + \alpha_{2}^{old})} \tag{8.5.5}$$
得到最终修正后的 $\alpha_2$ 取值
$$\alpha_{2}^{new, clipped}  = \begin{cases}
H \qquad  if  \quad  \alpha_{2}^{new} \geq   H \\ 
\alpha_{2}^{new}  \quad  if \quad  L <  \alpha_{2}^{new} < H \\
L \qquad  if\quad   \alpha_{2}^{new } \leq  L 
\end{cases} \tag{8.5.6}$$
此时由 
$$y_{1} \alpha_{1}^{(new)} + y_{2}\alpha_{2}^{(new)} = y_{1}\alpha_{1}^{(old)} + y_{2}\alpha_{2}^{(old)}$$
容易得到(我们取$s=y_1y_2$):
$$\boxed{\alpha_{1}^{new} = \alpha_{1}^{old} + s(\alpha_{2}^{old} - \alpha_{2}^{new, clipped })} \tag{8.5.7}$$
实际上每次优化都是在两个 $\alpha_1, \alpha_2$ 范围内, 选取目标函数 $\psi$ 的极小值实现最优化

需要注意: 在上述优化过程中, $y = wx + b$ 中的参数 $b$ 实际上被抹去了, 仅仅对 w 进行了优化, 因此每次优化之后, $b$ 都需要进行重新计算:
$$b_{1 }=  E_{1} +  y_{1} (\alpha_{1}^{new} - \alpha_{1}^{old })K_{11}  + y_{2} (\alpha_{2}^{new,clipped } - a_{2}) K_{12}   + b^{old}$$
$$b_{2}  = E_{2} +  y_{1} (\alpha_{1}^{new} - \alpha_{1}^{old })K_{12}  + y_{2} (\alpha_{2}^{new,clipped } - a_{2}) K_{22}   + b^{old}$$
新的 b 选取为:
$$b_{new} =  \begin{cases}
b_{1}  \qquad  0 < \alpha_{1} <   C  \\
b_{2} \qquad  0 < \alpha_{2} < C  \quad  (when \space b_{1}, b_{2}  \text{ all not  at bound, b1=b2})\\
\frac{1}{2} (b_{1} + b_{2}) \qquad  b_{1}, b_{2}  \text{ both at boundary}
\end{cases}$$

#### 2. 基于启发式优化的乘子选取
由于每一次能够优化 $\alpha_1, \alpha_2, \dots  \alpha_n$ 中的两个, 但是我们仍然没有选取优化哪两个乘子参数, 一般可以采用启发式算法进行加速计算和收敛。

首先, 启发式算法会遍历所有的拉格朗日乘子 $\alpha_1,  \alpha_2, \dots \alpha_n$, 并找出<mark style="background: transparent; color: red">一对不符合 KKT 条件的 Lagrange 乘子</mark>$\alpha$ 进行优化。

但是**有时也不会遍历全部的乘子部分**, 一般地, 在第一次遍历找不符合乘子时, 会顺便筛选出<mark style="background: transparent; color: red">非边界乘子</mark>($\alpha_i \neq 0, \alpha_i \neq C$的部分), 之后也仅在这一部分循环, 每次找出一对不符合 KKT 条件的乘子进行优化。

## 三、高维空间映射与常用核函数
### (1) 高维空间映射概念
首先解释为什么引入核函数以及映射的好处

### (2) 常用核函数
#### 1. 点积形式
获得的是线性分类器
$$K(x,y) = x \cdot  y$$
#### 2. 多项式形式的核函数
获取d阶多项式分类器:
$$K(x,y) = \left\{ (x \cdot y) + 1\right\}^{d}$$
#### 3. 径向基函数核函数(高斯核)
径向基函数分类器
$$\Large K(x,y) = \exp\left(-\frac{||x - y||^{2}}{2\sigma^{2}}\right)$$
其中, $\sigma$ 为高斯核带宽。

#### 4. Sigmoid 形式核函数
$$K(x,y) = \tanh (kx \cdot y - \delta)$$
其中, $k > 0$, $\delta <  0$;

#### 5. 指数核函数(拉普拉斯核)
$$k(x_{i}, x_{j}) = \exp \left(- \frac{||x_{i} - x_{j}||}{\sigma}\right)$$
核函数对支持向量机的成功分类很重要, 必须选取合适的核函数才能保证正确分类。
需要说明的是，参考 [Global sensitivity analysis using support vector regression](https://pdf.sciencedirectassets.com/271589/1-s2.0-S0307904X17X00061) 部分, 根据 Mercer 理论, 核函数必须满足条件:
$$\boxed{\iint k(x_{i}, x_{j}) \phi (x_{i}) \phi (x_{j} ) dx_{i} dx_{j} = 0}$$
其中$k(x_{i}, x_{j}) = \varphi(x_{i}) \cdot  \varphi(x_{j})$为核函数， 其中$\phi(x)$是任意的不等于0且满足$\int \phi^{2}(x)< \infty$的函数; 具体证明参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/🚧结构可靠性设计基础/推导部分/mercer's theorem.pdf|mercer's theorem.pdf]] 

> [!caution] 说明
> 实际使用中, 由于核函数的投射方式不同, 所以**往往不同的核函数会导致结果的很大差别**。


## 五、支持向量机回归算法
首先， 考虑训练样本集:
$$D = \left\{ (x_{1}, y_{1}), \dots, (x_{l}, y_{l} )  \right\}$$
假设样本集$D$是$\varepsilon -$线性近似的, 即**存在一个超平面表达式$g_{SVR}(x) = w\cdot x + b$使得下式成立**:
$$\left| g_{SVR} (x_{i}) - y_{i} \right|  \leq  \varepsilon\quad  \rightarrow  \quad d =   \frac{|w \cdot  x + b - y_{i}|}{\sqrt{||w||^{2} + 1}} \leq  \frac{\gamma}{2}$$
图像如下:
![[Excalidraw/第八章 响应面方法和支持向量机 2024-03-30 11.13.46|200]]

> [!important] SVM分类和回归的比较
> 区别于**分类算法的目标是保证样本点全在分类直线范围外的情况下, 最大化间隔带宽度**$\gamma$。 
> 而回归算法的目标是在所有的点都满足$|g_{SVR} (x_{i}) - y_{i}| \leq \varepsilon$的情况下, 此时最大化的容差距离为:
> $$\gamma = \frac{2|\varepsilon|}{\sqrt{||w||^{2} + 1}}$$
> 此时，我们将容差 $< \varepsilon$ 的部分不计入损失, 而大于的部分计入损失; 则**要求最大化容差距离且最小化额外损失**， 公式成为
> $$\min_{w,b} \frac{1}{2} ||w||^{2} + C \sum^{n}_{i=1} (g(x_{i} - y_{i})) * ((y-g(x))\geq \varepsilon)$$
> 后面的部分是以$\xi$表示的(对于容差距离内的点,$\xi = \xi^{*} = 0$)，优化目标是求解不同的$w,b$设置下, 最小化这个损失

与分类问题完全类似，参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/推导部分/支持向量机SVM分类与回归算法推导过程.pdf|支持向量机SVM分类与回归算法推导过程.pdf]] 得到非线性SVM回归的优化问题:
$$\max_{\alpha, \alpha^{*}\geq 0 } \sum^{l}_{i=1}  y_{i} (\alpha_{i}^{*} - \alpha_{i}) - \varepsilon(\alpha_{i}  + \alpha^{*}_{i}) - \frac{1}{2}\sum^{l}_{i=1} \sum^{l}_{j=1} (\alpha^{*}_{i}- \alpha_{i})(\alpha^{*}_{j}- \alpha_{j}) \mathcal{K}(x_{i}, x_{j})\qquad s.t.  \quad  \sum^{l}_{i=1}  (\alpha^{*}_{i} - \alpha_{i})= 0 , \alpha_{i}, \alpha_{i}^{*}  \in [0,C]$$
求解对应的$\alpha_i, \alpha^*_i$。最终支持向量回归表达式为:
$$\Large\boxed{g_{SVR} (x) =  w \varphi (x) + b =  \sum^{l}_{i=1}(\alpha^{*}_{i} - \alpha_{i})k (x_{i}, x) + b }$$
其中$b$可以使用以下式子取平均值进行获取:
$$b = \begin{cases}
y_{j} + \varepsilon - \sum^{l}_{i=1} (\alpha_{i} - \alpha^{*} _{i})k(x_{i}, x_{j}) \qquad  \alpha_{j}  \in (0, C)\\
y_{j} - \varepsilon - \sum^{l}_{i=1} (\alpha_{i} - \alpha^{*}_{i})k(x_{i}, x_{j}) \qquad \alpha^{*}_{j} \in (0, C)
\end{cases}$$

`````ad-note
title: 多分类支持向量机
collapse: close
多分类支持向量机即寻找多个超平面的问题。主要包括直接法和间接法两种: 

- 直接法：直接修改目标函数，将多个分类面的参数求解合并到一个最优化问题中，通过求解该最优化问题“一次性”实现多类分类。
> 特点：简单，但计算复杂度较高，实现较困难，只适合小型问题。 ⋅⋅间接法：主要是通过组合多个二分类器来实现多分类器的构造，如：一对多（one-against-all）和一对一（one-against-one）方法。 

- 一对多和多对一方法

![[Pasted image 20221122030801.png|500]]
![[Pasted image 20221122030826.png|500]]
`````

## 六、交叉验证误差
<mark style="background: transparent; color: red">支持向量机的推广能力</mark>即在算法对于未知的测试集上对数据进行拟合的精度。为了评估模型的推广能力，往往<mark style="background: transparent; color: red">使用测试即并来取验证误差来估计预测的精度</mark>。
### (1) 留一法交叉验证误差
一般使用 $e_{LOO}$ 指代, 并且定义分类问题的**留一法交叉验证误差**(Leave one out Cross-Validation)为:
$$e_{LOO} = \frac{1}{l} \sum^{l}_{i=1} I(- y_{i} g_{SVM}^{\sim i}(x) )$$
其中, $g_{SVM}^{\sim i}$ 表示除去第i个样本之后, <mark style="background: transparent; color: red">使用剩余的样本点构建的支持向量分类模型</mark>。s
而回归问题的留一法交叉验证误差为
$$e_{LOO}  = \frac{1}{l} \sum^{l}_{i=1} I(y_{i}- g_{SVR}^{\sim i}(x) )^{2}$$
对应地 $g_{SVM}^{\sim i}$ 表示除去第i个样本之后, <mark style="background: transparent; color: red">使用剩余的样本点构建的支持向量回归模型</mark>。 
其中右上角$\sim i$表示除去第i个样本构建的支持向量分类模型。 
$$I = \begin{cases}
0 \qquad X > 0 \\
1 \qquad  X \leq 0
\end{cases}$$
### (2) K 折交叉验证误差
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/6. 最小二乘, RBF, 岭回归和 Lasso 回归#(5) 交叉验证, 网格搜索和随机搜索寻优|6. 最小二乘, RBF, 岭回归和 Lasso 回归]] 部分,
