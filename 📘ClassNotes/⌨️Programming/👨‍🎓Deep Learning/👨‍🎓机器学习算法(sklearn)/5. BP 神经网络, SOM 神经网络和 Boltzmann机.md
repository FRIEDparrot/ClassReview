## 一、BP 神经网络理论
### (1) BP 神经网络基本简介
初始的神经网络模型是MP模型. 将人工神经元视为一个二值开关元件。并根据组合方式处理逻辑运算。
最早的有使用价值的神经网络模型是感知器模型（梯度下降也是感知器网络）。而后期发展的 RBF 神经网络模型可以容易地模拟出任何一种非线性数据的变化趋势。

BP 算法即多层前馈神经网络的学习算法。一般是利用外界的输入样本的次级， 并通过不断迭代修正权重向量，使网络输出与期望相符合。<mark style="background: transparent; color: red">一个基本的 BP 神经网络分为如下几个部分</mark>:
1. 输入层
2. 激活函数层 
3. 误差计算 
4. 输出层 
5. 迭代公式

下图说明了单个神经元和 BP 网络的基本结构，
![[Excalidraw/5. 神经网络理论初步 2024-09-29 15.49.56|600]]
输出层即输入上一层的权重和上一层部分的乘积，然后输出分类标签向量部分。

### (2) BP 网络的反向传播机制
<b><mark style="background: transparent; color: blue">和多层感知机不同, BP神经网络的每一都计算与期望结果的偏差, 并按照反向传播的方法将误差传递到上一层， 用于修正上一层的权重</mark></b>。
#### 1. 正向传播
正向传播过程中, 设输入为 $i$ 输出为 $o$, 隐藏层 $h$, 
$$\text{net} =  w^{T} o +b$$
其中, b 为阈值, 实际上也是在 $x$ 首列加入均为 1 的向量; 

实际上按照上图右侧的部分: i~h, h~o 中各有一系权 $w_{ih}, w_{ho}$, 是一个带有两个权重层的输入输出结构。并依次传递计算为:
$$\Large  \boxed{h_{i} =  w_{ih} x_{i} + b_{h} \quad h_{o} =f_{1}(hi), \quad yi = w_{ho} \times  h_{o} + b_{o}  \quad  y_{o} = f_{2}(y_{i})}$$
其中, 每一层的传递函数(激活函数)为:
$$f(x) = \frac{1}{1 + e^{-x}} = y$$
#### 2. 误差计算
在误差计算中, 首先会**计算网络实际输出 $y_{o}$ 和期望输出 $y_d$ 的差**。<mark style="background: transparent; color: red">并判断差是否低于容限</mark>, 如果高于容限，则进行误差反向传播。即取:
1. 误差向量
$$\text{err} =d_{o}- y_{o}$$
2. 全局误差函数
我们在后面计算误差反向传播时, 实际上是最小化全局误差函数: 
$$e  = f_{\text{err}} = \frac{1}{2} \sum (d_{o} - y_{o})^{2}$$
#### 3. 反向传播
我们取 dlogit 函数为 sigmoid 函数的导函数:
$$f'(\text{net}) = \frac{1}{1 + e^{-\text{net}}} - \frac{1}{(1 + e^{-net})^{2}}  = y(1-y)$$
输出层的误差对于$w_{ho}$的微分可以写为:
$$\frac{\partial \text{e}}{\partial w_{ho}} = \frac{\partial \text{e}}{\partial y_{i}} \frac{\partial y_{i}}{\partial w_{ho}}$$
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/推导部分/BP 神经网络的反向传播计算.pdf|BP 神经网络的反向传播计算.pdf]]，可以获取到每一项的导数:
$$\Large\boxed{\frac{\partial e}{\partial w_{ho}} = - \delta_{0} h_{o}\qquad  \frac{\partial e}{\partial w_{ih}} =  - \delta_{h} x_{i}}$$
其中, 设 $\delta$ 为每一项的梯度, 则根据下式得到<b><mark style="background: transparent; color: blue"></mark></b>为:
$$\boxed{\frac{\partial e}{\partial h_{o}}  = - \delta_{o}\qquad  \delta_{o} = (d-y_{o}) f_{2}'(y_{i})}\tag{1}$$
$$\boxed{\frac{\partial e}{\partial h_{i}} =  - \delta_{h}\qquad  \delta_{h} = - \delta_{0} w_{ho} f_{1}'(h_{i})}\tag{2}$$
并得到每一层的权重迭代更新公式:
$$\boxed{\Large\begin{cases}
w_{ho} ^{N+1} = w_{ho}^{N} + \eta_{1} \cdot  \delta_{o} h_{o} \\
w_{ih}^{N+1} = w_{ih}^{N} +\eta_{2}  \cdot   \delta_{h} x_{i}
\end{cases}}$$
### (3) BP 网络的动量因子简介
除了学习率 $\eta$, 迭代次数 (max_iter) 和误差容限 (error_boundary), 往往我们还使用<mark style="background: transparent; color: red">动量因子</mark>来进行 BP 网络的参数调优。一般可以取为0.3左右; 

引入动量因子的好处: 最早的BP算法在修正权值时, 需要**按照迭代次数 t 的梯度进行调整**。而不考虑在 t-1 部分的梯度方向的影响, <mark style="background: transparent; color: red">导致网络发生震荡, 收敛缓慢等等</mark>, 而<mark style="background: transparent; color: red">动量因子考虑分配 t 时刻和 t-1 时刻的梯度</mark>; 此外，<u>没有附加动量因子时, 网络有可能陷入局部极小值。</u>

具体的分配方法是设动量因子为 $MC$ 并**记录上一次的梯度大小**:
$$w_{ih}^{N+1} = w_{ih} + (1 - MC)\times  \eta \left. \frac{\partial e}{\partial w_{ih}}\right|_{t} + MC \times \eta \left. \frac{\partial e}{\partial w_{ih}} \right|_{t =  t-1}$$

另外, 由于网络的初始权值是随机选取的, BP网络对于权重的初始值是非常敏感的,  导致迭代过程中误差函数的计算结果也不相同, 得到曲线也不相同。

误差信号反向传播计算过程中, 参考(1),(2); 首先计算 output 和 y_true 的差 err, 并根据 output = y, 可以直接采用 $f_{2}'(y) = y(1-y)$ 计算出输出层梯度, 然后直接采用 $\text{err}* f_{2}'(x)$ 直接得到 $\delta_{o}$，然后即可直接采用 $\delta_o * h$ 得到第一个梯度 $\frac{\partial e}{\partial w_{ho}}$ , 同时记录这个梯度以备动量因子使用。

对于学习速率的动态调节, 策略是<mark style="background: transparent; color: red">在迭代中检验权值是否导致误差函数值的降低; 如果降低, 则说明当前的学习速率可以增加, 则适当增加一个量; 如果不是, 则说明调整过度, 此时可以降低学习速率</mark>.

## 二、自组织特征映射神经网络(SOM)
### (1) SOM 网络框架
自组织映射神经网络(Self-Organization Feature Map, SOM)是根据仿生思想设计的一类神经网络, 也称为Kohonen 映射，是一种无监督学习算法，并用于解决模式识别类的问题。同时，SOM方法不需要提供预先的聚类数量，而是由网络自动识别出来的。一般仍然是<mark style="background: transparent; color: red">将距离小的个体划分为同一类别, 而距离大划分为不同的类别</mark>。

基本原理是, 在外界信息输入时, 大脑皮层的相应功能区会产生兴奋, 此时位置相邻的神经元具有相近的模式,而远离的神经元则具有较大差别。例如<mark style="background: transparent; color: red">当声波的频率与大脑皮层神经元中的某些存储模式接近, 则相应的模式容易被识别。</mark> 

与其它神经网络不同的是,  SOM 方法与<mark style="background: transparent; color: red">同层的神经元之间建立侧向连接</mark>, 并通过权值学习形成特定的模式, 而输出层为棋盘形状, 其神经元可以形成多种形式并映射出不同的模式。

<b><mark style="background: transparent; color: red">SOM 网络仅包含输入层和输出层两层</mark></b>, 其中, 节点数即为样本的维度。并**通过权重向量将训练数据传递到各个输出单元**。
![[Excalidraw/5. 神经网络理论初步 2024-10-04 10.33.18|350]]

1. 首先在输入时, 需要归一化数据集
2. 构建==输出层网络==, 例如<mark style="background: transparent; color: red"> 2维输入且能够分为 4 类</mark>, 则采用 4 x 2 的矩阵进行分类。需要说明的是， 4是输出层的预估分类数, 实际的聚类数量是由数据本身的分布决定的。
3. 权重节点的维度是**由输入层数据集的维度和预估分类数决定的**, 权重列数一般选择和分类数相同， 而<mark style="background: transparent; color: red">行数一般和输入数据的维数相同</mark>, 例如, 2 维输入数据分为 4 类, 则<mark style="background: transparent; color: red">权重的行数设置为 2, 权重列数设置为 4</mark>.
4. 动态学习函数定义
5. <b><mark style="background: transparent; color: blue">聚类半径函数定义</mark></b>: 一般而言, SOM 方法采用**定义的学习半径**进行分类, 而学习半径不同, 则聚类效果也不同

在 SOM 网络中, 需要**定义网络的行数 m 和列数** n, 并且满足 $m \times  n =$ 聚类的总个数。(建立网格的维数 = 输入数据的维数)
```python
def __init_net_grid(self, net_m, net_n):  
    """  
    :param net_m: 网络的行数  
    :param net_n: 网络的列数  
    """   
	self.M = net_m  
    self.N = net_n  
    self.cls = self.M * self.N  # number of approximate clusters  
    net_grid  = np.zeros((self.cls, self.nd))  
  
    for i in range(self.M):  
        for j in range(self.N):  
            net_grid[i * self.N + j] = np.array([i, j])  # 生成网格节点
```

初始生成的网络如图所示: 
![[Excalidraw/5. 神经网络理论初步 2024-10-04 15.39.50|700]]

SOM 方法的聚类过程如下:
1. <mark style="background: transparent; color: red">首先计算学习率和学习半径, 并且从训练集中随机选取一个样本</mark>作为**初始节点**。 
2. 以特征数量 (n) * 分类数目 (cls) 为大小建立权重矩阵,
3. 优胜邻域 : 根据两个节点计算出聚类的邻域, 并找出邻域内所有的节点 
4. **调整权重** 

对于 iris 数据集, 输入维数是特征的个数, 一般为 4 

SOM 核心思路: 
1. 首先<mark style="background: transparent; color: red">针对于每一个目标类, 生成一个大小为 n (特征数量) 的向量, 实际上形成一个 n * cls 的权重矩阵 wt, 表示每个分类的向量方向</mark>,  取<mark style="background: transparent; color: red">其中和当前随机样距离最近的一条</mark>为优胜节点
2. 构造一个类似大脑皮层的, 仅用于表示距离的网格 (grid), 其大小为 ndim * cls 
3. 每一次随机选取一个样本, 找到与之最近的向量 `wt[i]` 并投到大脑皮层 grid 中(找到 grid 中相应的点), 此时寻找 grid 中距离小于 r 的点, 并反投射到 wt 中。
4. 对于所有wt中投射距离小于 r 的点, 更新权重, 方法是用这个样本点的值 * 学习率;
5. 预测时, 采用距离最近的 $wt$ 作为实际分类的预测值。
```python fold title:对于SOM方法的python代码实现
import numpy as np  
from sklearn.preprocessing import StandardScaler  
from sklearn.datasets import load_iris  
import matplotlib.pyplot as plt  
  
maxr = 1.5  
minr = 0.5  
max_iter = 3000 # 迭代次数设定 
  
lr_max = 0.2  
lr_min = 0.001  
  
class SOM:  
    def __init__(self):  
        x, y_true = load_iris(return_X_y=True)  
        std = StandardScaler()  
        self.x = np.array(std.fit_transform(x), dtype=np.float32)  
        self.y = np.array(y_true)  
        self.nd = np.ndim(self.x)  # number of dimensions  
        self.m, self.n = x.shape   # 5-features input dataset (shape is m x n)  
        self.grid = self.__init_net_grid(1,3)  # generate 1x3 grid  

    def lr_radius(self, t):  
        lr = lr_max - (t + 1) * (lr_max - lr_min) / max_iter  
        r = maxr - (t + 1) * (maxr - minr) / max_iter  
        return lr, r 
        
    def dist(self, x1, x2):  
        return np.sqrt(np.sum((x1 - x2)**2, axis = -1))  # sum distance in the last axis  
  
    # 初始化网格  
    def __init_net_grid(self, net_m, net_n):  
        """  
        :param net_m: 网络的行数  
        :param net_n: 网络的列数  
        """       
        self.M = net_m  
        self.N = net_n  
        self.cls = self.M * self.N  # number of approximate clusters  
        net_grid  = np.zeros((self.cls, self.nd))  
        for i in range(self.M):  
            for j in range(self.N):  
                net_grid[i * self.N + j] = np.array([i, j])  # 生成网格节点  
        return net_grid
        
    def train_net(self):  
        # initialize input layer5 
        self.wt = np.random.rand(self.cls, self.n)  # n-dimension x cluster  
        # train the network 
         for iter in range(max_iter):  
            lr, r = self.lr_radius(iter)   # 获取权重向量以及学习率
            # select a sample randomly from the input  
            rid = np.random.randint(0, len(self.x))  
            xr = self.x[rid,:]  
  
            # calculate the distance between the sample and the weight nodes  
            idx_min = np.argmin(self.dist(xr, self.wt), axis=0)      # find the nearest node  
  
            """说明: idx_min 是各个权重向量中, 与 xr 最近的权重向量的下标, 显然 idx_min < self.M * self.N """            
            d1 = np.ceil(idx_min/self.N) # 获取该权重向量在网格中的行下标  
            d2 =  idx_min // self.N      # 获取该权重向量在网格中的列下标  
  
            # 构造距离矩阵, 计算每个grid 与所得部分投影之间的距离  
            dist_mat = self.dist(np.array([d1, d2], dtype=np.float32), self.grid)  
            idx_les = (dist_mat < r).nonzero()[0]  # 在网络中, 获取所有神经元间距小于 r 的节点的下标  
  
            # 更新分类的权重, 将投影距离小于分类部分的权重进行学习更新  
            self.wt[idx_les, :] += lr * (xr - self.wt[idx_les, :])  
  
    def predict(self):  
        y_true = self.y  
        y_pred = np.zeros(self.m)  
        for i in range(self.m):  
            y_pred[i] = np.argmin(self.dist(self.x[i,:], self.wt))  # 采用最终的 wt 和 输入进行比较, 找到距离最近的权重向量作为分类预测值  
  
        fig, axes = plt.subplots(1,2, figsize=(10, 5))  
        axes[0].scatter(self.x[:,0],self.x[:,1], c = y_true)  
        axes[0].set_title('True Labels')  
        axes[1].scatter(self.x[:,0],self.x[:,1], c = y_pred)  
        axes[1].set_title('Predicted Labels')  
        plt.show()  
  
if __name__ == '__main__':  
    som = SOM()  
    som.train_net()  
    som.predict()
```

结果如图所示 :
![[attachments/Pasted image 20241004205950.png|450]]

### (2) 采用第三方库建立 SOM 网络
对于 sklearn 中, 没有提供 SOM 网络的建立函数, 但是可以采用如下的方法进行建立: 
```shell 
pip install sklearn-som
from sklearn_som.som import SOM 
```

实际的 SOM 网络建立和编写, 可以采用如下方法进行:
```python fold title:SOM自组织映射神经网络的建立
import numpy as np  
from sklearn_som.som import SOM  
from sklearn.datasets import load_iris  
from sklearn.preprocessing import StandardScaler  
from sklearn.pipeline import Pipeline  
import matplotlib.pyplot as plt  
from sklearn.inspection import DecisionBoundaryDisplay  
from sklearn.decomposition import PCA 

class CustomSOM(SOM):  
    """  
    重定义 SOM 类，添加 fit_predict 方法  
    SOM 是继承SOM类, 用于给 pipeline 进行使用  
    """    def fit_predict(self, X, y=None):  
        self.fit(X)  
        return self.predict(X)  
  
X, y_true = load_iris(return_X_y=True)  
  
# 创建Pipeline  
clf = Pipeline(steps=[  
    ('scaler', StandardScaler()),  
    ('som', CustomSOM(m=1, n=3, dim=4))  
])  
  
# 训练并预测  
y_pred = clf.fit_predict(X)  
fig, axes = plt.subplots(1, 3, figsize=(15, 5))  
axes[0].scatter(X[:, 0], X[:, 1], c=y_true)  
axes[0].set_title('True Labels')  
axes[1].scatter(X[:, 0], X[:, 1], c=y_pred)  
axes[1].set_title('Predicted Labels')  

# 用于从二维数据中 Pipeline 提取 SOM 分类边界模型  
clf2 = Pipeline(steps=[  
    ('scaler2', StandardScaler()),  
    ('som2', CustomSOM(m=1, n=3, dim=2))  
])  
  
y_pred2 = clf2.fit_predict(X[:, 0:2], y_true)  
DecisionBoundaryDisplay.from_estimator(clf2,  
                                       X[:, 0:2],  
                                       response_method="predict",  
                                       xlabel="sepal length (cm)",  
                                       ylabel="sepal width (cm)",  
                                       ax=axes[2])  
axes[2].set_title('Decision Boundary')  
axes[2].scatter(X[:, 0], X[:, 1], c=y_pred2)  
plt.show()
```
显然新的 SOM 方法比初始的 SOM 方法获得了更好的分类效果;
![[attachments/Pasted image 20241004204653.png|600]]

### (3) Boltzmann 机算法(模拟退火算法)
Boltzmann机方法是在研究[Hopfield 网络](https://en.wikipedia.org/wiki/Hopfield_network#:~:text=The%20Hopfield%20network%2C%20named%20for,neuron%20j%20to%20neuron%20i.)时提出的思想, 这是由于采用 boltzmann 分布(或Gibbs分布)作为网络的激活函数.
模拟退火算法参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/🌟机器学习优化算法合集/1. 模拟退火算法(SA)|1. 模拟退火算法(SA)]], 熵和 Gibbs 方程参考 [[📘ClassNotes/👨‍🔧Mechanics/🌊Thermal and Fluid dynamics/♨️Engineering thermodynamics/第三章 理想气体的性质与过程|理想气体的性质与过程]];  
首先我们了解 [Boltzmann 分布](https://en.wikipedia.org/wiki/Boltzmann_distribution), 该分布用于描述一个系统在某种状态的概率. 其分布形式是:
$$p_{i} \propto \exp  \left(- \frac{\varepsilon_{i}}{k T}\right)$$
其中,  $\varepsilon$ 为状态的能量, $k$ 为玻尔兹曼常数, 而 $T$ 为温度,实际分布如图所示:
![[attachments/Pasted image 20241004221753.png|300]]
对于能量较低的部分, 往往会具有较高的可能性。同时有两个状态之间的转移关系:
$$\frac{p_{i} }{p_{j}} = \exp \left( \frac{\varepsilon_{j} -  \varepsilon_{i}}{k T }\right)$$
分布的概率密度函数给出为:
$$\Large \boxed {p_{i} = \frac{\exp (- \frac{\varepsilon_{i}}{k T} )}{\sum_{j = 1}^{M} \exp  (- \frac{\varepsilon_{i}}{k T })}}$$
此时, 玻尔兹曼分布的最大化熵部分实际上是:
$$S(p_{1},  p_{2}, \dots p_{M }) = - \sum_{i = 1}^{M} p_{i} \log_{2} p_{i}$$
我们在输出向量中取概率为 $p$ 我们仅将一般神经网络中的 $\text{net} = w^T x$ 进行修改。
$$P\left\{ Y = 1 | x \right\} = p(x) = \frac{1}{1 + e^{-\frac{\text{net}}{T}}}\qquad  P \left\{Y = 0| x \right\} = \frac{1}{1 + e^{\frac{\text{net}}{T}}}$$
其中, 如果 Y=1，则概率为 $p$, 同时符合(参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/4. 最优化方法, 梯度寻优法及Logistic 回归#(2) Logistic 函数和 Logistic 梯度下降法|Logistic 函数]]):
$$\Delta = \frac{P\left\{ Y = 1| x\right\}}{P\left\{Y = 0| x\right\}} = e^{\frac{\text{net}}{T}}$$
其中 $\Delta$ 称为 Boltzmann 因子, <b><mark style="background: transparent; color: orange">对应地, 使用这种分布的神经网络称为 Boltzmann 机或者 Boltzmann 神经网络</mark></b> 
一般地, T 的选择会对优化结果产生很大影响, T 越大, 则logistic 曲线越平滑, <mark style="background: transparent; color: red">当有 T = 0 时, 则函数变为 hardlim 硬限幅函数</mark>;
![[Excalidraw/5. 神经网络理论初步 2024-10-04 23.43.45|300]]
一般取 $T(k + 1) = \lambda T(k)$ , 而在实际情况下, 为了在迭代过程中最小化 $\varepsilon$, 我们每一次退火时,采用如下方法决定跳变:
$$\lambda = \frac{p_{k + 1} }{p_{k}} =\min\left\{ \exp\left(-\frac{\varepsilon_{k} - \varepsilon_{k + 1 } }{T}  \right), 1\right\}$$
```python
def boltzmann(new, old, T):
	return (new - old)/T
```
然后随机生成一个数字 t,  当有 $t < \lambda$ 时, 则进行跳变($\text{cur\_val} +=  lr * (\varepsilon_{k+1} - \varepsilon_k)$) 或者进行交换当前状态, 最终即可求得最小值的解。

