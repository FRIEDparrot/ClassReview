## 一、机器学习基本内容
神经网络的主线包含: 
1. 分类, 回归和预测 
2. 贝叶斯理论和智能推理 
3. 算法部分

```python
a = [1,2,3,4]
np.mat(a)
```
通过 numpy 库可以实现矢量化编程, 通过 GPU 进行并行计算和大规模浮点运算, 可以提供较高的处理性能。

### (1) 范数, 欧式距离, 切比雪夫距离和汉明距离

> [!caution] 范数的定义
> 对于一般的范数, Lp 范数定义为各个元素的p次方的和的 1/p 次方, 而距离一般定义为:
> $$(\sum^{n}_{i=1} |p_{i} - q_{i}|^{k})^{\frac{1}{k}}$$

一般地, 我们将两个向量的特征采用浮点数进行表示,则可以采用距离(范数)来说明某个数据, 而一般地, 距离越近的向量, 越容易归为一类。

对于**两点的欧氏距离**,可以采用下式进行计算:
$$d_{12}  =  \sqrt{(A - B) (A- B)^{T}}$$
而**切比雪夫距离**为 $\max(|x2 - x1|, |y2 - y1|)$, 可以采用
$$\lim_{k \rightarrow \infty} (\sum^{n}_{i=1} |x_{1i} - x_{2i}|^{k})^{\frac{1}{k}}$$
其二, 我们常常通过方向余弦衡量样本量之间的差异, 参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/第八章 向量代数和空间解析几何|第八章 向量代数和空间解析几何]], 有
$$\cos \theta = \frac{\vec{a}\cdot\vec{b}}{|a||b|}$$

> [!NOTE] 汉明距离
> **汉明距离**定义为两个等长字符串之间的<mark style="background: transparent; color: red">汉明距离定义为</mark>**将其中一个变为另外一个的最小交换次数**:
> 一般可以采用如下方法获取:
> ```python
>  s1 = np.mat([1,0,1,0,0,1,1])
>  s2 = np.mat([0,1,0,1,1,1,0])
>  len(np.nonzero(s1 - s2)[1])
> ```
> 汉明距离常用于信息编码, 一般会使编码之间的汉明距离尽可能大以增加容错率

**杰卡德相似系数**:对于两个集合 A, B 中的交集元素在 A, B 并集中占的比例为杰卡德相似系数。用于衡量两个集合的相似度
$$J(A,B) =  \frac{|A \bigcap B|}{|A\bigcup B |}$$
**杰卡德(Jaccard)距离**: 定义为 
$$J_{\delta} (A, B) = 1 - J(A, B) = \frac{|A \bigcup  B | + |A \bigcap B |}{|A \bigcup B |}$$
> [!hint] 常用的距离
> 欧式距离， 相关系统， Jaccard 距离和余弦相似度等

### (2) 多元统计基础
> [!NOTE] 常见的多元统计算法
> 包括朴素贝叶斯分析,回归分析,统计学习基础, 聚类分析, 主成分分析(PCA), 概率图模型等等。

最常用的贝叶斯公式表达了条件概率的关系:
$$P(B|A) =  \frac{P (A |B) P(B)}{P(A)}$$
从概率统计的角度, 一个对象可以表示为n个随机变量的整体, 其中 $X = (X_1,X_2, \dots X_n)$ 为 n 维随机变量或者随机向量。**每个对象是随机向量中的一组取值, 而矩阵中的所有对象构成了随机向量的联合和边缘密度概率分布**。
例如, 对于10个苹果, 其中红的8个, 黄的2个; 而梨有黄色和绿色, 其中黄色为9个。此时, 可以构建出如下的概率表, 对应的**联合概率密度分布**和**边缘概率密度分布**如图:
![[Excalidraw/1.机器学习算法基本内容 2024-09-05 11.09.17|650]]
所有以概率相关的算法**均以对象的联合概率密度分布和边缘概率密度分布为运算基础**。

相关系数定义参考[[📘ClassNotes/📐Mathmatics/🎣Probability Theory/第四章 随机变量的数字特征#三、协方差及其相关系数|随机变量的数字特征]], 定义为(范围为 -1 ~ 1):
$$\boxed{\rho_{XY} = \frac{E\{[X - E(X)][Y - E(Y)]\}}{\sqrt{D(X)} \sqrt{D(Y)}}}$$
而**相关距离**定义为1与该系数的差值: 
$$D_{XY} = 1 - \rho_{XY}$$
根据随机变量的数字特征, 我们可以将欧式距离发展为马氏距离公式:

**马氏距离**:
马氏距离(Mahalanobis Distance),定义为对于M个样本向量 $X_1 \sim X_{m}$, 其协方差矩阵记为$S$, 均值为 $\mu$, 则样本向量到均值 $\mu$ 的马氏距离定义为: 
$$D(x) = \sqrt{(\boldsymbol{x} - \mu)^{T} S^{-1} (\boldsymbol{x} - \mu)}$$
其中, 对于两个向量 $X_{i} ,X_{j}$, 则其马氏距离定义为:
$$\Large\boxed{D(X_{i}, X_{j}) =  \sqrt{(X_{i} - X_{j})^{T }  S^{-1} (X_{i} - X_{j})}}$$
其中当协方差为对角矩阵时, 转换为欧氏距离。其<mark style="background: transparent; color: red">优点是量纲无关, 排除变量之间的相互干扰</mark>。

计算代码如下, 比较简单:
```python
import numpy as np
import numpy.linalg as la  

def mahalanobis_distance(x1 : np.matrix, x2:np.matrix, S):  
    """  
    Calculates the Mahalanobis distance between two vectors x1 and x2 given the covariance matrix S.  
    Parameters:    x1 (numpy.ndarray): The first vector.    x2 (numpy.ndarray): The second vector.    S (numpy.ndarray): The covariance matrix.    Returns:    float: The Mahalanobis distance between x1 and x2.    """    diff = x1 - x2  
    D = np.sqrt(diff.T * la.inv(S) * diff)  
    return D  
  
if __name__ == "__main__" :  
    x1 = np.mat([1, 2, 3])  
    x2 = np.mat([4, 5, 6])  
    S  = np.mat([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  
    print(mahalanobis_distance(x1.T, x2.T, S))
```

### (3) 矩阵的空间变换
空间的变换特性是矩阵最重要的特性之一, <mark style="background: transparent; color: red">由特征列的取值范围所有构成的矩阵空间应当具完整性， 并能够反事物的空间形式或者变化规律</mark>。
一般而言, 训练集所构成的矩阵常常包含大量的样本, 要求: 
1. 向量和矩阵的相关运算是具有意义的运算, 并且能够解释出事物的空间形式， 
2. 计算结果应当能够反映其本质特征, 从而映射出对象集合表达的形式和规律。
n维正交空间即为 n 个彼此正交的基地向量构成的空间

在基底进行一定的变换时, 相应的所得的坐标也会发生相应的变换, 例如在直角坐标系中的 $\alpha = (1,2)$, 计算新坐标系i'=(3,1), j'=(1,3)下的坐标, 容易得出是 i' * 1 + j' * 2 = (5,7):

则从(1,2) 到 (5,7) 的过程, 则<mark style="background: transparent; color: red">可以找到一个变换矩阵</mark>, 直接完成变换过程 :
```python
A = [[3,1], [1,3]]
i' =  A * i 
j' = A * j
```
$$\alpha' = A * \alpha = [\alpha_{1}, \alpha_{2}] * \alpha  =  (5,7)$$
其中 `A = [[3,1], [1,3]]` 为变换矩阵, 将坐标从 i, j 变换到 $[\alpha_1, \alpha_2]$ 构成的新坐标系。

需要说明的是, 向量的变换可以分解为伸缩和旋转, 类似复数运算, 设 $\alpha = [r \cos \theta, r \sin \theta]$, 则
$$  \lambda  e^{i\theta} * \alpha =  \lambda  *  \left[\begin{matrix}
\cos \Delta \theta  &  - \sin \Delta  \theta  \\ 
\sin \Delta  \theta  &  \cos \Delta  \theta  
\end{matrix}\right]  *  \left[\begin{matrix}
r \cos \theta  \\ r \sin \theta
\end{matrix}\right]$$
另外, 根据矩阵的特征向量和特征值$\lambda$, 我们可以还原初始的矩阵:
$$A = P  \Lambda P^{-1}$$
其中 P 为特征向量构成的矩阵, $\Lambda$ 为特征值排成的对角矩阵; 特征值 ($Av = \lambda v$)
```python 
import numpy as np  
import numpy.linalg as la  
  
A = np.mat([[8,1,6],[3,5,7],[4,9,2]], dtype=np.float64)  
lmbda, v = la.eig(A)  
  
Sigma = np.diag(lmbda)  
B = v * Sigma * la.inv(v)  
print(B)
```

针对数据的归一化一般的方法是通过比例缩放, 对特征进行规范化处理, 映射到某个区间。
$$X^{*} =  \frac{x - \mu}{\sigma}$$
## 二、文本挖掘方法
一般的做法是从大量文本数据中抽取事先未知的, 可理解的, 可用的知识的过程。主要包含: 
1. 搜索和信息检索
2. 文本聚类, 文本分类,文本挖掘, 信息抽取(IE), 自然语言处理(NLP), 概念提取

目前的文本分类算法主流包含: 
1. 基于模式系统
2. 基于分类模型 (机器学习, 一般需要一组预分类和训练来建立分类) 

<mark style="background: transparent; color: red">完全解决中文分词算法的是基于概率图模型的条件概率随机场(CRF)方案, </mark> 

### (1) 向量空间模型
向量空间模型中, 一般将文本作为向量进行存储, 向量中每个特征表示文本中出现的词语。通常将训练集中的不同字符串均作为一个维度, 虽然能够有效处理高维空间的文本, 但是可能导致较高维的存空间。

避免维度灾难的方法一是存储某些停用词并进行过滤。通过输入的停用词表。

### (2) 权重策略和 TF-IDF 方法
依据[参考资料](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) term-frequency - inverse documentation frequency. 在tf-idf方法中, 首先需统计文档中, 各个词的词频信息。而 tf-idf 方法的词频统计规则如下: 
1. <mark style="background: transparent; color: red">如果一个词或者短语在一篇文章中出现频率高, 而其他很少出现, 则认为该词适合用于文章分类</mark> 
2. <mark style="background: transparent; color: red">高频率词应该具有最高权重, 除非其也是最高文档频率</mark> 

设所有的文本由 $i = 1,2, \dots n$ 个文件构成, 而其中有j = 1,2, ...m 个词语, 则某个词 i 在某个文件 j 的词频 $TF_{ij}$ 可以计算为: 
$$TF_{ij} = \frac{n_{i,j}}{\sum_{k=1}  n_{k,j}}$$
另外引入增强频率(用于放止文章过长)
$$\text{tf}(t, D) = 0.5 + 0.5  \frac{f_{t,d}}{\max \{ f_{t', d}: t'\in  D\}}$$
<mark style="background: transparent; color: red">其中需要除以分母(文档j中所有的词的出现次数之和,包括其他所有类型词汇)</mark>,逆文件频率用于衡量一个词提供信息的多少。
$$IDF_{i}  = \log  \frac{|D|}{|\{ j : t_{i} \in d_{j}\}|}$$
N : 文件总数
j : 出现某个词的文章总数
一般地, IDF 越小, 则这个词汇更多可能是常用词汇，因此提供信息量就小; 此时即可计算: 
$$TF-IDF(i,j,D) = tf_{ij}  \times  idf (i, D)$$
对应的, TF-IDF 较高的词语，也是分类特征较强的词语。
采用 sklearn 的 rcv1 新闻文章数据集作为文本分词依据, [参考资料](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_rcv1.html#sklearn.datasets.fetch_rcv1) 
```python
from  sklearn.datasets import fetch_rcv1
rcv1_data = fetch_rcv1( download_if_missing=True, shuffle=False)
```
文件训练集路径为 `C:\Users\Parrot\scikit_learn_data\RCV1` 

The **RCV1** dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.

RCV1 包含 rcv1-v1 和 rcv1-v2, 其中 RCV1-v2 做了一定修改, 用于获取更好的测试集合。
![[attachments/Pasted image 20240910101400.png|600]]
在 RCV1 中, 主体码主要分为四等级(CCAT, ECAT, GCAT, MCAT) 
![[attachments/Pasted image 20240910105001.png]]
Each story was required to have <mark style="background: transparent; color: red">at least one Topic code and one Region code.</mark> 

对于 rcv1 数据集, 训练集含有 23149 样本, 共有 47236 词汇, 可分类 126 种主题词。但是有的部分 RCV1 在分类时不实际使用, 因此实际上是 103 分类问题。主要有 C, E, G, M 四个大类。具体从 target_names 中可以找到:
```python
array(['C11', 'C12', 'C13', 'C14', 'C15', 'C151', 'C1511', 'C152', 'C16',
       'C17', 'C171', 'C172', 'C173', 'C174', 'C18', 'C181', 'C182',
       'C183', 'C21', 'C22', 'C23', 'C24', 'C31', 'C311', 'C312', 'C313',
       'C32', 'C33', 'C331', 'C34', 'C41', 'C411', 'C42', 'CCAT', 'E11',
       'E12', 'E121', 'E13', 'E131', 'E132', 'E14', 'E141', 'E142',
       'E143', 'E21', 'E211', 'E212', 'E31', 'E311', 'E312', 'E313',
       'E41', 'E411', 'E51', 'E511', 'E512', 'E513', 'E61', 'E71', 'ECAT',
       'G15', 'G151', 'G152', 'G153', 'G154', 'G155', 'G156', 'G157',
       'G158', 'G159', 'GCAT', 'GCRIM', 'GDEF', 'GDIP', 'GDIS', 'GENT',
       'GENV', 'GFAS', 'GHEA', 'GJOB', 'GMIL', 'GOBIT', 'GODD', 'GPOL',
       'GPRO', 'GREL', 'GSCI', 'GSPO', 'GTOUR', 'GVIO', 'GVOTE', 'GWEA',
       'GWELF', 'M11', 'M12', 'M13', 'M131', 'M132', 'M14', 'M141',
       'M142', 'M143', 'MCAT'], dtype=object)
```

特征代码(character codes)是从 I00000-I90000部分的, 但是需要注意: 有部分不同编码但类别相同的部分, 例如 I22400 和 I22470 部分。均为 NON-FERROUS METALS.  此外, I65000 没有对应的文章, 

一般的代码部分均为 6-8 位(末尾加0表示padded). 

```python 
import re  
import os  
import sklearn  
from sklearn import feature_extraction  
from sklearn.feature_extraction.text import TfidfVectorizer      # tf-idf 向量生成类 
from sklearn.feature_extraction.text import TfidfTransformer  # tf-idf 向量转换类
```

对于 tfidf_vectorizer, 主要的用法如下: 
```python
word_list = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?"
]

vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)
tfidf_matrix = vectorizer.fit_transform(word_list)
```

因此只需要对应构造出相应的数据集，即可得到其中的tf-idf矩阵, 
```python
import re  
import os  
import sklearn  
from sklearn import feature_extraction  
from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import TfidfVectorizer  
from lxml import html, etree  
from bs4 import BeautifulSoup

def get_content_in_html(html_content):  
    text_content = BeautifulSoup(html_content, 'html.parser').get_text()  
    text_content = re.sub(r'[^a-zA-Z0-9\s\-_]', '', text_content)  # eliminate the special characters  
    return text_content  
  
if __name__ ==  "__main__" :  
    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)  
    transformer = TfidfTransformer()  
  
    contents_dictionary = []  
    files = os.listdir("./html")  
  
    for file in files:  
        with open("html/" + file, 'rb') as f:  
            contents = f.read()  
            doc_text = get_content_in_html(contents)  
            contents_dictionary.append(doc_text)  
  
	tfidf_matrix = vectorizer.fit_transform(contents_dictionary)  
	print(tfidf_matrix.shape)  
	print(vectorizer.vocabulary_)
```
即可得到对应的词汇的相应的频率。

一般的, 由于我们是预测频率, 所以采用多项式朴素贝叶斯方法进行预测。其中, 常用的分类算法包含 KNN 算法, 朴素贝叶斯算法和支持向量机算法。 <mark style="background: transparent; color: red">一般对于短文本, 朴素贝叶斯方法分类效果较好， 一般采用多项式朴素贝叶斯均可以达到很高的精度</mark>。
```python
from sklearn.naive_bayes import MultinomialNB  
from sklearn.datasets._base import Bunch

vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)  
transformer = TfidfTransformer()  
  
train_set = Bunch(tfidf_matrix=None, target_label=None)  
train_set.tfidf_matrix = vectorizer.fit_transform(contents_dictionary)  
train_set.target_value = [0,0,1,1,1,1,1,1] 

# use Multinomial Naive Bayes classifier  
mbs = MultinomialNB(alpha=0.001)   # for smaller alpha value, it  
mbs.fit(train_set.tfidf_matrix, train_set.target_value)  
predict = mbs.predict(train_set.tfidf_matrix)  
print("predicted result on train set:", predict)  
  
with open("UserManual_UQLib.html", 'rb') as f:  
    text_test = get_content_in_html(f.read())  
    list_test = [text_test]  
    tfidf_mat_test = vectorizer.transform(list_test)  
    print("predicted result on test set:", mbs.predict(tfidf_mat_test))
```

但是需要注意的是, 在上面的测试中, 文本是一个单分类数据集问题。而对于 RCV1 自然语言处理数据集, 则涉及到多分类问题, <mark style="background: transparent; color: red">对应的分类范畴是 103 宽的稀疏矩阵, 并且一个文本可能属于多个类别</mark>。
另外, max_df 和 min_df 是构建使用的 document frequency 部分;设置不同的门槛值, 可以

`````ad-note
title: 多标签分类示例
collapse: close
多标签分类需要导入  `from sklearn.multioutput import MultiOutputClassifier`, 

```python fold
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import accuracy_score, classification_report

# 示例数据集
data = {
    'text': [
        'This is a news article about politics.',
        'Sports events are exciting to watch.',
        'Technology is evolving rapidly.',
        'Health and wellness are important topics.',
        'Economics and finance news.'
    ],
    'labels': [
        [1, 0, 0, 0, 0],  # Politics
        [0, 1, 0, 0, 0],  # Sports
        [0, 0, 1, 0, 0],  # Technology
        [0, 0, 0, 1, 0],  # Health
        [0, 0, 0, 0, 1]   # Economics
    ]
}

# 转换为 DataFrame
df = pd.DataFrame(data)

# 特征和标签
X = df['text']
y = np.array(df['labels'].tolist())

# 文本向量化
vectorizer = TfidfVectorizer()
X_vect = vectorizer.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.2, random_state=42)

# 多标签分类器
model = MultiOutputClassifier(MultinomialNB())
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 评估
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
```

`````

> [!CAUTION] scipy 中的 toarray 方法
> 需要注意的是, scipy.spare.toarray() 和 numpy.array 并不相通, 稀疏矩阵可以通过 toarray() 方法进行转换为标准的密集 numpy 数组, 但 numpy.array 不会将其进行转换。

利用完全类似的方法进行 RCV1 数据集分类(多分类文本数据集), 得到的精度在 0.6 左右, 代码如下:
```python fold title:RCV1数据集的多项式朴素贝叶斯预测(精度0.6左右)
import numpy as np  
from sklearn.datasets import fetch_rcv1  
from sklearn.datasets._base import Bunch  
from bs4 import BeautifulSoup  
from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer  
from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB  
from sklearn.linear_model import SGDClassifier  
from sklearn.multioutput import MultiOutputClassifier  
from sklearn.metrics import accuracy_score, classification_report, jaccard_score  
  
classifier = MultiOutputClassifier(MultinomialNB(alpha=0.001)) #  ComplementNB(alpha=0.001)  
  
class Load_RCV1:  
    def __init__(self):  
        rcv1_train_data = fetch_rcv1(subset="train",download_if_missing=True, shuffle=False)  
        rcv1_test_data = fetch_rcv1(subset="test",download_if_missing=True, shuffle=False)  
  
        # train dataset fetching and initialization -> Bunch object  
        self.data = rcv1_train_data.data                   # tf-idf matrix  
        self.target = rcv1_train_data.target.toarray()     # multi-target settings  
        self.target_names = rcv1_train_data.target_names   # 103 categories  
        # self.sample_id    = rcv1_test_data.sample_id  
        # validation dataset fetching        self.valid_data = rcv1_test_data.data  
        self.valid_target = rcv1_test_data.target.toarray()  # multi-target target (must change to array)  
        self.valid_target_names = rcv1_test_data.target_names  
  
        print(rcv1_train_data.DESCR)  
        print("train data shape: ", self.data.shape,  rcv1_train_data.keys())  
        print("test data shape: ", self.valid_data.shape, rcv1_test_data.keys())  
        print("total sample counts : ", self.data.shape[0] + self.valid_data.shape[0])  
        # total 804414 data -> rcv1-V2  
  
    def run(self):  
        vectorizer = TfidfVectorizer(max_df = 0.5, min_df= 0)  
        # train data (note : the "data" has alerday been converted to tf-idf matrix)  
        train_set = Bunch(data = self.data, target = self.target, target_names = self.target_names)  
  
        classifier.fit(train_set.data, train_set.target)       # train the text classification model  
        self.y_pred_train = classifier.predict(self.data)    # predict the result  
        self.y_pred_valid  = classifier.predict(self.valid_data)  # data for valid  
  
        print("prediction on train data", self.y_pred_train)  
        print("accuracy score of prediction on train data", accuracy_score(self.target,self.y_pred_train))  
        print("prediction on test data", self.y_pred_valid)  
        print("accuracy score of prediction on test data", accuracy_score(self.valid_target, self.y_pred_valid))  
  
if  __name__ == "__main__":  
    a = Load_RCV1()  
    a.run()
```

机器学习的文本分类结果评估方案: 
首先称预测对的为 "检索出相关文档", 而检索出错误的为"检索出不相关"
(1) 召回率(recall rate, R): **检索出相关文档数和文档库找到的所有相关文档数的比例。实际上是检索系统的查全率**
(2) 准确率(Precision, P) : 即精度, **检索出相关文档数和检索出的文档的总数的比例, 衡量检索系统的准确率**
![[Excalidraw/1.机器学习算法基本内容 2024-09-11 13.11.50|600]]
为了在召回率和 准确率之间找到均衡， 我们采用如下标准衡量分类结果的准确性:
$$\Large\boxed{F_{\beta} = \frac{ (\beta^{2} + 1) PR}{ \beta^{2}P  + R} } $$
其中取 $\beta$ = 1 得到常用的 F1 measure 分类结果评估标准
$$\Large\boxed{F_{1} = \frac{2PR}{ P + R}}$$
对应的 F1 measure 以及上述的 Precision 和 recall 准确率可以直接采用 sklearn 中的metrics 获得
```python 
from sklearn.metrics import f1_score, precision score, recall_score 
recall_score(y_true, y_pred); precision_score(y_true, y_pred); f1_score(y_true, y_pred)
```

对于 precision_score, 要求输入是两个二值向量。
例如 `[1,0,1,1]` , 实际为 `[1,1,0,1]` 则检索到的为 0,2,3, A = (0,3), 则 P = R = 0.666

### (3) KNN 算法
具体参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/4. KNN算法和KMeans聚类相关算法|4. KNN算法和KMeans聚类相关算法]], 实际上只需建立一个词频向量或者tf-idf 向量, 进行 KNN 判断即可。使用方法如下:
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics import accuracy_score

knn= KNeighborsClassifier(metric='cosine',n_neighbors=2)
labels = [0, 1, 0, 1, 0, 2]
texts = [ "这是 一篇 关于 机器学习 的 文章", "这 篇 文章 讨论 了 深度学习 的 发展", "机器学习 在 医疗领域 的 应用", "深度学习 是 人工智能的一部分", "这篇文章 讲述 了 机器学习 技术 的 发展", "人工智能 在 未来 的 应用前景" ]
text_test = ["机器学习算法原理与编程实践"]

vectorizer= TfidfVectorizer()
X = vectorizer.fit_transform(texts, labels)
knn.fit(X,y=labels)
knn.predict(vectorizer.transform(text_test))
result = knn.predict(X)
print(result, accuracy_score(labels, result))
# 结果 : [0 1 0 1 0 1] 0.8333333333333334 
```

其中我们定义距离 metric = 'cosine'，具体可以参考 [scipy.spatial](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)部分。

另外, 在 `sklearn.neighbors` 中也包含了 KDTree, BallTree 等内容, 实际上使用 NearestNeighbors 即可。 
具体可以参考[scikit-learn-neighbors](https://scikit-learn.org/stable/modules/neighbors.html#)  
```python
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KDTree, BallTree
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
distances, indices = nbrs.kneighbors(X)
print(indices,distances)
print(nbrs.kneighbors_graph(X).toarray()) 
```
注意这个设置了 n_Neighbors = 2， 仅获取2个最邻近的点参数。

