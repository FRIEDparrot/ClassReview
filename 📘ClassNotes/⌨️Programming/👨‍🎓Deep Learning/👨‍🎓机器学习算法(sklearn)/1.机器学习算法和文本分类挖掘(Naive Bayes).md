## ä¸€ã€æœºå™¨å­¦ä¹ åŸºæœ¬å†…å®¹
ç¥ç»ç½‘ç»œçš„ä¸»çº¿åŒ…å«: 
1. åˆ†ç±», å›å½’å’Œé¢„æµ‹ 
2. è´å¶æ–¯ç†è®ºå’Œæ™ºèƒ½æ¨ç† 
3. ç®—æ³•éƒ¨åˆ†

```python
a = [1,2,3,4]
np.mat(a)
```
é€šè¿‡ numpy åº“å¯ä»¥å®ç°çŸ¢é‡åŒ–ç¼–ç¨‹, é€šè¿‡ GPU è¿›è¡Œå¹¶è¡Œè®¡ç®—å’Œå¤§è§„æ¨¡æµ®ç‚¹è¿ç®—, å¯ä»¥æä¾›è¾ƒé«˜çš„å¤„ç†æ€§èƒ½ã€‚

### (1) èŒƒæ•°, æ¬§å¼è·ç¦», åˆ‡æ¯”é›ªå¤«è·ç¦»å’Œæ±‰æ˜è·ç¦»

> [!caution] èŒƒæ•°çš„å®šä¹‰
> å¯¹äºä¸€èˆ¬çš„èŒƒæ•°, Lp èŒƒæ•°å®šä¹‰ä¸ºå„ä¸ªå…ƒç´ çš„pæ¬¡æ–¹çš„å’Œçš„ 1/p æ¬¡æ–¹, è€Œè·ç¦»ä¸€èˆ¬å®šä¹‰ä¸º:
> $$(\sum^{n}_{i=1} |p_{i} - q_{i}|^{k})^{\frac{1}{k}}$$

ä¸€èˆ¬åœ°, æˆ‘ä»¬å°†ä¸¤ä¸ªå‘é‡çš„ç‰¹å¾é‡‡ç”¨æµ®ç‚¹æ•°è¿›è¡Œè¡¨ç¤º,åˆ™å¯ä»¥é‡‡ç”¨è·ç¦»(èŒƒæ•°)æ¥è¯´æ˜æŸä¸ªæ•°æ®, è€Œä¸€èˆ¬åœ°, è·ç¦»è¶Šè¿‘çš„å‘é‡, è¶Šå®¹æ˜“å½’ä¸ºä¸€ç±»ã€‚

å¯¹äº**ä¸¤ç‚¹çš„æ¬§æ°è·ç¦»**,å¯ä»¥é‡‡ç”¨ä¸‹å¼è¿›è¡Œè®¡ç®—:
$$d_{12}  =  \sqrt{(A - B) (A- B)^{T}}$$
è€Œ**åˆ‡æ¯”é›ªå¤«è·ç¦»**ä¸º $\max(|x2 - x1|, |y2 - y1|)$, å¯ä»¥é‡‡ç”¨
$$\lim_{k \rightarrow \infty} (\sum^{n}_{i=1} |x_{1i} - x_{2i}|^{k})^{\frac{1}{k}}$$
å…¶äºŒ, æˆ‘ä»¬å¸¸å¸¸é€šè¿‡æ–¹å‘ä½™å¼¦è¡¡é‡æ ·æœ¬é‡ä¹‹é—´çš„å·®å¼‚, å‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/ğŸ“ˆAdvanced Mathematics/ç¬¬å…«ç«  å‘é‡ä»£æ•°å’Œç©ºé—´è§£æå‡ ä½•|ç¬¬å…«ç«  å‘é‡ä»£æ•°å’Œç©ºé—´è§£æå‡ ä½•]], æœ‰
$$\cos \theta = \frac{\vec{a}\cdot\vec{b}}{|a||b|}$$

> [!NOTE] æ±‰æ˜è·ç¦»
> **æ±‰æ˜è·ç¦»**å®šä¹‰ä¸ºä¸¤ä¸ªç­‰é•¿å­—ç¬¦ä¸²ä¹‹é—´çš„<mark style="background: transparent; color: red">æ±‰æ˜è·ç¦»å®šä¹‰ä¸º</mark>**å°†å…¶ä¸­ä¸€ä¸ªå˜ä¸ºå¦å¤–ä¸€ä¸ªçš„æœ€å°äº¤æ¢æ¬¡æ•°**:
> ä¸€èˆ¬å¯ä»¥é‡‡ç”¨å¦‚ä¸‹æ–¹æ³•è·å–:
> ```python
>  s1 = np.mat([1,0,1,0,0,1,1])
>  s2 = np.mat([0,1,0,1,1,1,0])
>  len(np.nonzero(s1 - s2)[1])
> ```
> æ±‰æ˜è·ç¦»å¸¸ç”¨äºä¿¡æ¯ç¼–ç , ä¸€èˆ¬ä¼šä½¿ç¼–ç ä¹‹é—´çš„æ±‰æ˜è·ç¦»å°½å¯èƒ½å¤§ä»¥å¢åŠ å®¹é”™ç‡

**æ°å¡å¾·ç›¸ä¼¼ç³»æ•°**:å¯¹äºä¸¤ä¸ªé›†åˆ A, B ä¸­çš„äº¤é›†å…ƒç´ åœ¨ A, B å¹¶é›†ä¸­å çš„æ¯”ä¾‹ä¸ºæ°å¡å¾·ç›¸ä¼¼ç³»æ•°ã€‚ç”¨äºè¡¡é‡ä¸¤ä¸ªé›†åˆçš„ç›¸ä¼¼åº¦
$$J(A,B) =  \frac{|A \bigcap B|}{|A\bigcup B |}$$
**æ°å¡å¾·(Jaccard)è·ç¦»**: å®šä¹‰ä¸º 
$$J_{\delta} (A, B) = 1 - J(A, B) = \frac{|A \bigcup  B | + |A \bigcap B |}{|A \bigcup B |}$$
> [!hint] å¸¸ç”¨çš„è·ç¦»
> æ¬§å¼è·ç¦»ï¼Œ ç›¸å…³ç³»ç»Ÿï¼Œ Jaccard è·ç¦»å’Œä½™å¼¦ç›¸ä¼¼åº¦ç­‰

### (2) å¤šå…ƒç»Ÿè®¡åŸºç¡€
> [!NOTE] å¸¸è§çš„å¤šå…ƒç»Ÿè®¡ç®—æ³•
> åŒ…æ‹¬æœ´ç´ è´å¶æ–¯åˆ†æ,å›å½’åˆ†æ,ç»Ÿè®¡å­¦ä¹ åŸºç¡€, èšç±»åˆ†æ, ä¸»æˆåˆ†åˆ†æ(PCA), æ¦‚ç‡å›¾æ¨¡å‹ç­‰ç­‰ã€‚

æœ€å¸¸ç”¨çš„è´å¶æ–¯å…¬å¼è¡¨è¾¾äº†æ¡ä»¶æ¦‚ç‡çš„å…³ç³»:
$$P(B|A) =  \frac{P (A |B) P(B)}{P(A)}$$
ä»æ¦‚ç‡ç»Ÿè®¡çš„è§’åº¦, ä¸€ä¸ªå¯¹è±¡å¯ä»¥è¡¨ç¤ºä¸ºnä¸ªéšæœºå˜é‡çš„æ•´ä½“, å…¶ä¸­ $X = (X_1,X_2, \dots X_n)$ ä¸º n ç»´éšæœºå˜é‡æˆ–è€…éšæœºå‘é‡ã€‚**æ¯ä¸ªå¯¹è±¡æ˜¯éšæœºå‘é‡ä¸­çš„ä¸€ç»„å–å€¼, è€ŒçŸ©é˜µä¸­çš„æ‰€æœ‰å¯¹è±¡æ„æˆäº†éšæœºå‘é‡çš„è”åˆå’Œè¾¹ç¼˜å¯†åº¦æ¦‚ç‡åˆ†å¸ƒ**ã€‚
ä¾‹å¦‚, å¯¹äº10ä¸ªè‹¹æœ, å…¶ä¸­çº¢çš„8ä¸ª, é»„çš„2ä¸ª; è€Œæ¢¨æœ‰é»„è‰²å’Œç»¿è‰², å…¶ä¸­é»„è‰²ä¸º9ä¸ªã€‚æ­¤æ—¶, å¯ä»¥æ„å»ºå‡ºå¦‚ä¸‹çš„æ¦‚ç‡è¡¨, å¯¹åº”çš„**è”åˆæ¦‚ç‡å¯†åº¦åˆ†å¸ƒ**å’Œ**è¾¹ç¼˜æ¦‚ç‡å¯†åº¦åˆ†å¸ƒ**å¦‚å›¾:
![[Excalidraw/1.æœºå™¨å­¦ä¹ ç®—æ³•åŸºæœ¬å†…å®¹ 2024-09-05 11.09.17|650]]
æ‰€æœ‰ä»¥æ¦‚ç‡ç›¸å…³çš„ç®—æ³•**å‡ä»¥å¯¹è±¡çš„è”åˆæ¦‚ç‡å¯†åº¦åˆ†å¸ƒå’Œè¾¹ç¼˜æ¦‚ç‡å¯†åº¦åˆ†å¸ƒä¸ºè¿ç®—åŸºç¡€**ã€‚

ç›¸å…³ç³»æ•°å®šä¹‰å‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/ğŸ£Probability Theory/ç¬¬å››ç«  éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾#ä¸‰ã€åæ–¹å·®åŠå…¶ç›¸å…³ç³»æ•°|éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾]], å®šä¹‰ä¸º(èŒƒå›´ä¸º -1 ~ 1):
$$\boxed{\rho_{XY} = \frac{E\{[X - E(X)][Y - E(Y)]\}}{\sqrt{D(X)} \sqrt{D(Y)}}}$$
è€Œ**ç›¸å…³è·ç¦»**å®šä¹‰ä¸º1ä¸è¯¥ç³»æ•°çš„å·®å€¼: 
$$D_{XY} = 1 - \rho_{XY}$$
æ ¹æ®éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾, æˆ‘ä»¬å¯ä»¥å°†æ¬§å¼è·ç¦»å‘å±•ä¸ºé©¬æ°è·ç¦»å…¬å¼:

**é©¬æ°è·ç¦»**:
é©¬æ°è·ç¦»(Mahalanobis Distance),å®šä¹‰ä¸ºå¯¹äºMä¸ªæ ·æœ¬å‘é‡ $X_1 \sim X_{m}$, å…¶åæ–¹å·®çŸ©é˜µè®°ä¸º$S$, å‡å€¼ä¸º $\mu$, åˆ™æ ·æœ¬å‘é‡åˆ°å‡å€¼ $\mu$ çš„é©¬æ°è·ç¦»å®šä¹‰ä¸º: 
$$D(x) = \sqrt{(\boldsymbol{x} - \mu)^{T} S^{-1} (\boldsymbol{x} - \mu)}$$
å…¶ä¸­, å¯¹äºä¸¤ä¸ªå‘é‡ $X_{i} ,X_{j}$, åˆ™å…¶é©¬æ°è·ç¦»å®šä¹‰ä¸º:
$$\Large\boxed{D(X_{i}, X_{j}) =  \sqrt{(X_{i} - X_{j})^{T }  S^{-1} (X_{i} - X_{j})}}$$
å…¶ä¸­å½“åæ–¹å·®ä¸ºå¯¹è§’çŸ©é˜µæ—¶, è½¬æ¢ä¸ºæ¬§æ°è·ç¦»ã€‚å…¶<mark style="background: transparent; color: red">ä¼˜ç‚¹æ˜¯é‡çº²æ— å…³, æ’é™¤å˜é‡ä¹‹é—´çš„ç›¸äº’å¹²æ‰°</mark>ã€‚

è®¡ç®—ä»£ç å¦‚ä¸‹, æ¯”è¾ƒç®€å•:
```python
import numpy as np
import numpy.linalg as la  

def mahalanobis_distance(x1 : np.matrix, x2:np.matrix, S):  
    """  
    Calculates the Mahalanobis distance between two vectors x1 and x2 given the covariance matrix S.  
    Parameters:    x1 (numpy.ndarray): The first vector.    x2 (numpy.ndarray): The second vector.    S (numpy.ndarray): The covariance matrix.    Returns:    float: The Mahalanobis distance between x1 and x2.    """    diff = x1 - x2  
    D = np.sqrt(diff.T * la.inv(S) * diff)  
    return D  
  
if __name__ == "__main__" :  
    x1 = np.mat([1, 2, 3])  
    x2 = np.mat([4, 5, 6])  
    S  = np.mat([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  
    print(mahalanobis_distance(x1.T, x2.T, S))
```

### (3) çŸ©é˜µçš„ç©ºé—´å˜æ¢
ç©ºé—´çš„å˜æ¢ç‰¹æ€§æ˜¯çŸ©é˜µæœ€é‡è¦çš„ç‰¹æ€§ä¹‹ä¸€, <mark style="background: transparent; color: red">ç”±ç‰¹å¾åˆ—çš„å–å€¼èŒƒå›´æ‰€æœ‰æ„æˆçš„çŸ©é˜µç©ºé—´åº”å½“å…·å®Œæ•´æ€§ï¼Œ å¹¶èƒ½å¤Ÿåäº‹ç‰©çš„ç©ºé—´å½¢å¼æˆ–è€…å˜åŒ–è§„å¾‹</mark>ã€‚
ä¸€èˆ¬è€Œè¨€, è®­ç»ƒé›†æ‰€æ„æˆçš„çŸ©é˜µå¸¸å¸¸åŒ…å«å¤§é‡çš„æ ·æœ¬, è¦æ±‚: 
1. å‘é‡å’ŒçŸ©é˜µçš„ç›¸å…³è¿ç®—æ˜¯å…·æœ‰æ„ä¹‰çš„è¿ç®—, å¹¶ä¸”èƒ½å¤Ÿè§£é‡Šå‡ºäº‹ç‰©çš„ç©ºé—´å½¢å¼ï¼Œ 
2. è®¡ç®—ç»“æœåº”å½“èƒ½å¤Ÿåæ˜ å…¶æœ¬è´¨ç‰¹å¾, ä»è€Œæ˜ å°„å‡ºå¯¹è±¡é›†åˆè¡¨è¾¾çš„å½¢å¼å’Œè§„å¾‹ã€‚
nç»´æ­£äº¤ç©ºé—´å³ä¸º n ä¸ªå½¼æ­¤æ­£äº¤çš„åŸºåœ°å‘é‡æ„æˆçš„ç©ºé—´

åœ¨åŸºåº•è¿›è¡Œä¸€å®šçš„å˜æ¢æ—¶, ç›¸åº”çš„æ‰€å¾—çš„åæ ‡ä¹Ÿä¼šå‘ç”Ÿç›¸åº”çš„å˜æ¢, ä¾‹å¦‚åœ¨ç›´è§’åæ ‡ç³»ä¸­çš„ $\alpha = (1,2)$, è®¡ç®—æ–°åæ ‡ç³»i'=(3,1), j'=(1,3)ä¸‹çš„åæ ‡, å®¹æ˜“å¾—å‡ºæ˜¯ i' * 1 + j' * 2 = (5,7):

åˆ™ä»(1,2) åˆ° (5,7) çš„è¿‡ç¨‹, åˆ™<mark style="background: transparent; color: red">å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå˜æ¢çŸ©é˜µ</mark>, ç›´æ¥å®Œæˆå˜æ¢è¿‡ç¨‹ :
```python
A = [[3,1], [1,3]]
i' =  A * i 
j' = A * j
```
$$\alpha' = A * \alpha = [\alpha_{1}, \alpha_{2}] * \alpha  =  (5,7)$$
å…¶ä¸­ `A = [[3,1], [1,3]]` ä¸ºå˜æ¢çŸ©é˜µ, å°†åæ ‡ä» i, j å˜æ¢åˆ° $[\alpha_1, \alpha_2]$ æ„æˆçš„æ–°åæ ‡ç³»ã€‚

éœ€è¦è¯´æ˜çš„æ˜¯, å‘é‡çš„å˜æ¢å¯ä»¥åˆ†è§£ä¸ºä¼¸ç¼©å’Œæ—‹è½¬, ç±»ä¼¼å¤æ•°è¿ç®—, è®¾ $\alpha = [r \cos \theta, r \sin \theta]$, åˆ™
$$  \lambda  e^{i\theta} * \alpha =  \lambda  *  \left[\begin{matrix}
\cos \Delta \theta  &  - \sin \Delta  \theta  \\ 
\sin \Delta  \theta  &  \cos \Delta  \theta  
\end{matrix}\right]  *  \left[\begin{matrix}
r \cos \theta  \\ r \sin \theta
\end{matrix}\right]$$
å¦å¤–, æ ¹æ®çŸ©é˜µçš„ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼$\lambda$, æˆ‘ä»¬å¯ä»¥è¿˜åŸåˆå§‹çš„çŸ©é˜µ:
$$A = P  \Lambda P^{-1}$$
å…¶ä¸­ P ä¸ºç‰¹å¾å‘é‡æ„æˆçš„çŸ©é˜µ, $\Lambda$ ä¸ºç‰¹å¾å€¼æ’æˆçš„å¯¹è§’çŸ©é˜µ; ç‰¹å¾å€¼ ($Av = \lambda v$)
```python 
import numpy as np  
import numpy.linalg as la  
  
A = np.mat([[8,1,6],[3,5,7],[4,9,2]], dtype=np.float64)  
lmbda, v = la.eig(A)  
  
Sigma = np.diag(lmbda)  
B = v * Sigma * la.inv(v)  
print(B)
```

é’ˆå¯¹æ•°æ®çš„å½’ä¸€åŒ–ä¸€èˆ¬çš„æ–¹æ³•æ˜¯é€šè¿‡æ¯”ä¾‹ç¼©æ”¾, å¯¹ç‰¹å¾è¿›è¡Œè§„èŒƒåŒ–å¤„ç†, æ˜ å°„åˆ°æŸä¸ªåŒºé—´ã€‚
$$X^{*} =  \frac{x - \mu}{\sigma}$$
## äºŒã€æ–‡æœ¬æŒ–æ˜æ–¹æ³•
ä¸€èˆ¬çš„åšæ³•æ˜¯ä»å¤§é‡æ–‡æœ¬æ•°æ®ä¸­æŠ½å–äº‹å…ˆæœªçŸ¥çš„, å¯ç†è§£çš„, å¯ç”¨çš„çŸ¥è¯†çš„è¿‡ç¨‹ã€‚ä¸»è¦åŒ…å«: 
1. æœç´¢å’Œä¿¡æ¯æ£€ç´¢
2. æ–‡æœ¬èšç±», æ–‡æœ¬åˆ†ç±»,æ–‡æœ¬æŒ–æ˜, ä¿¡æ¯æŠ½å–(IE), è‡ªç„¶è¯­è¨€å¤„ç†(NLP), æ¦‚å¿µæå–

ç›®å‰çš„æ–‡æœ¬åˆ†ç±»ç®—æ³•ä¸»æµåŒ…å«: 
1. åŸºäºæ¨¡å¼ç³»ç»Ÿ
2. åŸºäºåˆ†ç±»æ¨¡å‹ (æœºå™¨å­¦ä¹ , ä¸€èˆ¬éœ€è¦ä¸€ç»„é¢„åˆ†ç±»å’Œè®­ç»ƒæ¥å»ºç«‹åˆ†ç±») 

<mark style="background: transparent; color: red">å®Œå…¨è§£å†³ä¸­æ–‡åˆ†è¯ç®—æ³•çš„æ˜¯åŸºäºæ¦‚ç‡å›¾æ¨¡å‹çš„æ¡ä»¶æ¦‚ç‡éšæœºåœº(CRF)æ–¹æ¡ˆ, </mark> 

### (1) å‘é‡ç©ºé—´æ¨¡å‹
å‘é‡ç©ºé—´æ¨¡å‹ä¸­, ä¸€èˆ¬å°†æ–‡æœ¬ä½œä¸ºå‘é‡è¿›è¡Œå­˜å‚¨, å‘é‡ä¸­æ¯ä¸ªç‰¹å¾è¡¨ç¤ºæ–‡æœ¬ä¸­å‡ºç°çš„è¯è¯­ã€‚é€šå¸¸å°†è®­ç»ƒé›†ä¸­çš„ä¸åŒå­—ç¬¦ä¸²å‡ä½œä¸ºä¸€ä¸ªç»´åº¦, è™½ç„¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é«˜ç»´ç©ºé—´çš„æ–‡æœ¬, ä½†æ˜¯å¯èƒ½å¯¼è‡´è¾ƒé«˜ç»´çš„å­˜ç©ºé—´ã€‚

é¿å…ç»´åº¦ç¾éš¾çš„æ–¹æ³•ä¸€æ˜¯å­˜å‚¨æŸäº›åœç”¨è¯å¹¶è¿›è¡Œè¿‡æ»¤ã€‚é€šè¿‡è¾“å…¥çš„åœç”¨è¯è¡¨ã€‚

### (2) æƒé‡ç­–ç•¥å’Œ TF-IDF æ–¹æ³•
ä¾æ®[å‚è€ƒèµ„æ–™](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) term-frequency - inverse documentation frequency. åœ¨tf-idfæ–¹æ³•ä¸­, é¦–å…ˆéœ€ç»Ÿè®¡æ–‡æ¡£ä¸­, å„ä¸ªè¯çš„è¯é¢‘ä¿¡æ¯ã€‚è€Œ tf-idf æ–¹æ³•çš„è¯é¢‘ç»Ÿè®¡è§„åˆ™å¦‚ä¸‹: 
1. <mark style="background: transparent; color: red">å¦‚æœä¸€ä¸ªè¯æˆ–è€…çŸ­è¯­åœ¨ä¸€ç¯‡æ–‡ç« ä¸­å‡ºç°é¢‘ç‡é«˜, è€Œå…¶ä»–å¾ˆå°‘å‡ºç°, åˆ™è®¤ä¸ºè¯¥è¯é€‚åˆç”¨äºæ–‡ç« åˆ†ç±»</mark> 
2. <mark style="background: transparent; color: red">é«˜é¢‘ç‡è¯åº”è¯¥å…·æœ‰æœ€é«˜æƒé‡, é™¤éå…¶ä¹Ÿæ˜¯æœ€é«˜æ–‡æ¡£é¢‘ç‡</mark> 

è®¾æ‰€æœ‰çš„æ–‡æœ¬ç”± $i = 1,2, \dots n$ ä¸ªæ–‡ä»¶æ„æˆ, è€Œå…¶ä¸­æœ‰j = 1,2, ...m ä¸ªè¯è¯­, åˆ™æŸä¸ªè¯ i åœ¨æŸä¸ªæ–‡ä»¶ j çš„è¯é¢‘ $TF_{ij}$ å¯ä»¥è®¡ç®—ä¸º: 
$$TF_{ij} = \frac{n_{i,j}}{\sum_{k=1}  n_{k,j}}$$
å¦å¤–å¼•å…¥å¢å¼ºé¢‘ç‡(ç”¨äºæ”¾æ­¢æ–‡ç« è¿‡é•¿)
$$\text{tf}(t, D) = 0.5 + 0.5  \frac{f_{t,d}}{\max \{ f_{t', d}: t'\in  D\}}$$
<mark style="background: transparent; color: red">å…¶ä¸­éœ€è¦é™¤ä»¥åˆ†æ¯(æ–‡æ¡£jä¸­æ‰€æœ‰çš„è¯çš„å‡ºç°æ¬¡æ•°ä¹‹å’Œ,åŒ…æ‹¬å…¶ä»–æ‰€æœ‰ç±»å‹è¯æ±‡)</mark>,é€†æ–‡ä»¶é¢‘ç‡ç”¨äºè¡¡é‡ä¸€ä¸ªè¯æä¾›ä¿¡æ¯çš„å¤šå°‘ã€‚
$$IDF_{i}  = \log  \frac{|D|}{|\{ j : t_{i} \in d_{j}\}|}$$
N : æ–‡ä»¶æ€»æ•°
j : å‡ºç°æŸä¸ªè¯çš„æ–‡ç« æ€»æ•°
ä¸€èˆ¬åœ°, IDF è¶Šå°, åˆ™è¿™ä¸ªè¯æ±‡æ›´å¤šå¯èƒ½æ˜¯å¸¸ç”¨è¯æ±‡ï¼Œå› æ­¤æä¾›ä¿¡æ¯é‡å°±å°; æ­¤æ—¶å³å¯è®¡ç®—: 
$$TF-IDF(i,j,D) = tf_{ij}  \times  idf (i, D)$$
å¯¹åº”çš„, TF-IDF è¾ƒé«˜çš„è¯è¯­ï¼Œä¹Ÿæ˜¯åˆ†ç±»ç‰¹å¾è¾ƒå¼ºçš„è¯è¯­ã€‚
é‡‡ç”¨ sklearn çš„ rcv1 æ–°é—»æ–‡ç« æ•°æ®é›†ä½œä¸ºæ–‡æœ¬åˆ†è¯ä¾æ®, [å‚è€ƒèµ„æ–™](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_rcv1.html#sklearn.datasets.fetch_rcv1) 
```python
from  sklearn.datasets import fetch_rcv1
rcv1_data = fetch_rcv1( download_if_missing=True, shuffle=False)
```
æ–‡ä»¶è®­ç»ƒé›†è·¯å¾„ä¸º `C:\Users\Parrot\scikit_learn_data\RCV1` 

TheÂ **RCV1**Â dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.

RCV1 åŒ…å« rcv1-v1 å’Œ rcv1-v2, å…¶ä¸­ RCV1-v2 åšäº†ä¸€å®šä¿®æ”¹, ç”¨äºè·å–æ›´å¥½çš„æµ‹è¯•é›†åˆã€‚
![[attachments/Pasted image 20240910101400.png|600]]
åœ¨ RCV1 ä¸­, ä¸»ä½“ç ä¸»è¦åˆ†ä¸ºå››ç­‰çº§(CCAT, ECAT, GCAT, MCAT) 
![[attachments/Pasted image 20240910105001.png]]
Each story was required to have <mark style="background: transparent; color: red">at least one Topic code and one Region code.</mark> 

å¯¹äº rcv1 æ•°æ®é›†, è®­ç»ƒé›†å«æœ‰ 23149 æ ·æœ¬, å…±æœ‰ 47236 è¯æ±‡, å¯åˆ†ç±» 126 ç§ä¸»é¢˜è¯ã€‚ä½†æ˜¯æœ‰çš„éƒ¨åˆ† RCV1 åœ¨åˆ†ç±»æ—¶ä¸å®é™…ä½¿ç”¨, å› æ­¤å®é™…ä¸Šæ˜¯ 103 åˆ†ç±»é—®é¢˜ã€‚ä¸»è¦æœ‰ C, E, G, M å››ä¸ªå¤§ç±»ã€‚å…·ä½“ä» target_names ä¸­å¯ä»¥æ‰¾åˆ°:
```python
array(['C11', 'C12', 'C13', 'C14', 'C15', 'C151', 'C1511', 'C152', 'C16',
       'C17', 'C171', 'C172', 'C173', 'C174', 'C18', 'C181', 'C182',
       'C183', 'C21', 'C22', 'C23', 'C24', 'C31', 'C311', 'C312', 'C313',
       'C32', 'C33', 'C331', 'C34', 'C41', 'C411', 'C42', 'CCAT', 'E11',
       'E12', 'E121', 'E13', 'E131', 'E132', 'E14', 'E141', 'E142',
       'E143', 'E21', 'E211', 'E212', 'E31', 'E311', 'E312', 'E313',
       'E41', 'E411', 'E51', 'E511', 'E512', 'E513', 'E61', 'E71', 'ECAT',
       'G15', 'G151', 'G152', 'G153', 'G154', 'G155', 'G156', 'G157',
       'G158', 'G159', 'GCAT', 'GCRIM', 'GDEF', 'GDIP', 'GDIS', 'GENT',
       'GENV', 'GFAS', 'GHEA', 'GJOB', 'GMIL', 'GOBIT', 'GODD', 'GPOL',
       'GPRO', 'GREL', 'GSCI', 'GSPO', 'GTOUR', 'GVIO', 'GVOTE', 'GWEA',
       'GWELF', 'M11', 'M12', 'M13', 'M131', 'M132', 'M14', 'M141',
       'M142', 'M143', 'MCAT'], dtype=object)
```

ç‰¹å¾ä»£ç (character codes)æ˜¯ä» I00000-I90000éƒ¨åˆ†çš„, ä½†æ˜¯éœ€è¦æ³¨æ„: æœ‰éƒ¨åˆ†ä¸åŒç¼–ç ä½†ç±»åˆ«ç›¸åŒçš„éƒ¨åˆ†, ä¾‹å¦‚ I22400 å’Œ I22470 éƒ¨åˆ†ã€‚å‡ä¸º NON-FERROUS METALS.  æ­¤å¤–, I65000 æ²¡æœ‰å¯¹åº”çš„æ–‡ç« , 

ä¸€èˆ¬çš„ä»£ç éƒ¨åˆ†å‡ä¸º 6-8 ä½(æœ«å°¾åŠ 0è¡¨ç¤ºpadded). 

```python 
import re  
import os  
import sklearn  
from sklearn import feature_extraction  
from sklearn.feature_extraction.text import TfidfVectorizer      # tf-idf å‘é‡ç”Ÿæˆç±» 
from sklearn.feature_extraction.text import TfidfTransformer  # tf-idf å‘é‡è½¬æ¢ç±»
```

å¯¹äº tfidf_vectorizer, ä¸»è¦çš„ç”¨æ³•å¦‚ä¸‹: 
```python
word_list = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?"
]

vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)
tfidf_matrix = vectorizer.fit_transform(word_list)
```

å› æ­¤åªéœ€è¦å¯¹åº”æ„é€ å‡ºç›¸åº”çš„æ•°æ®é›†ï¼Œå³å¯å¾—åˆ°å…¶ä¸­çš„tf-idfçŸ©é˜µ, 
```python
import re  
import os  
import sklearn  
from sklearn import feature_extraction  
from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import TfidfVectorizer  
from lxml import html, etree  
from bs4 import BeautifulSoup

def get_content_in_html(html_content):  
    text_content = BeautifulSoup(html_content, 'html.parser').get_text()  
    text_content = re.sub(r'[^a-zA-Z0-9\s\-_]', '', text_content)  # eliminate the special characters  
    return text_content  
  
if __name__ ==  "__main__" :  
    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)  
    transformer = TfidfTransformer()  
  
    contents_dictionary = []  
    files = os.listdir("./html")  
  
    for file in files:  
        with open("html/" + file, 'rb') as f:  
            contents = f.read()  
            doc_text = get_content_in_html(contents)  
            contents_dictionary.append(doc_text)  
  
	tfidf_matrix = vectorizer.fit_transform(contents_dictionary)  
	print(tfidf_matrix.shape)  
	print(vectorizer.vocabulary_)
```
å³å¯å¾—åˆ°å¯¹åº”çš„è¯æ±‡çš„ç›¸åº”çš„é¢‘ç‡ã€‚

ä¸€èˆ¬çš„, ç”±äºæˆ‘ä»¬æ˜¯é¢„æµ‹é¢‘ç‡, æ‰€ä»¥é‡‡ç”¨å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æ–¹æ³•è¿›è¡Œé¢„æµ‹ã€‚å…¶ä¸­, å¸¸ç”¨çš„åˆ†ç±»ç®—æ³•åŒ…å« KNN ç®—æ³•, æœ´ç´ è´å¶æ–¯ç®—æ³•å’Œæ”¯æŒå‘é‡æœºç®—æ³•ã€‚ <mark style="background: transparent; color: red">ä¸€èˆ¬å¯¹äºçŸ­æ–‡æœ¬, æœ´ç´ è´å¶æ–¯æ–¹æ³•åˆ†ç±»æ•ˆæœè¾ƒå¥½ï¼Œ ä¸€èˆ¬é‡‡ç”¨å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯å‡å¯ä»¥è¾¾åˆ°å¾ˆé«˜çš„ç²¾åº¦</mark>ã€‚
```python
from sklearn.naive_bayes import MultinomialNB  
from sklearn.datasets._base import Bunch

vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)  
transformer = TfidfTransformer()  
  
train_set = Bunch(tfidf_matrix=None, target_label=None)  
train_set.tfidf_matrix = vectorizer.fit_transform(contents_dictionary)  
train_set.target_value = [0,0,1,1,1,1,1,1] 

# use Multinomial Naive Bayes classifier  
mbs = MultinomialNB(alpha=0.001)   # for smaller alpha value, it  
mbs.fit(train_set.tfidf_matrix, train_set.target_value)  
predict = mbs.predict(train_set.tfidf_matrix)  
print("predicted result on train set:", predict)  
  
with open("UserManual_UQLib.html", 'rb') as f:  
    text_test = get_content_in_html(f.read())  
    list_test = [text_test]  
    tfidf_mat_test = vectorizer.transform(list_test)  
    print("predicted result on test set:", mbs.predict(tfidf_mat_test))
```

ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯, åœ¨ä¸Šé¢çš„æµ‹è¯•ä¸­, æ–‡æœ¬æ˜¯ä¸€ä¸ªå•åˆ†ç±»æ•°æ®é›†é—®é¢˜ã€‚è€Œå¯¹äº RCV1 è‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†, åˆ™æ¶‰åŠåˆ°å¤šåˆ†ç±»é—®é¢˜, <mark style="background: transparent; color: red">å¯¹åº”çš„åˆ†ç±»èŒƒç•´æ˜¯ 103 å®½çš„ç¨€ç–çŸ©é˜µ, å¹¶ä¸”ä¸€ä¸ªæ–‡æœ¬å¯èƒ½å±äºå¤šä¸ªç±»åˆ«</mark>ã€‚
å¦å¤–, max_df å’Œ min_df æ˜¯æ„å»ºä½¿ç”¨çš„ document frequency éƒ¨åˆ†;è®¾ç½®ä¸åŒçš„é—¨æ§›å€¼, å¯ä»¥

`````ad-note
title: å¤šæ ‡ç­¾åˆ†ç±»ç¤ºä¾‹
collapse: close
å¤šæ ‡ç­¾åˆ†ç±»éœ€è¦å¯¼å…¥  `from sklearn.multioutput import MultiOutputClassifier`, 

```python fold
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import accuracy_score, classification_report

# ç¤ºä¾‹æ•°æ®é›†
data = {
    'text': [
        'This is a news article about politics.',
        'Sports events are exciting to watch.',
        'Technology is evolving rapidly.',
        'Health and wellness are important topics.',
        'Economics and finance news.'
    ],
    'labels': [
        [1, 0, 0, 0, 0],  # Politics
        [0, 1, 0, 0, 0],  # Sports
        [0, 0, 1, 0, 0],  # Technology
        [0, 0, 0, 1, 0],  # Health
        [0, 0, 0, 0, 1]   # Economics
    ]
}

# è½¬æ¢ä¸º DataFrame
df = pd.DataFrame(data)

# ç‰¹å¾å’Œæ ‡ç­¾
X = df['text']
y = np.array(df['labels'].tolist())

# æ–‡æœ¬å‘é‡åŒ–
vectorizer = TfidfVectorizer()
X_vect = vectorizer.fit_transform(X)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.2, random_state=42)

# å¤šæ ‡ç­¾åˆ†ç±»å™¨
model = MultiOutputClassifier(MultinomialNB())
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
```

`````

> [!CAUTION] scipy ä¸­çš„ toarray æ–¹æ³•
> éœ€è¦æ³¨æ„çš„æ˜¯, scipy.spare.toarray() å’Œ numpy.array å¹¶ä¸ç›¸é€š, ç¨€ç–çŸ©é˜µå¯ä»¥é€šè¿‡ toarray() æ–¹æ³•è¿›è¡Œè½¬æ¢ä¸ºæ ‡å‡†çš„å¯†é›† numpy æ•°ç»„, ä½† numpy.array ä¸ä¼šå°†å…¶è¿›è¡Œè½¬æ¢ã€‚

åˆ©ç”¨å®Œå…¨ç±»ä¼¼çš„æ–¹æ³•è¿›è¡Œ RCV1 æ•°æ®é›†åˆ†ç±»(å¤šåˆ†ç±»æ–‡æœ¬æ•°æ®é›†), å¾—åˆ°çš„ç²¾åº¦åœ¨ 0.6 å·¦å³, ä»£ç å¦‚ä¸‹:
```python fold title:RCV1æ•°æ®é›†çš„å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯é¢„æµ‹(ç²¾åº¦0.6å·¦å³)
import numpy as np  
from sklearn.datasets import fetch_rcv1  
from sklearn.datasets._base import Bunch  
from bs4 import BeautifulSoup  
from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer  
from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB  
from sklearn.linear_model import SGDClassifier  
from sklearn.multioutput import MultiOutputClassifier  
from sklearn.metrics import accuracy_score, classification_report, jaccard_score  
  
classifier = MultiOutputClassifier(MultinomialNB(alpha=0.001)) #  ComplementNB(alpha=0.001)  
  
class Load_RCV1:  
    def __init__(self):  
        rcv1_train_data = fetch_rcv1(subset="train",download_if_missing=True, shuffle=False)  
        rcv1_test_data = fetch_rcv1(subset="test",download_if_missing=True, shuffle=False)  
  
        # train dataset fetching and initialization -> Bunch object  
        self.data = rcv1_train_data.data                   # tf-idf matrix  
        self.target = rcv1_train_data.target.toarray()     # multi-target settings  
        self.target_names = rcv1_train_data.target_names   # 103 categories  
        # self.sample_id    = rcv1_test_data.sample_id  
        # validation dataset fetching        self.valid_data = rcv1_test_data.data  
        self.valid_target = rcv1_test_data.target.toarray()  # multi-target target (must change to array)  
        self.valid_target_names = rcv1_test_data.target_names  
  
        print(rcv1_train_data.DESCR)  
        print("train data shape: ", self.data.shape,  rcv1_train_data.keys())  
        print("test data shape: ", self.valid_data.shape, rcv1_test_data.keys())  
        print("total sample counts : ", self.data.shape[0] + self.valid_data.shape[0])  
        # total 804414 data -> rcv1-V2  
  
    def run(self):  
        vectorizer = TfidfVectorizer(max_df = 0.5, min_df= 0)  
        # train data (note : the "data" has alerday been converted to tf-idf matrix)  
        train_set = Bunch(data = self.data, target = self.target, target_names = self.target_names)  
  
        classifier.fit(train_set.data, train_set.target)       # train the text classification model  
        self.y_pred_train = classifier.predict(self.data)    # predict the result  
        self.y_pred_valid  = classifier.predict(self.valid_data)  # data for valid  
  
        print("prediction on train data", self.y_pred_train)  
        print("accuracy score of prediction on train data", accuracy_score(self.target,self.y_pred_train))  
        print("prediction on test data", self.y_pred_valid)  
        print("accuracy score of prediction on test data", accuracy_score(self.valid_target, self.y_pred_valid))  
  
if  __name__ == "__main__":  
    a = Load_RCV1()  
    a.run()
```

æœºå™¨å­¦ä¹ çš„æ–‡æœ¬åˆ†ç±»ç»“æœè¯„ä¼°æ–¹æ¡ˆ: 
é¦–å…ˆç§°é¢„æµ‹å¯¹çš„ä¸º "æ£€ç´¢å‡ºç›¸å…³æ–‡æ¡£", è€Œæ£€ç´¢å‡ºé”™è¯¯çš„ä¸º"æ£€ç´¢å‡ºä¸ç›¸å…³"
(1) å¬å›ç‡(recall rate, R): **æ£€ç´¢å‡ºç›¸å…³æ–‡æ¡£æ•°å’Œæ–‡æ¡£åº“æ‰¾åˆ°çš„æ‰€æœ‰ç›¸å…³æ–‡æ¡£æ•°çš„æ¯”ä¾‹ã€‚å®é™…ä¸Šæ˜¯æ£€ç´¢ç³»ç»Ÿçš„æŸ¥å…¨ç‡**
(2) å‡†ç¡®ç‡(Precision, P) : å³ç²¾åº¦, **æ£€ç´¢å‡ºç›¸å…³æ–‡æ¡£æ•°å’Œæ£€ç´¢å‡ºçš„æ–‡æ¡£çš„æ€»æ•°çš„æ¯”ä¾‹, è¡¡é‡æ£€ç´¢ç³»ç»Ÿçš„å‡†ç¡®ç‡**
![[Excalidraw/1.æœºå™¨å­¦ä¹ ç®—æ³•åŸºæœ¬å†…å®¹ 2024-09-11 13.11.50|600]]
ä¸ºäº†åœ¨å¬å›ç‡å’Œ å‡†ç¡®ç‡ä¹‹é—´æ‰¾åˆ°å‡è¡¡ï¼Œ æˆ‘ä»¬é‡‡ç”¨å¦‚ä¸‹æ ‡å‡†è¡¡é‡åˆ†ç±»ç»“æœçš„å‡†ç¡®æ€§:
$$\Large\boxed{F_{\beta} = \frac{ (\beta^{2} + 1) PR}{ \beta^{2}P  + R} } $$
å…¶ä¸­å– $\beta$ = 1 å¾—åˆ°å¸¸ç”¨çš„ F1 measure åˆ†ç±»ç»“æœè¯„ä¼°æ ‡å‡†
$$\Large\boxed{F_{1} = \frac{2PR}{ P + R}}$$
å¯¹åº”çš„ F1 measure ä»¥åŠä¸Šè¿°çš„ Precision å’Œ recall å‡†ç¡®ç‡å¯ä»¥ç›´æ¥é‡‡ç”¨ sklearn ä¸­çš„metrics è·å¾—
```python 
from sklearn.metrics import f1_score, precision score, recall_score 
recall_score(y_true, y_pred); precision_score(y_true, y_pred); f1_score(y_true, y_pred)
```

å¯¹äº precision_score, è¦æ±‚è¾“å…¥æ˜¯ä¸¤ä¸ªäºŒå€¼å‘é‡ã€‚
ä¾‹å¦‚ `[1,0,1,1]` , å®é™…ä¸º `[1,1,0,1]` åˆ™æ£€ç´¢åˆ°çš„ä¸º 0,2,3, A = (0,3), åˆ™ P = R = 0.666

### (3) KNN ç®—æ³•
å…·ä½“å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/4. KNNç®—æ³•å’ŒKMeansèšç±»ç›¸å…³ç®—æ³•|4. KNNç®—æ³•å’ŒKMeansèšç±»ç›¸å…³ç®—æ³•]], å®é™…ä¸Šåªéœ€å»ºç«‹ä¸€ä¸ªè¯é¢‘å‘é‡æˆ–è€…tf-idf å‘é‡, è¿›è¡Œ KNN åˆ¤æ–­å³å¯ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹:
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics import accuracy_score

knn= KNeighborsClassifier(metric='cosine',n_neighbors=2)
labels = [0, 1, 0, 1, 0, 2]
texts = [ "è¿™æ˜¯ ä¸€ç¯‡ å…³äº æœºå™¨å­¦ä¹  çš„ æ–‡ç« ", "è¿™ ç¯‡ æ–‡ç«  è®¨è®º äº† æ·±åº¦å­¦ä¹  çš„ å‘å±•", "æœºå™¨å­¦ä¹  åœ¨ åŒ»ç–—é¢†åŸŸ çš„ åº”ç”¨", "æ·±åº¦å­¦ä¹  æ˜¯ äººå·¥æ™ºèƒ½çš„ä¸€éƒ¨åˆ†", "è¿™ç¯‡æ–‡ç«  è®²è¿° äº† æœºå™¨å­¦ä¹  æŠ€æœ¯ çš„ å‘å±•", "äººå·¥æ™ºèƒ½ åœ¨ æœªæ¥ çš„ åº”ç”¨å‰æ™¯" ]
text_test = ["æœºå™¨å­¦ä¹ ç®—æ³•åŸç†ä¸ç¼–ç¨‹å®è·µ"]

vectorizer= TfidfVectorizer()
X = vectorizer.fit_transform(texts, labels)
knn.fit(X,y=labels)
knn.predict(vectorizer.transform(text_test))
result = knn.predict(X)
print(result, accuracy_score(labels, result))
# ç»“æœ : [0 1 0 1 0 1] 0.8333333333333334 
```

å…¶ä¸­æˆ‘ä»¬å®šä¹‰è·ç¦» metric = 'cosine'ï¼Œå…·ä½“å¯ä»¥å‚è€ƒ [scipy.spatial](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)éƒ¨åˆ†ã€‚

å¦å¤–, åœ¨ `sklearn.neighbors` ä¸­ä¹ŸåŒ…å«äº† KDTree, BallTree ç­‰å†…å®¹, å®é™…ä¸Šä½¿ç”¨ NearestNeighbors å³å¯ã€‚ 
å…·ä½“å¯ä»¥å‚è€ƒ[scikit-learn-neighbors](https://scikit-learn.org/stable/modules/neighbors.html#)  
```python
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KDTree, BallTree
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
distances, indices = nbrs.kneighbors(X)
print(indices,distances)
print(nbrs.kneighbors_graph(X).toarray()) 
```
æ³¨æ„è¿™ä¸ªè®¾ç½®äº† n_Neighbors = 2ï¼Œ ä»…è·å–2ä¸ªæœ€é‚»è¿‘çš„ç‚¹å‚æ•°ã€‚

