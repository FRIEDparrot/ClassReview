---
~
---
## 一、最优化和计算复杂性
### (1) 最优化方法概念
主要介绍机器学习中的最优化方法，计算复杂性, 梯度下降法, 算法分析和随机梯度下降法。
对于多种因素影响决策和目标相互影响的情况, 往往<b><mark style="background: transparent; color: blue">具有多元化和非线性特点</mark></b>, 例如工厂产量和价格等等受大量因素的影响。

而<mark style="background: transparent; color: red">线性规划</mark>也是最早的优化方法, 而当前的最优化问题往往处理问题较为复杂并具有很高的计算精度 , 其最重要的基础如下:
1. 矩阵理论
2. 数值分析
3. 计算机 

<b><mark style="background: transparent; color: blue">最优化方法的定义</mark></b> : 最优化方法是应用数学的重要研究领域，是<mark style="background: transparent; color: red">研究在给定约束之下如何寻求某些因素, 使某一指标达到最优的学科的总称。</mark>最优化问题的基本数学模型为:
$$\Large  \boxed{\min_{x\in R}f(x),\max f(x)\qquad  \text{s.t.}\begin{cases}
h_{i} (x) = 0;  \\
g_{j} (x) \leq 0 
\end{cases}}$$
其中, $f(x)$ 称为**目标函数或者代价函数(Const Function). 而有 $h(x)$ 为等式约束, $g(x)$ 为不等式约束**。

> [!hint] 说明
> 根据目标和约束函数的不同形式， 可以继续细化优化问题的分类。例如:
> 1. $f(x), g(x), h(x)$ 均为线性函数,则称为<mark style="background: transparent; color: red">线性规划</mark>
> 2. 若其中有黑猩猩函数, 则称为非线性规划(例如目标函数为二次-> 二次规划)
> 3. 目标函数为向量函数, 称为多目标规划, 等等

此外, 对于最优化问题中的无约束问题， 可以表述为
$$\Large\boxed{\text{arg}\max_{x} f(x) , (\text{arg}\min_{x} f(x))}$$
最终实际上是求解最优化情况下 $x$ 取值的问题。

### (2) 凸集和分离定理
定义: **在实数域 R 上的向量空间中, 如果集合 S 中任意两点的连线上的点都在 S 内, 则称 S 为凸集** 
![[Excalidraw/4. 最优化方法, 梯度寻优法及其应用 2024-09-19 19.38.53|300]]

设 $X \in R^n$  是一个凸集, 当且仅当: 
$$\alpha x_{1} + (1- \alpha)  x_{2}  \in   X , \quad  \forall x_{1} , x_{2} \in  X \quad \forall  \alpha \in [ 0.1 ]$$
其中, $X$ 是一个集合或者矩阵, $x_1, x_2$ 是集合 $X$ 中的两个向量。

**超平面:** 定义为点集 X: 
$$\Large\boxed{X =  \{x |  c^{T} x = z\}}$$
其中 $x$ 是集合 $X$ 中的一组向量, $c$ 是同维度的系数向量,  即 Ax + By + Cz + .. = d

**支撑超平面** :  设凸集 X 上的一个超平面 $c^{T}x =z$, 给出凸集 $x$ 的边界点 $w$, 得到  $c^T w= z$ 超平面, 并调整z使凸集 $X$ 的所有点位于此超平面的一侧。则成 $c^T w = z$ 为凸集 $X$ 的支撑平面。

#### 分离定理
1. 有了超平面和支撑平面的概念, 分离空间中的两个凸集(或者 n 个), 对于两个凸集之间没有交叉或者重合的部分可以采用超平面将二者隔开, 称为<b><mark style="background: transparent; color: blue">凸集分离定理</mark></b>，描述如下:
设 $S_1, S_2$ 为两个非空集合, 若存在非零向量 $p \in R^{n}$ 以及 $\alpha \in  R$, 使得:
$$S_{1} \subseteq  H^{-}  = \left\{x \in R | p^{T}x  \leq  \alpha  \right\}$$
$$S_{2} \subset  H^{+} =  \left\{ x\in R |  p^{T}x \geq  \alpha \right\}$$
此时称超平面 $H = \left\{ x \in R^n |  p^T x = \alpha \right\}$ 分离了集合 $S_1$ 与 $S_2$  

### (3) 凸函数及其性质
凸函数是凸集中的元素的数学特征, 体现出了凸集中元素的规律性, 被定义为某个向量空间的凸子集 $C$ 上的实值函数 $f$, 若对于其上的任意的两点 $x_1, x_2$; 以及 $\alpha \in [0,1]$, 都有:
$$\alpha  f( x_{1} ) + (1 - \alpha)f ( x_{2})\geq   f(\alpha x_{1} + (1- \alpha) x_{2})$$
![[Excalidraw/4. 最优化方法, 梯度寻优法及其应用 2024-09-21 10.28.01|450]]
需要说明: 
1. 凸函数仅是定义在凸集上的函数, 反映了凸集中元素的存在形式或者规律。
2. 凸函数<mark style="background: transparent; color: red">不要求函数是连续的和可微的</mark>。

凸函数的判定:
1. 如果 $f(x)$ 为凸函数, 则 $\alpha f(x)$ 也是凸函数($\alpha \geq 0$); 如果 $f_1, f_2$ 都是凸函数, 则 $f_1 + f_2$ 也是凸函数。
2. 取 $f_1, f_2 , \dots  f_k$ 是凸集 $S$ 上的凸函数, 则有 $\phi (x) = \sum^{k}_{i=1} \lambda_{i} f_{i}(x), \forall  \lambda_{i} \geq 0$ 和 $\psi(x) = \max_{1 \leq i \leq k} f_{i}(x)$ 都是 $S$ 上的凸函数。
3. 设在凸集$D \subseteq R^{n}$上, 满足$f(x)$可微, 则 f(x) 在 **D 上为凸函数的充要条件**是对于任意的 $x, y \in  D$, 均有如下公式成立:
$$f(y) \geq  f(x) + \nabla f(x)^{T} (y - x)$$
4. 在开凸集 $D\subseteq R$ 之内, 若 $f(x)$ 二阶可微, 则 $f(x)$ 是 $D$ 内 凸函数的充要条件为:
	- 对于任意的 $x$, $f(x)$ 的 Hesse 矩阵半正定。 即
$$G(x)  = \nabla^{2}  f(x) =  \left[\begin{matrix}
\frac{\partial^{2} f}{\partial x_{1}^{2}} & \frac{\partial^{2} f}{\partial x_{1}\partial x_{2}} & \dots  & \frac{\partial^{2} f}{\partial x_{1}\partial x_{n}}    \\   \frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}  & \ddots   \\  \vdots   \\  \frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}  &\dots  & \dots &   \frac{\partial^{2} f}{\partial x_{n}^{2}}  
\end{matrix}\right]$$
为半正定矩阵。

> [!HINT] 常用的凸函数
> 1. 线性函数和仿射函数都是凸函数 
> 2. 最大值函数 
> 3. 幂函数 $a \in [0,1]$ 时,  $x^{\alpha}$ 是凸函数。以及绝对值的幂函数
> 4. 对数函数 $log(x)$ 在 $R_{++}$ 上是凸函数。
> 5. 指数和的对数是凸函数  $f(x) = \log (\exp x_1 + \dots + \exp x_n)$ 
> 6. 几何平均: 
>    $$f(x) = (\prod^{n}_{i=1} x_{i})^{\frac{1}{n}}$$
> 7. 范数

另外, 在优化过程中需要防止陷入局部最优, 因此<b><mark style="background: transparent; color: blue">往往采用凸函数作为优化问题的目标函数</mark></b>. 即一般转换为凸优化问题。其中==最典型的是 Logistic 函数==。

机器学习中, 使用最优化求解的目标函数基本都是凸函数。 包括==**激活函数, 支持向量机等的核函数等等**==。

## 二、计算复杂性与 NP 问题
### (1) NP 问题和自动机模型
NP 问题即 $P = NP?$ 类问题;
由于衡量问题是否可解的重要指标是: **该问题是否能在多项式时间内求解? 还是仅能够在指数时间内求解**。因而多项式时间求解的问题一般称为易解问题， 而指数为难解问题。

P类问题是能够<b><mark style="background: transparent; color: blue">以多项式时间的确定性算法</mark></b>进行判定和求解的问题。即最终一定能确定一个唯一的结果。而 $NP$ 类问题是指可以以多项式时间的非确定性算法进行判定或者求解。即<mark style="background: transparent; color: red">大多是非确定性的， 但时间复杂度可能是多项式级别的。</mark> 但是 NP <mark style="background: transparent; color: red">问题中有一个 NP 完全问题子类, 是最难的 NP 问题，且任何一个问题均无法用多项式复杂度求解</mark>。

首先说明<b><mark style="background: transparent; color: blue">自动机模型</mark></b> : <b><mark style="background: transparent; color: blue">自动机模型</mark></b>实际上指<mark style="background: transparent; color: red">基于状态变化进行迭代的算法。</mark>例如图灵机, 有限状态自动机, 玻尔兹曼机等等。

确定性: 针对各种自动机模型, 根据状态输入, 若<mark style="background: transparent; color: red">自动机的状态是唯一确定的，则称确定性</mark>。若**自动机有多个状态可以选择, 并且尝试执行每个可选状态时, 则称为非确定性**。
机器学习的算法一般是针对解决 NP 类问题的算法, **而算法的状态转移具有非确定性。产生的结果位于一个误差允许范围内**。而最优化理论也主要是针对该问题进行的。

### (2) 逐次逼近法
#### 1. 推导过程
对于如下形式的方程组:
$$Ax = b$$
其中 $A$ 为非奇异矩阵(行列式不为零), <mark style="background: transparent; color: red">对阶数不高的方程, 一般采用主元消去求解。但是对于阶数大的大型稀疏矩阵, 一般需要采用迭代法或逐次逼近法进行求近似解</mark>。例如:
$$\begin{cases} 
8 x_{1}  - 3 x_{2} + 2 x_{3} = 20  \\
4 x_{1} + 11x_{2} - x_{3}  = 33  \\
6 x_{1} +  3 x_{2} + 12 x_{3} = 36 
\end{cases}$$
取 $Ax = b$, 此时将系数同除 $x_1, x_2,x_3$ 的对角系数 $[8, 11, 12]$ 并移项, 得到:
$$\begin{cases}
x_{1} =  \qquad  0.375 x_{2}  - 0.25 x_{3} + 2.5  \\
x_{2}= -0.3636 x_{1}+ 0.0909 x_{3} +    3   \\
x_{3} =  0.1667 x_{1} + 0.08334 x_{2}  +  3 
\end{cases}$$
记 $f_{i} = \frac{b_{i}}{a_{ii}}$, 则**方程可以变形成为**: 
$$\boxed{\Large  x = B x +  f}$$
我们可以取初始值为 $x = [0,0,0]^{T}$, 每一次做迭代, 则有:
$$\boxed{ \Large  x^{(k+1)} = B  x^{(k)} + f}$$
代 x 收敛之后, 得到 x 的近似解。求解的 python 代码如下:
```python 
import numpy as np  
import numpy.linalg as la  
  
A = np.mat([[8 , -3 , 2] , [4 , 11 , -1] ,[6,  3 , 12]],dtype=np.float32)  
b = np.array([[20],[33], [36]])  
  
x0 = np.array([0,0,0]).reshape(-1, 1)  
  
lmda = np.diag(A)   # diagnoal matrix of A  
B = -np.copy(A)/lmda.reshape(-1, 1)  
np.fill_diagonal(B,0)  
f = b/lmda.reshape(-1, 1)  
  
max_iter_time = 1000  
  
errlst = []  
xk = x0; xk__ = xk  
for k in range(max_iter_time):  
    xk = np.dot(B, xk) + f  
    err = la.norm(xk - xk__)  
    errlst.append(err) # error list  
    xk__ = xk  
    if err < 1e-6:  
        breakprint(xk)  
print("total iteration time:", len(errlst), "error: ", errlst[-1])
# [[3.0000318  1.99987399 0.99988127]] 
```

一般停止准则根据 $||\varepsilon^{(k)}||$ 定义, 定义为:
$$||\varepsilon^{(k)}|| =  || x^{(k)} - x^{*} ||$$
也可以采用 $||x^{(k)} - x^{(k-1)}||$ 代替。

需要说明的是, 迭代法并不总是收敛, 此时一般是循环类型: 例如:
$$\begin{cases}
x_{1} = 2 x_{2}  + 5  \\
x_{2}  = 3 x_{1} +5 
\end{cases}$$
#### 2. 定义
**定义**: 对于给定的方程组 
$$\Large x^{(k+1)} = B_{0}x^{(k)} + f$$
代入求解称为迭代法。

说明: 
1. 如果 $\lim_{k \rightarrow \infty} x^{(k + 1)}$ 存在 , 则称为迭代法收敛，则 $x^*$ 为方程组的解。否则**称为迭代法发散**。
2. 研究 $\left\{x^{(k)} \right\}$ 的收敛性, 引入误差向量
$$\varepsilon^{(k + 1)} = x^{(k + 1)}  - x^{*} =  B_{0} x^{(k)} + f - Bx^{*}  - f = B (x^{(k)} - x^{*})$$
此时有:
$$\varepsilon^{(k+ 1)} =  B \varepsilon^{(k)} \rightarrow   \varepsilon^{(k)} = B^{(k)} \varepsilon^{(0)}$$
对于考察$\left\{ x^{(k)}\right\}$的收敛性, ==只需研究 $\lim_{k \rightarrow \infty} B^{k} = 0$ 的条件==。

### (3) 梯度下降法 
梯度下降法参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/⚓Deep Learning Basic Concepts/Chapter3 Linear Neural  Networks for regression(back Propagation)#2. Minibatch Stochastic gradient descent(SGD)|SGD Method]], 一般仅需在迭代过程中求解 $f(x)$ 的负梯度方向并添加步进即可。
推广到 $k$ 次迭代, 定义负梯度向量为:
$$\nabla^{(k)} = -  \frac{\nabla f(x_{k})}{||\nabla  f(x_{k})||}$$
此时, 从 $x_k$ 出发, 引入一个梯度下降步长 $\rho_{k}$, 则有**迭代公式**: 
$$x_{k+1}  = x_{k}  + \rho_{k} \nabla^{(k)}\qquad  f(x_{k + 1}) =  f(x + \rho_{k}\nabla^{(k)})$$

## 三、线性分类器
### (1) 线性分类器概念
**线性分类器**是<b><mark style="background: transparent; color: blue">最早的神经网络模型, 也称为感知器模型</mark></b>; 根据支撑超平面和凸集定理, 如果训练数据集是两个互不相交的凸集子集, 则可以<mark style="background: transparent; color: red">找到一个支撑超平面分开两个子集, 称为线性分类器</mark>。

一般感知器包含<mark style="background: transparent; color: red">一个线性变换和一个激活函数</mark>。例如, 采用变换
$$\Large \boxed{f(x) = w^{T}  x + b}$$
首先在有样本的前提下, 在样本中添加一列$b$, 一般取单位列向量。即 $b = x_0$, 有:
$$f (x) =  w_{0} x_{0} + w_{1} x_{1} + \dots  + w_{n}x_{n}$$
在二分类线性分类器中, **一般可以使用硬限幅函数(Hardlim), 即 x < 0 时预测类别为0, >=0 时预测类别1**;
![[Excalidraw/4. 最优化方法, 梯度寻优法及其应用 2024-09-21 22.28.39|350]]
计算激活函数之后即可获得预测分类结果 hardlim(f),此时与 classLabel 比较获得 error;
$$\text{error} =  \text{classLabel}- \text{harmlim}(f)$$
为了让误差趋近于零, 即 err->0, 则我们考虑 $\frac{\partial \text{error}}{\partial w}$, 如果 $\frac{\partial \text{error}}{\partial w} >0$, 则 $w$ 增加, $\text{error}$ 上升。反之 $w$ 增加, $\text{error}$ 下降。因此我们可以取迭代公式:
$$\Large \boxed{w_{k + 1} = w_{k} - \alpha\frac{\partial \text{error}}{\partial w_{k}}}$$
其中 $\alpha$ 为学习率; 显然 error 导数难以求解, 由于不考虑hardlim作用时, 有:
$$\frac{\partial \text{error}}{\partial w_{k}} = \begin{cases}
-x_{k} \qquad  \text{error} =1  \\
x_{k} \qquad  \text{error  = -1 }
\end{cases}\qquad  \frac{\partial \text{error}}{\partial b} = \begin{cases}
-1   \qquad  \text{error = 1}  \\
1 \qquad   \text{error = -1}
\end{cases}$$
所以我们只需要用 $x_{k} \times \text{-error}$ 代替 $\frac{\partial \text{error}}{\partial w_{k}}$, 得到: 
$$\boxed{\Large  w_{k+1} = w_{k} + \alpha x^{T} \times  \text{error}}$$
显然对于b有: 
$$\boxed{b_{k+1}  = b_{k} + \alpha \times  \text{error}}$$
为<b><mark style="background: transparent; color: blue">训练集预测的梯度迭代公式</mark></b> (其中$b_{k}$为第一列)
### (2) Logistic 函数和 Logistic 梯度下降法
#### 1. 定义
Logistic 函数<mark style="background: transparent; color: red">也叫 sigmoid 函数, 并定义为</mark>: 
$$\text{Logistic} (x) = \frac{1}{1  + \exp{(- w^{T} x})}$$
选取 Logistic 函数的原因是:
$$\ln\frac{p}{1-p}   = \ln\frac{\frac{1}{1 + \exp -w^{T}x}}{\frac{\exp (- w^{T} x)}{1 + \exp  (-w^{T} x)}} =\ln  \frac{1}{\exp (- w^{T}x)} =  w^{T} x$$
实际上这是<mark style="background: transparent; color: red">训练集梯度的计算结果</mark>, 此时，可以**直接采用梯度结果计算函数的极值**。即可以节省运算量。

> [!caution] Logistic 拟合梯度下降原理
> 实际上是直接采用 Logistic 函数去拟合 y, 然后直接计算误差 ($w = (w_1, w_2 ,\dots  w_n)$)

> [!HINT] Logistic 映射
> 一般地, 有以下形式的 Logistic 函数:
> $$f(x) = \frac{1}{1 + e^{-k x}}$$
> 显然有 $\frac{\partial f(x)}{\partial x} = kf(x) (1 - f(x))$, 即 Logistic 映射:
> $$X_{n + 1} = k X_{n} (1 - X_{n})$$
> 对于不同的 k 取值范围, 有不同的周期特性, 其中 $0\leq k < 3$ 时, 是收敛的, 而 > 3 时，会呈现 2 周期, 4 周期, $\dots$ 等混沌特性, 实际对应费根鲍姆图。

#### 2. 极值特性
对于 Logistic 函数的极值情况, 首先，采样本集各个编辑分布的乘积表示样本的联合分布。并采用对数似然函数表达: 取 $y_i$ 为第i个样本的真实值, 参考[[📘ClassNotes/📐Mathmatics/🎣Probability Theory/第二章 随机变量及其分布#1. 0-1分布(两点分布)|两点分布的概率表示]], 则<mark style="background: transparent; color: red">边缘概率密度可以表示为所有样本的乘积</mark>:
$$l (w) = \prod^{n}_{i=1}(P\left\{  Y = 1 | x_{i}\right\} )^{y_{i}} (1 -P\left\{ Y= 1 | x_{i} \right\})^{1- y_{i}}$$
显然, 要最大化实际预测准确率, <mark style="background: transparent; color: red">只需最大化边缘概率密度</mark>。其中对于 Logistic 模型, 其对数称为对数似然函数: 
$$\boxed{L (w) =   \ln(l(w)) = \sum_{i = 1}^{n} y_{i} \ln (p(x_{i})) + (1 -  y_{i}) \ln (1- p(x_{i})) = (1 -y_{i}) (- wx_{i}) - \ln  (1 + e^{- wx_{i}})}$$
则有: 
$$\Large\boxed{\text{arg}\max_{w} (L(w))}$$
此时, 参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/1. ML estimation 最大似然估计|1. ML estimation 最大似然估计]]，采用如下计算极值(推导简单,略去):
$$\boxed{\frac{\partial L}{\partial w} =\sum_{i = 1}^{n} \left(y_{i} - \frac{1}{1 +  \exp (- w x_{i})}\right) x_{i}  = 0}$$
取导数为0时,显然没有解析解, 但是显然有:
$$\frac{\partial L}{\partial w_{i}} =  (y_{i} - \text{Logistic}(x_{i})) \times x_{i}  =  \text{error} \times x_{i}$$
此外, Logistic 函数的**目标函数 $L(w)$ 也是凸函数**(参考[[#(3) 凸函数及其性质]]), 因此仍然可和上面部分公式相同。即为了<b><mark style="background: transparent; color: blue">最大化 L, 每一次采用加上其导数的方案</mark></b>(这一点区分于err的最小化)
$$w_{k +1 } = w_{k} +  \alpha\frac{\partial L}{\partial w_{i}}$$
以下的代码示例采用 iris 数据集建立 Logistic 函数进行二分类 (1,2视为一类)
```python
import numpy as np  
from sklearn.datasets import load_iris  
import matplotlib.pyplot as plt  

def logistic(w:np.ndarray, x : np.ndarray) -> np.ndarray:  
    return 1/(1 + np.exp(-np.matmul(w, x.T)))  
  
def train_pred(x_train:np.ndarray, y_train:np.ndarray, iter_time = 1000 , alpha = 0.001):  
    y = np.array([1 if item != 0 else 0 for item in y_train]).reshape(1,-1)  
    w = (np.random.rand(x_train.shape[1] + 1) * 2 - 1).reshape(1, -1)  
    X = np.c_[np.ones(x_train.shape[0]),  x_train/np.max(x_train, axis=0)]     # concatenate ones to the feature matrix  
  
    for i in range(iter_time):  
        err = y - logistic(w, X)  
        w = w + alpha * np.matmul( err, X)  
        if i % 100 == 0:  
            print(f"iter {i}, w: {w}")  
    y_pred = logistic(w, X)  
    return y_pred.squeeze(0), w.squeeze(0)  
  
if __name__ == "__main__":  
    X,y = load_iris(return_X_y=True)  
    y_pred, w = train_pred(x_train=X, y_train=y)  
    y_pred= [0 if item < 0.5 else 1 for item in y_pred]  
    # draw figure  
    fig, axes = plt.subplots(1,2, figsize=(5,5))  
    axes[0].scatter(X[:,0], X[:,1], c=y)  
    axes[1].scatter(X[:,0], X[:,1], c=y_pred)  
    plt.show()
```
结果如下: 
![[attachments/Pasted image 20240929135337.png|450]]
一般可以研究权重和截距分量的变化来衡量求解过程的收敛性。

### (3) 随机梯度算法
随机梯度算法是Logistic 算法的改进算法, 用于克服在迭代过程中的由于学习率 $\alpha$ 固定导致的无法接近最优点, 收敛速度较慢的问题。
随机梯度法对于每个向量重新计算步长, 并且可以衡量每个向量的变化趋势。对于每个向量, 在计算时$\alpha$ 参数的计算公式为 : 
$$\alpha =  \frac{2}{1.0 + i  +j} +  0.0001$$
上式中, i = 迭代次数, j = 抽取次数; 而 2 为经验常数; 经验常数越大，动态变化的$\alpha$ 值导致的震荡越大。
在后期的迭代中, $\alpha$ 的值逐步平缓至趋近于零。
