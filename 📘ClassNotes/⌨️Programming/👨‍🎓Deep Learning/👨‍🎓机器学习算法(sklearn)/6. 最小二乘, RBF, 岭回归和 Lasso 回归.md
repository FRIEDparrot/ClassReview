## 一、线性系统及其预测
预测中的核心概念是<mark style="background: transparent; color: red">回归问题</mark>(Regression), 回归反映了系统的随机运动总是趋向于其整体运动规律的趋势。
### (1) 最小二乘回归
#### 1. 基本模型
首先, 我们的基本模型是
$$Y= a X + b$$
其中 a,b 为常数; 取残差
$$\xi_{i} = y_{i}- ax_{i} - b$$
则采用残差的平方和:
$$Q = \sum_{i = 1}^{n} \xi_{i} ^{2} \overset{\min}{\longrightarrow} \frac{\partial Q}{\partial a} = \frac{\partial Q}{\partial b} = 0 $$
得到:
$$\begin{cases}
\sum_{i = 1}^{n}  ( x_{i}y_{i} - ax_{i}^{2} - bx_{i}) = 0  \\
\sum_{i = 1}^{n}  (y_{i} - a x_{i} - b ) = 0  
\end{cases}$$
此时, 将 $bx_i$ 移动到右侧解出 $a$, 有: 
$$\sum_{i = 1}^{n} (x_{i}y_{i} - ax_{i} ^{2} ) =  n * b \overline{x} =  \sum_{i = 1}^{n} \overline{x} (y_{i} - a x_{i})$$
得到<b><mark style="background: transparent; color: blue">最小二乘公式</mark></b>:
$$\boxed{a = \frac{\sum_{i = 1}^{n} x_{i} y_{i} - n\overline{x} \overline{y}}{\sum_{i = 1}^{n}x_{i}^{2} - n \overline{x}^{2}}\qquad b = \overline{y} - a \overline{x} }$$
#### 2. 正规方程组
一般的方法参考 [MATLAB最小二乘多项式拟合](https://mp.weixin.qq.com/s?__biz=Mzg2MDY5MTY4NA==&mid=2247484025&idx=1&sn=0b580f3a94bf79d0c148e0d783ecc30d&chksm=ce23cff2f95446e428a8b6c4289d6167d5f199235b628242efc78ca82a47ffdf8111a0ab8582&token=777628491#rd), 对于实际的样本点, 我们有: 
$$X = \left[\begin{matrix}
x_{11} & \dots   & x_{1n} \\  
x_{21} & \dots & x_{2n}  \\ 
\vdots   \\ && x_{mn}
\end{matrix}\right]$$
而有: $y_1 = a_{1} x_{11} + a_2 x_{12} + \dots +  a_n x_{1n} + b_{i}$, 则取 $a = (a_{1}, \dots  a_{n})^T$, 得到 $Y = Xa + b$, 此时我们将 $b$ 合并到 $X$ 第一列得到:
$$Y =   X\boldsymbol{a} \quad  \rightarrow \quad   X^{T} Y = X^{T } X \boldsymbol{a}$$
显然为了方程组有解, **对称矩阵 $X^{T} X$ 必须非奇异**; 并有
$$A = (X^{T}  X)^{-1}  (X^{T}Y)$$
为<mark style="background: transparent; color: red">方程组的最小二乘解</mark>. 同时, 一般采用[[📘ClassNotes/📐Mathmatics/🎣Probability Theory/第四章 随机变量的数字特征#三、协方差及其相关系数|相关系数]]方法进行评估, 衡量线性程度.

### (2) 径向基函数网络
RBF (Radius Basis Function)网络可以<b><mark style="background: transparent; color: orange">以任意精度逼近任意连续或者离散函数  并且特别适合进行非线性预测和分类等等问题</mark></b>。基本架构如下:
![[Excalidraw/6. 预测回归初步和 RBF 网络 2024-11-25 11.24.01|450]]
其中<mark style="background: transparent; color: red">激活函数为</mark>高斯核函数(正态分布函数), 参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/8. SVM支持向量机#3. 径向基函数核函数(高斯核)|5.SVM支持向量机]]:
$$ W(i,j) =  \exp \left(- \frac{1}{2\sigma^{2} }||x  - y||^{2}\right)\quad (j = 1,2, \dots   n)$$
对于径向基函数而言，常见方法有精确求解，OLS求解和KMeans聚类方法， 其Matlab中的建立极为容易:
```Matlab  
newrbe    %精确求解  
neerb      %OLS方法  
``` 
需要说明的是，对于精确求解方法，有n个样本点就有n个径向基函数，会导致极大复杂性增加。在matlab中，可以使用定义误差的方法，而网络误差是随着径向基增加而下降的过程。

其一般原理是先计算欧式距离再 RBF 拟合的方法; 权值调整是通过激活函数实现的, 而不采用反向传播或者[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/5. BP 神经网络, SOM 神经网络和 Boltzmann机|5. BP 神经网络, SOM 神经网络和 Boltzmann机|BP 神经网络]]类似的误差反馈权值更新。

#### 1. 训练过程
首先, 径向基函数仍然是以 [[#2. 正规方程组]] 思路, 建立方程
$$Y  = X\boldsymbol{a}  \quad  \overset{}{\longrightarrow}  X^{T} X \boldsymbol{a}  = X^{T} Y\overset{}{\longrightarrow} a = (X^{T} X)^{-1 } X^{T} Y $$
我们将系数向量 $a$ 用 $w$ 表示, 成为
$$w = (X^{T} X^{-1})X^{T}  Y$$
<mark style="background: transparent; color: red">核方法将低维空间中的数据矩阵  X 映射到 新的设计矩阵 K, 而新的设计矩阵 K 实际上是通过采用 M 个基函数构造的</mark> 
$$K_{ij} = \phi\left(\frac{||x_{i} - m_{j}||}{\sigma}\right) \qquad \overset{K 替代X, w 替代a}{\longrightarrow}  \qquad K w = y$$

首先, 设样本矩阵 i, j 行分别为  $x_i, x_j$ 则<b><mark style="background: transparent; color: orange">径向基函数插值办法会计算两者距离</mark></b>, 并**形成一个 $m \times m$ 的距离矩阵 K**:
$$d_{ij} = x_{i} -  x_{j}$$
我们**取插值矩阵 K $(m \times m 矩阵)$ 为高斯核函数**:
$$K_{i,j}  = \exp \left( -  \frac{d_{i,j}  \times  d_{i,j}^{T}}{2 \sigma^{2}} \right) = \exp \left(- \frac{||x_{i} -  x_{j}||^{2}}{2 \sigma^{2}}\right)$$
此时 K 已经求出, 其中 $K$ 表示某个输入点 $x_i$ 到 $x_j$ 的相似性, $w$ 是一个 $m \times 1$ 的向量, 表示权重。有
$$y= K w \qquad \overset{分量}{\longrightarrow}\qquad   y_{i} = \sum_{j = 1}^{m} K_{i,j} w_{j} $$
即可解出对应的向量 $w$ 

#### 2. 预测过程
则**最终的预测方法, 首先需要 <b><mark style="background: transparent; color: orange">计算与所有训练点的核函数</mark></b>**,  设新的向量为 $x$, 则先计算和每个训练点的距离, 仍然取 $y = K w$ 然后将距离 $K$ 乘 $w$ 得到:
$$\boxed{\Large   y = \sum_{i = 1}^{m}  \exp \left( -   \frac{||x - p_{i} ||^{2}}{2 \sigma^{2}}\right)  \times w}$$
其中 $p_i$ 为上述所有的训练点。

### (3) RBF 神经网络代码实现
下面的代码采用 RBF 的插值方式, 实现了经典 matlab 函数 peaks 的随机 50 点 插值绘制:

**需要注意的是, 其中 rbf_fun 部分, 采用矩阵方法** `np.exp(- 1 / (2 * sigma ** 2) * np.linalg.norm(d, axis=-1) ** 2) ` 适配了多个距离向量情况下的距离计算。实现了**在预测时能够快速计算与其他各个训练点的距离向量**

```python fold title:RBF径向基函数插值的python实现
import numpy as np  
import sys  
import os  
import matplotlib.pyplot as plt  
from  mpl_toolkits.mplot3d import Axes3D  
from scipy.interpolate import griddata  
  
os.chdir(os.path.dirname(os.path.abspath(__file__)))  
  
mu = 0.02  
k  = 0.03  
  
def activate_fun(x,W,c):  
    y = np.sum(np.exp(-1/(2 * sigma **2) * np.linalg.norm(x-c)**2))  
  
class RBF_test():  
    def __init__(self, x,y):  
        self.x = x  
        self.y = y  
        if np.ndim(x) == 2:  
            self.m, self.n = x.shape  
        else:  
            raise ValueError("x must be 2D array")  
  
    def train(self):  
        K = np.zeros((self.m, self.m))  
        for i in range(self.m):  
            for j in range(self.m):  
                d  = self.x[i,:] - self.x[j,:]   # 计算两点之间的距离向量  
                K[i,j] = self.rbf_fun(d)      # 权重矩阵的每一项按照高斯函数计算;  
        self.w = np.linalg.solve(K, self.y)  
  
    def predict(self, x):  
        d = (self.x - x)  # 计算输入向量与所有训练集向量之间的距离向量  
        y = np.sum(self.rbf_fun(d) @ self.w)        # 计算预测值  
        return y  
  
    def rbf_fun(self, d, sigma = 1.0):  
        # 对于 d 为矩阵时, 实际上不是 d @  d.T, 而是采用 np.linalg.norm        return np.exp(- 1 / (2 * sigma ** 2) * np.linalg.norm(d, axis=-1) ** 2)  # 采用 axis = -1 表示对最后一个维度进行计算  
  
  
def peaks(x, y):  
    term1 = 3 * (1 - x) ** 2 * np.exp(-x ** 2 - (y + 1) ** 2)  
    term2 = -10 * (x / 5 - x ** 3 - y ** 5) * np.exp(-x ** 2 - y ** 2)  
    term3 = -1 / 3 * np.exp(-(x + 1) ** 2 - y ** 2)  
    return term1 + term2 + term3  
  
  
if __name__ == '__main__':  
    # sample 50 points  
    x1 = np.random.rand(50) * 6 - 3  
    x2 = np.random.rand(50) * 6 - 3  
    X  = np.array([x1, x2]).T  
    Y = peaks(x1, x2)    #  np.sin(np.sqrt(x1 ** 2 + x2 ** 2)) *  np.sqrt(x1 ** 2 + x2 ** 2)  
    rt = RBF_test(X, Y)  
    rt.train()  
  
    # 生成网格点  
    x_grid = np.linspace(-3, 3, 100)  
    y_grid = np.linspace(-3, 3, 100)  
    x_mesh, y_mesh = np.meshgrid(x_grid, y_grid)  
    z_mesh = np.zeros_like(x_mesh)  
  
    X_input = np.array([np.hstack(x_mesh), np.hstack(y_mesh)]).T  # 将网格点转换为输入 X 矩阵  
  
    for i in range (x_mesh.shape[0]):  
        for j in range(x_mesh.shape[1]):  
            z_mesh[i,j] = rt.predict([x_mesh[i,j], y_mesh[i,j]])  
    fig = plt.figure(figsize=(10, 7))  
    ax = fig.add_subplot(1, 2, 1, projection='3d')  
    ax.plot_surface(x_mesh, y_mesh, z_mesh, cmap='viridis')  
  
    # 绘制原始函数的插值显示  
    ax2 = fig.add_subplot(1, 2, 2, projection='3d')  
    z_mesh = griddata(X, Y, (x_mesh, y_mesh), method='linear') # 插值（线性）  
    ax2.plot_surface(x_mesh, y_mesh, z_mesh, cmap='viridis')  
    plt.show()
```

获得如下的插值图像 (第一张是插值结果)
![[attachments/Pasted image 20241125171319.png|450]]

对于 Python 中的 Scipy 模块已经有 `scipy.interpolate.Rbf` 可以直接进行插值和数据拟合。因此上述代码可以简化为:  
```python
from scipy.interpolate import Rbf
import numpy as np
import matplotlib.pyplot as plt

# 定义样本点
x1 = np.random.rand(50) * 6 - 3
x2 = np.random.rand(50) * 6 - 3
X = np.array([x1, x2]).T
Y = peaks(x1, x2)  # 原始数据的值

# 使用 Rbf 类进行插值
rbf = Rbf(x1, x2, Y, function='gaussian', epsilon=1.0)  # 默认使用高斯核

# 生成网格点
x_grid = np.linspace(-3, 3, 100)
y_grid = np.linspace(-3, 3, 100)
x_mesh, y_mesh = np.meshgrid(x_grid, y_grid)

# 预测网格点上的值
z_mesh = rbf(x_mesh, y_mesh)

# 绘制结果
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.plot_surface(x_mesh, y_mesh, z_mesh, cmap='viridis')
plt.show()
```

常用的核函数有 `multiquadric`, `inverse`, `gaussian`, `linear` 等 


## 二、岭回归和 Lasso 回归
### (1) 多重共线性的概念与验证
在最小二乘法和 RBF 高斯核函数预测的, **有一个限制要求**, 即样本矩阵的行列式不能为0, 或者<b><mark style="background: transparent; color: blue">样本的各个特征之间不表现为强烈的线性相关性</mark></b>, **并将此称为多重共线性**. 这种会导致模型估计失真, 而对于大多数数据, 有可能有近似共线性出现。

岭回归(Ridge Regression)是一种最小二乘的改进策略,  是对于线性系统常用的估计方式。
首先, 我们可以通过计算相关系数矩阵得到每一列的相关性。这个可以通过 np.corrcoef 得到 Pearson 相关系数 
```python
import numpy as np
data = np.random.rand(10, 2) # 随机生成 10x2 数据
correlation_matrix = np.corrcoef(data, rowvar=True)
# 得到每一行的向量
np.linalg.det(x.T @ x)  # 计算共线性程度
```

首先, 矩阵的随机变量之间存在多重共线性, 即有:
$$\left| X^{T}  X\right| \approx 0 $$
我们加上一个参数矩阵 $kI$, 此时 $X^{T}  X+ KI$ 的奇异程度显然会显著减小, 则标准化后仍然采用 $X$ 表示, 并定义对于原始方程:
$$Y = X \alpha $$
取 $\alpha$ 的近似:
$$\Large\boxed{\alpha(k) = (X^{T} X +  \lambda I)^{-1}  X^{T }Y}$$
称为 $\alpha(k)$ 的岭回归估计; 并称**其中 $\lambda$ 为岭参数**;

`````ad-note
title: 补充: 岭回归和古典回归的表达以及相关推导
collapse: open
相较于古典回归的损失函数, 有 $Y= X \theta$, 认为损失函数为:
$$\hat{\theta} = \arg\min_{\theta} \left\| Y - X\theta \right\|_2^2$$
其中, $\theta$ 为表示权重的参变量

相比于古典回归, <b><mark style="background: transparent; color: orange">岭回归</mark></b>通过在**普通最小二乘法的目标函数中加入一个正则化项来约束回归系数**。正则化项通常是 1 范数或者 2 范数 (此处取为2范数平方)，旨在**缩小回归系数的绝对值，从而提高模型的稳定性**，特别是在存在多重共线性的情况下。

岭回归对应的目标函数如下:
$$\hat{\theta} = \arg\min_{\theta} \left( \left\| Y - X\theta \right\|_2^2 + \lambda \left\| \theta\right\|_2^2 \right)$$

展开合并, 则 $J(\theta)= \hat{\theta}$ 为我们的目标函数 :
$$\hat{\theta} = \text{arg}\min_{\theta} (Y - X\theta)^{T} (Y - X\theta) = Y^{T} Y - 2 \theta^{T} X^{T} Y + \theta^{T} X^{T} X \theta + \lambda \theta^{T}  \theta = J(\theta)$$
此时, 对目标函数求梯度,  变为 
$$\nabla_{\theta} J(\theta) = -2 v^{T}\cdot X^{T}Y  + 2 v^{T} X^{T} X \theta +  2 \lambda v^{T} \theta$$
取 $\nabla_{\theta} = 0$ 其中 $v = (1,1,1,\dots 1)^T$, 则最小化时, 将 $v^{T}$ 提出,  有:
$$X^{T } Y =  X^{T} X \theta  + \lambda I \theta$$
容易得到最小二乘的角度来看的最小值对应$\theta$公式: 
$$\theta =  (X^{T} X +  \lambda I)^{-1} X ^{T} Y$$
因此上面的两种表达形式是等价的。
`````

### (2) 岭迹分析
首先, 假设 X 已经标准化, 则 $X^T X$ 为自变量样本相关阵, <b><mark style="background: transparent; color: orange">如果 Y 也标准化过, 则计算结果是标准化后的岭回归估计</mark></b>。

显然，$A(\lambda)$ 是 $\lambda$ 的函数, 而 $A(\lambda)$ 随  的轨迹称为岭迹, 需要说明, $A (\lambda)$ 是回归参数 A 的有偏估计 
![[Excalidraw/6. 预测回归初步和 RBF 网络 2024-11-25 18.44.39|450]]

实际上 $A(\lambda)$ 作为 A 的有偏估计比最小二乘估计更加稳定。

对于**图1  所示的岭迹**, 当岭迹有显著的上升或者下降时, 则 $X^{T} X$ 对 $Y$ 的影响是比较大的, 只要在其上附加一定部分, 就会产生显著影响(主要指正负改变)。

例如图4中, $A_1(\lambda)$ 和 $A_{2}(\lambda)$, 虽然 $A_2(\lambda)$ 不稳定, 而总体上是稳定的, <b><mark style="background: transparent; color: orange">这种情况往往出现于相关性很大的场合, 此时从变量选择角度而言, A, B 保留一个就够了</mark></b>。(往往是符号不合理造成的)

如果将某一具体实例中, 判断最小二乘是否适用,  我们可以将所有的回归系数岭迹画出, 针对于图 5 中, 则可能最小二乘是不适用的; 而针对图 6, 最小二乘一般是适用的。

一般的 $\lambda$ 值选取原则: 
1. 所有回归系数的岭估计都是趋向于稳定的;
2. 最小二乘估计中， 符号不合理的回归系数, 在岭回归中变为合理
3. 残差平方和增大不太多 
4. 可剔除较小或者趋于零的不合理 $\lambda$ 值

最常用的包括 **==Horel-Kennard 公式, 岭迹法和交叉验证法==** 

### (3) Lasso 回归模型
对于线性模型, 上面我们已经推导出:
$$\alpha = (X^{T} X )^{-1 } X^{T} y$$
而损失函数为 $J(\alpha) = (y - \alpha X)^{T}(y - \alpha X)$

Lasso 回归是采用 1 范数代替岭回归中添加的 2 范数:
$$\hat{\theta} = \text{arg}\min_{\theta} || y - X \theta ||^{2} + \lambda ||\theta||_{1}$$
对于向量, 有 $||\theta||_{1} = \sum_{i = 1}^{n}  |\theta_{i}|$ 

Lass 方法的优点是可以解决高维数据的稀疏性，采用 Lasso 方法可以将不重要的变量系数压缩为 0, 从而实现了**变量降维**和参数估计。

需要说明,  Lasso 最优解由于约束条件不可导，无法直接求解梯度; 因此常用
1. 坐标轴下降法
2. 最小角回归法
(具体参考[知乎文章](https://www.zhihu.com/collection/971827377))

### (4) 波士顿房价岭回归预测实战
要求: **对于 Boston 房价部分, 建立岭回归和 Lasso 回归模型, 同时绘制岭迹图, 根据  RMSE (root mean squared error) 和 R2 分数, 测试模型的性能**, 并且<mark style="background: transparent; color: red">利用交叉验证和正则参数调优</mark>来获取最优的岭回归正则化参数 $\lambda$. 

可以直接调用 `from sklearn.linear_model import Ridge` 建立岭回归模型, 而也可以采用, 并采用
`coef_ : ndarray of shape (n_features,) or (n_targets, n_features): Weight vector(s).`
参数直接获取 $A(\lambda)$, 即 $\alpha$，权重向量.

`coef_` 参数表征了各个 $\lambda$ 的取值是一个 8 大小的数组;

1. 岭迹图绘制: 取 $\lambda$ 的范围为 0-5000, 绘制出岭迹图如下所示, 说明对于该问题岭迹图有较好的现象, 可以采用最小二乘方法类似的进行回归分析。(一般地, 当系数变化平稳时，可选择对应的 $\lambda$ 作为候选值)
![[attachments/Pasted image 20241126011042.png|350]]
需要说明的是, 往往我们会以 logspace 测试参数 $\lambda$ 的选取. 
初步确定出我们需要的 $\lambda$ 范围大概在 1000-4000 之间,

2. 基于 RidgeCV 的交叉验证
对于 Python 中已经集成了 RidgeCV 模块便于交叉验证, 可以实现自动优化:
可以直接选择 mean_squared_error 等标准直接交叉验证(参考[scoring parameter](https://scikit-learn.org/dev/modules/model_evaluation.html#scoring-parameter)部分)

```python fold title:岭回归预测波士顿房价
import matplotlib.pyplot as plt  
import numpy as np  
import pandas as pd  
from sklearn.linear_model import Ridge, RidgeCV  
from sklearn.model_selection import train_test_split  
from sklearn.datasets import fetch_california_housing  
from sklearn.preprocessing import StandardScaler  
from sklearn.pipeline import Pipeline  
from sklearn.metrics import r2_score, root_mean_squared_error  
# RMSE , R2 score for evaluation  
  
class California_housing():  
    def __init__(self):  
        housing = fetch_california_housing(download_if_missing=True)  
        self.features = housing.feature_names  
        self.data = pd.DataFrame(housing.data, columns=self.features)  
        self.target = housing.target   # the housing price  
  
        self.model = Pipeline([  
            ('scalar', StandardScaler()),  
            ('ridge', Ridge(alpha=1.0, fit_intercept=True, max_iter=None, solver='svd')),  
            #  solver{‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’, ‘lbfgs’}, default=’auto’  
        ])  
        # use 80% training and 20% testing  
        x_train, x_test, y_train, y_test = train_test_split(self.data, self.target, test_size=0.2, random_state=42)  
  
        print("============= first predict: ================")  
        self.build_ridge_model(x_train, y_train, 1)  
        self.do_predict(x_test, y_test)  
  
        self.draw_ridge_figue(x_train, y_train, np.logspace(-3, 3, 100))  # from  -1000 to 1000 with 500 steps  
  
        best_lmd = self.search_best_param(1000, 4000, step= 1)  
        self.build_ridge_model(x_train, y_train, best_lmd)  
        print("============ optimized results: =================")  
        self.do_predict(x_test, y_test)  
        print("best lambda value: ", best_lmd)  
  
    def build_ridge_model(self, x_train, y_train, lmda = 1):  
        self.model.set_params(ridge__alpha=lmda) # set the lambda needed  
        self.model.fit(x_train, y_train)  
  
    def do_predict(self, x_test, y_test):  
        y_pred  = self.model.predict(x_test)  
        r2sc = r2_score(y_test, y_pred, sample_weight=None)  
        rmse = root_mean_squared_error(y_test, y_pred, sample_weight=None)  
        print("R2 score: ", r2sc)  
        print("RMSE: ", rmse)  
  
    def draw_ridge_figue(self, x_train, y_train, lmds):  
        w_arr = np.empty((len(self.features) , 0))  
        for i in range(len(lmds)):  
            l = lmds[i]  
            self.model.set_params(ridge__alpha=l)  
            self.model.fit(x_train, y_train)  
            w = np.array(self.model.named_steps['ridge'].coef_).reshape(1,-1)  
            w_arr = np.concatenate((w_arr, w.T), axis=1)  
        fig = plt.figure()  
        fig.add_subplot(111, title='Ridge Map')  
        plt.plot(lmds, w_arr.T)  
        plt.legend(self.features)  
        plt.show()  
  
    def search_best_param(self, lmd_min, lmd_max, step=0.2, k=5):  
        """  
        采用交叉验证方法, 直接获取最佳的 lambda 取值  
        :param lmds:        :return:  
        """        lmds = np.arange(lmd_min, lmd_max, step)  
        ridgeCV = RidgeCV(alphas=lmds, scoring='neg_mean_squared_error', cv=k)  # neg_mean_squared_error  
        ridgeCV.fit(self.data, self.target)  
        return ridgeCV.alpha_  
  
if __name__ == "__main__":  
    a = California_housing()
```

### (5) 交叉验证, 网格搜索和随机搜索寻优 
`````ad-note
title: 补充: 一般常用的交叉验证代码
collapse: open 
首先，在一般数据集中,  常用的是 **K 折交叉验证**，将数据分成 K 份，轮流使用每一份数据作为验证集，剩余数据作为训练集。

此外有留一法交叉验证(参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓深度学习算法原理(sklearn)/补充知识/5.SVM支持向量机|5.SVM支持向量机]])

对于 sklearn.cross_val_score 已经写好了交叉验证代码, 只需要使用 `cv` 参数指定 k 折交叉验证即可, 如下:
```python
from sklearn.model_selection import cross_val_score
# 交叉验证
cv_scores = []
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    score = np.mean(cross_val_score(ridge, X, y, cv=10, scoring='neg_mean_squared_error'))
    cv_scores.append(score)

best_alpha_cv = alphas[np.argmax(cv_scores)]
print(f"最佳正则化参数（交叉验证）：{best_alpha_cv}")
```
`````

随机森林分类算法(参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/2. 决策树和回归树|决策树]]和[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓机器学习算法(sklearn)/补充知识/8. 随机森林|8. 随机森林]])部分, 网格搜索方法和随机搜索算法采用 `from sklearn.model_selection import GridSearchCV， RandomizedSearchCV  `, 参考[sklearn超参数优化部分](https://scikit-learn.org/dev/api/sklearn.model_selection.html#module-sklearn.model_selection) 
![[attachments/Pasted image 20241126145507.png|550]]

```python
from sklearn.datasets import load_iris  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.model_selection import GridSearchCV  

# 加载数据  
data = load_iris()  
X, y = data.data, data.target  

# 定义模型和参数网格  
model = RandomForestClassifier()  
param_grid = {  
    'n_estimators': [10, 50, 100],  
    'max_depth': [None, 10, 20]  
}  

# 网格搜索  
grid_search = GridSearchCV(model, param_grid, cv=5)  
grid_search.fit(X, y)  

# 输出最佳参数  
print("最佳参数:", grid_search.best_params_)
```


```python
from sklearn.datasets import load_iris  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.model_selection import RandomizedSearchCV  
from scipy.stats import randint  

# 加载数据  
data = load_iris()  
X, y = data.data, data.target  

# 定义模型和参数分布  
model = RandomForestClassifier()  
param_distributions = {  
    'n_estimators': randint(10, 200),  
    'max_depth': [None, 10, 20, 30]  
}  

# 随机搜索  
random_search = RandomizedSearchCV(model, param_distributions, n_iter=10, cv=5, random_state=42)  
random_search.fit(X, y)  

# 输出最佳参数  
print("最佳参数:", random_search.best_params_)
```
