## ä¸€ã€çº¿æ€§ç³»ç»ŸåŠå…¶é¢„æµ‹
é¢„æµ‹ä¸­çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯<mark style="background: transparent; color: red">å›å½’é—®é¢˜</mark>(Regression), å›å½’åæ˜ äº†ç³»ç»Ÿçš„éšæœºè¿åŠ¨æ€»æ˜¯è¶‹å‘äºå…¶æ•´ä½“è¿åŠ¨è§„å¾‹çš„è¶‹åŠ¿ã€‚
### (1) æœ€å°äºŒä¹˜å›å½’
#### 1. åŸºæœ¬æ¨¡å‹
é¦–å…ˆ, æˆ‘ä»¬çš„åŸºæœ¬æ¨¡å‹æ˜¯
$$Y= a X + b$$
å…¶ä¸­ a,b ä¸ºå¸¸æ•°; å–æ®‹å·®
$$\xi_{i} = y_{i}- ax_{i} - b$$
åˆ™é‡‡ç”¨æ®‹å·®çš„å¹³æ–¹å’Œ:
$$Q = \sum_{i = 1}^{n} \xi_{i} ^{2} \overset{\min}{\longrightarrow} \frac{\partial Q}{\partial a} = \frac{\partial Q}{\partial b} = 0 $$
å¾—åˆ°:
$$\begin{cases}
\sum_{i = 1}^{n}  ( x_{i}y_{i} - ax_{i}^{2} - bx_{i}) = 0  \\
\sum_{i = 1}^{n}  (y_{i} - a x_{i} - b ) = 0  
\end{cases}$$
æ­¤æ—¶, å°† $bx_i$ ç§»åŠ¨åˆ°å³ä¾§è§£å‡º $a$, æœ‰: 
$$\sum_{i = 1}^{n} (x_{i}y_{i} - ax_{i} ^{2} ) =  n * b \overline{x} =  \sum_{i = 1}^{n} \overline{x} (y_{i} - a x_{i})$$
å¾—åˆ°<b><mark style="background: transparent; color: blue">æœ€å°äºŒä¹˜å…¬å¼</mark></b>:
$$\boxed{a = \frac{\sum_{i = 1}^{n} x_{i} y_{i} - n\overline{x} \overline{y}}{\sum_{i = 1}^{n}x_{i}^{2} - n \overline{x}^{2}}\qquad b = \overline{y} - a \overline{x} }$$
#### 2. æ­£è§„æ–¹ç¨‹ç»„
ä¸€èˆ¬çš„æ–¹æ³•å‚è€ƒ [MATLABæœ€å°äºŒä¹˜å¤šé¡¹å¼æ‹Ÿåˆ](https://mp.weixin.qq.com/s?__biz=Mzg2MDY5MTY4NA==&mid=2247484025&idx=1&sn=0b580f3a94bf79d0c148e0d783ecc30d&chksm=ce23cff2f95446e428a8b6c4289d6167d5f199235b628242efc78ca82a47ffdf8111a0ab8582&token=777628491#rd), å¯¹äºå®é™…çš„æ ·æœ¬ç‚¹, æˆ‘ä»¬æœ‰: 
$$X = \left[\begin{matrix}
x_{11} & \dots   & x_{1n} \\  
x_{21} & \dots & x_{2n}  \\ 
\vdots   \\ && x_{mn}
\end{matrix}\right]$$
è€Œæœ‰: $y_1 = a_{1} x_{11} + a_2 x_{12} + \dots +  a_n x_{1n} + b_{i}$, åˆ™å– $a = (a_{1}, \dots  a_{n})^T$, å¾—åˆ° $Y = Xa + b$, æ­¤æ—¶æˆ‘ä»¬å°† $b$ åˆå¹¶åˆ° $X$ ç¬¬ä¸€åˆ—å¾—åˆ°:
$$Y =   X\boldsymbol{a} \quad  \rightarrow \quad   X^{T} Y = X^{T } X \boldsymbol{a}$$
æ˜¾ç„¶ä¸ºäº†æ–¹ç¨‹ç»„æœ‰è§£, **å¯¹ç§°çŸ©é˜µ $X^{T} X$ å¿…é¡»éå¥‡å¼‚**; å¹¶æœ‰
$$A = (X^{T}  X)^{-1}  (X^{T}Y)$$
ä¸º<mark style="background: transparent; color: red">æ–¹ç¨‹ç»„çš„æœ€å°äºŒä¹˜è§£</mark>. åŒæ—¶, ä¸€èˆ¬é‡‡ç”¨[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/ğŸ£Probability Theory/ç¬¬å››ç«  éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾#ä¸‰ã€åæ–¹å·®åŠå…¶ç›¸å…³ç³»æ•°|ç›¸å…³ç³»æ•°]]æ–¹æ³•è¿›è¡Œè¯„ä¼°, è¡¡é‡çº¿æ€§ç¨‹åº¦.

### (2) å¾„å‘åŸºå‡½æ•°ç½‘ç»œ
RBF (Radius Basis Function)ç½‘ç»œå¯ä»¥<b><mark style="background: transparent; color: orange">ä»¥ä»»æ„ç²¾åº¦é€¼è¿‘ä»»æ„è¿ç»­æˆ–è€…ç¦»æ•£å‡½æ•°  å¹¶ä¸”ç‰¹åˆ«é€‚åˆè¿›è¡Œéçº¿æ€§é¢„æµ‹å’Œåˆ†ç±»ç­‰ç­‰é—®é¢˜</mark></b>ã€‚åŸºæœ¬æ¶æ„å¦‚ä¸‹:
![[Excalidraw/6. é¢„æµ‹å›å½’åˆæ­¥å’Œ RBF ç½‘ç»œ 2024-11-25 11.24.01|450]]
å…¶ä¸­<mark style="background: transparent; color: red">æ¿€æ´»å‡½æ•°ä¸º</mark>é«˜æ–¯æ ¸å‡½æ•°(æ­£æ€åˆ†å¸ƒå‡½æ•°), å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/8. SVMæ”¯æŒå‘é‡æœº#3. å¾„å‘åŸºå‡½æ•°æ ¸å‡½æ•°(é«˜æ–¯æ ¸)|5.SVMæ”¯æŒå‘é‡æœº]]:
$$ W(i,j) =  \exp \left(- \frac{1}{2\sigma^{2} }||x  - y||^{2}\right)\quad (j = 1,2, \dots   n)$$
å¯¹äºå¾„å‘åŸºå‡½æ•°è€Œè¨€ï¼Œå¸¸è§æ–¹æ³•æœ‰ç²¾ç¡®æ±‚è§£ï¼ŒOLSæ±‚è§£å’ŒKMeansèšç±»æ–¹æ³•ï¼Œ å…¶Matlabä¸­çš„å»ºç«‹æä¸ºå®¹æ˜“:
```Matlab  
newrbeÂ Â Â Â %ç²¾ç¡®æ±‚è§£  
neerbÂ Â Â Â Â Â %OLSæ–¹æ³•  
``` 
éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå¯¹äºç²¾ç¡®æ±‚è§£æ–¹æ³•ï¼Œæœ‰nä¸ªæ ·æœ¬ç‚¹å°±æœ‰nä¸ªå¾„å‘åŸºå‡½æ•°ï¼Œä¼šå¯¼è‡´æå¤§å¤æ‚æ€§å¢åŠ ã€‚åœ¨matlabä¸­ï¼Œå¯ä»¥ä½¿ç”¨å®šä¹‰è¯¯å·®çš„æ–¹æ³•ï¼Œè€Œç½‘ç»œè¯¯å·®æ˜¯éšç€å¾„å‘åŸºå¢åŠ è€Œä¸‹é™çš„è¿‡ç¨‹ã€‚

å…¶ä¸€èˆ¬åŸç†æ˜¯å…ˆè®¡ç®—æ¬§å¼è·ç¦»å† RBF æ‹Ÿåˆçš„æ–¹æ³•; æƒå€¼è°ƒæ•´æ˜¯é€šè¿‡æ¿€æ´»å‡½æ•°å®ç°çš„, è€Œä¸é‡‡ç”¨åå‘ä¼ æ’­æˆ–è€…[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/5. BP ç¥ç»ç½‘ç»œ, SOM ç¥ç»ç½‘ç»œå’Œ Boltzmannæœº|5. BP ç¥ç»ç½‘ç»œ, SOM ç¥ç»ç½‘ç»œå’Œ Boltzmannæœº|BP ç¥ç»ç½‘ç»œ]]ç±»ä¼¼çš„è¯¯å·®åé¦ˆæƒå€¼æ›´æ–°ã€‚

#### 1. è®­ç»ƒè¿‡ç¨‹
é¦–å…ˆ, å¾„å‘åŸºå‡½æ•°ä»ç„¶æ˜¯ä»¥ [[#2. æ­£è§„æ–¹ç¨‹ç»„]] æ€è·¯, å»ºç«‹æ–¹ç¨‹
$$Y  = X\boldsymbol{a}  \quad  \overset{}{\longrightarrow}  X^{T} X \boldsymbol{a}  = X^{T} Y\overset{}{\longrightarrow} a = (X^{T} X)^{-1 } X^{T} Y $$
æˆ‘ä»¬å°†ç³»æ•°å‘é‡ $a$ ç”¨ $w$ è¡¨ç¤º, æˆä¸º
$$w = (X^{T} X^{-1})X^{T}  Y$$
<mark style="background: transparent; color: red">æ ¸æ–¹æ³•å°†ä½ç»´ç©ºé—´ä¸­çš„æ•°æ®çŸ©é˜µ  X æ˜ å°„åˆ° æ–°çš„è®¾è®¡çŸ©é˜µ K, è€Œæ–°çš„è®¾è®¡çŸ©é˜µ K å®é™…ä¸Šæ˜¯é€šè¿‡é‡‡ç”¨ M ä¸ªåŸºå‡½æ•°æ„é€ çš„</mark> 
$$K_{ij} = \phi\left(\frac{||x_{i} - m_{j}||}{\sigma}\right) \qquad \overset{K æ›¿ä»£X, w æ›¿ä»£a}{\longrightarrow}  \qquad K w = y$$

é¦–å…ˆ, è®¾æ ·æœ¬çŸ©é˜µ i, j è¡Œåˆ†åˆ«ä¸º  $x_i, x_j$ åˆ™<b><mark style="background: transparent; color: orange">å¾„å‘åŸºå‡½æ•°æ’å€¼åŠæ³•ä¼šè®¡ç®—ä¸¤è€…è·ç¦»</mark></b>, å¹¶**å½¢æˆä¸€ä¸ª $m \times m$ çš„è·ç¦»çŸ©é˜µ K**:
$$d_{ij} = x_{i} -  x_{j}$$
æˆ‘ä»¬**å–æ’å€¼çŸ©é˜µ K $(m \times m çŸ©é˜µ)$ ä¸ºé«˜æ–¯æ ¸å‡½æ•°**:
$$K_{i,j}  = \exp \left( -  \frac{d_{i,j}  \times  d_{i,j}^{T}}{2 \sigma^{2}} \right) = \exp \left(- \frac{||x_{i} -  x_{j}||^{2}}{2 \sigma^{2}}\right)$$
æ­¤æ—¶ K å·²ç»æ±‚å‡º, å…¶ä¸­ $K$ è¡¨ç¤ºæŸä¸ªè¾“å…¥ç‚¹ $x_i$ åˆ° $x_j$ çš„ç›¸ä¼¼æ€§, $w$ æ˜¯ä¸€ä¸ª $m \times 1$ çš„å‘é‡, è¡¨ç¤ºæƒé‡ã€‚æœ‰
$$y= K w \qquad \overset{åˆ†é‡}{\longrightarrow}\qquad   y_{i} = \sum_{j = 1}^{m} K_{i,j} w_{j} $$
å³å¯è§£å‡ºå¯¹åº”çš„å‘é‡ $w$ 

#### 2. é¢„æµ‹è¿‡ç¨‹
åˆ™**æœ€ç»ˆçš„é¢„æµ‹æ–¹æ³•, é¦–å…ˆéœ€è¦ <b><mark style="background: transparent; color: orange">è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒç‚¹çš„æ ¸å‡½æ•°</mark></b>**,  è®¾æ–°çš„å‘é‡ä¸º $x$, åˆ™å…ˆè®¡ç®—å’Œæ¯ä¸ªè®­ç»ƒç‚¹çš„è·ç¦», ä»ç„¶å– $y = K w$ ç„¶åå°†è·ç¦» $K$ ä¹˜ $w$ å¾—åˆ°:
$$\boxed{\Large   y = \sum_{i = 1}^{m}  \exp \left( -   \frac{||x - p_{i} ||^{2}}{2 \sigma^{2}}\right)  \times w}$$
å…¶ä¸­ $p_i$ ä¸ºä¸Šè¿°æ‰€æœ‰çš„è®­ç»ƒç‚¹ã€‚

### (3) RBF ç¥ç»ç½‘ç»œä»£ç å®ç°
ä¸‹é¢çš„ä»£ç é‡‡ç”¨ RBF çš„æ’å€¼æ–¹å¼, å®ç°äº†ç»å…¸ matlab å‡½æ•° peaks çš„éšæœº 50 ç‚¹ æ’å€¼ç»˜åˆ¶:

**éœ€è¦æ³¨æ„çš„æ˜¯, å…¶ä¸­ rbf_fun éƒ¨åˆ†, é‡‡ç”¨çŸ©é˜µæ–¹æ³•** `np.exp(- 1 / (2 * sigma ** 2) * np.linalg.norm(d, axis=-1) ** 2) ` é€‚é…äº†å¤šä¸ªè·ç¦»å‘é‡æƒ…å†µä¸‹çš„è·ç¦»è®¡ç®—ã€‚å®ç°äº†**åœ¨é¢„æµ‹æ—¶èƒ½å¤Ÿå¿«é€Ÿè®¡ç®—ä¸å…¶ä»–å„ä¸ªè®­ç»ƒç‚¹çš„è·ç¦»å‘é‡**

```python fold title:RBFå¾„å‘åŸºå‡½æ•°æ’å€¼çš„pythonå®ç°
import numpy as np  
import sys  
import os  
import matplotlib.pyplot as plt  
from  mpl_toolkits.mplot3d import Axes3D  
from scipy.interpolate import griddata  
  
os.chdir(os.path.dirname(os.path.abspath(__file__)))  
  
mu = 0.02  
k  = 0.03  
  
def activate_fun(x,W,c):  
    y = np.sum(np.exp(-1/(2 * sigma **2) * np.linalg.norm(x-c)**2))  
  
class RBF_test():  
    def __init__(self, x,y):  
        self.x = x  
        self.y = y  
        if np.ndim(x) == 2:  
            self.m, self.n = x.shape  
        else:  
            raise ValueError("x must be 2D array")  
  
    def train(self):  
        K = np.zeros((self.m, self.m))  
        for i in range(self.m):  
            for j in range(self.m):  
                d  = self.x[i,:] - self.x[j,:]   # è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»å‘é‡  
                K[i,j] = self.rbf_fun(d)      # æƒé‡çŸ©é˜µçš„æ¯ä¸€é¡¹æŒ‰ç…§é«˜æ–¯å‡½æ•°è®¡ç®—;  
        self.w = np.linalg.solve(K, self.y)  
  
    def predict(self, x):  
        d = (self.x - x)  # è®¡ç®—è¾“å…¥å‘é‡ä¸æ‰€æœ‰è®­ç»ƒé›†å‘é‡ä¹‹é—´çš„è·ç¦»å‘é‡  
        y = np.sum(self.rbf_fun(d) @ self.w)        # è®¡ç®—é¢„æµ‹å€¼  
        return y  
  
    def rbf_fun(self, d, sigma = 1.0):  
        # å¯¹äº d ä¸ºçŸ©é˜µæ—¶, å®é™…ä¸Šä¸æ˜¯ d @  d.T, è€Œæ˜¯é‡‡ç”¨ np.linalg.norm        return np.exp(- 1 / (2 * sigma ** 2) * np.linalg.norm(d, axis=-1) ** 2)  # é‡‡ç”¨ axis = -1 è¡¨ç¤ºå¯¹æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œè®¡ç®—  
  
  
def peaks(x, y):  
    term1 = 3 * (1 - x) ** 2 * np.exp(-x ** 2 - (y + 1) ** 2)  
    term2 = -10 * (x / 5 - x ** 3 - y ** 5) * np.exp(-x ** 2 - y ** 2)  
    term3 = -1 / 3 * np.exp(-(x + 1) ** 2 - y ** 2)  
    return term1 + term2 + term3  
  
  
if __name__ == '__main__':  
    # sample 50 points  
    x1 = np.random.rand(50) * 6 - 3  
    x2 = np.random.rand(50) * 6 - 3  
    X  = np.array([x1, x2]).T  
    Y = peaks(x1, x2)    #  np.sin(np.sqrt(x1 ** 2 + x2 ** 2)) *  np.sqrt(x1 ** 2 + x2 ** 2)  
    rt = RBF_test(X, Y)  
    rt.train()  
  
    # ç”Ÿæˆç½‘æ ¼ç‚¹  
    x_grid = np.linspace(-3, 3, 100)  
    y_grid = np.linspace(-3, 3, 100)  
    x_mesh, y_mesh = np.meshgrid(x_grid, y_grid)  
    z_mesh = np.zeros_like(x_mesh)  
  
    X_input = np.array([np.hstack(x_mesh), np.hstack(y_mesh)]).T  # å°†ç½‘æ ¼ç‚¹è½¬æ¢ä¸ºè¾“å…¥ X çŸ©é˜µ  
  
    for i in range (x_mesh.shape[0]):  
        for j in range(x_mesh.shape[1]):  
            z_mesh[i,j] = rt.predict([x_mesh[i,j], y_mesh[i,j]])  
    fig = plt.figure(figsize=(10, 7))  
    ax = fig.add_subplot(1, 2, 1, projection='3d')  
    ax.plot_surface(x_mesh, y_mesh, z_mesh, cmap='viridis')  
  
    # ç»˜åˆ¶åŸå§‹å‡½æ•°çš„æ’å€¼æ˜¾ç¤º  
    ax2 = fig.add_subplot(1, 2, 2, projection='3d')  
    z_mesh = griddata(X, Y, (x_mesh, y_mesh), method='linear') # æ’å€¼ï¼ˆçº¿æ€§ï¼‰  
    ax2.plot_surface(x_mesh, y_mesh, z_mesh, cmap='viridis')  
    plt.show()
```

è·å¾—å¦‚ä¸‹çš„æ’å€¼å›¾åƒ (ç¬¬ä¸€å¼ æ˜¯æ’å€¼ç»“æœ)
![[attachments/Pasted image 20241125171319.png|450]]

å¯¹äº Python ä¸­çš„ Scipy æ¨¡å—å·²ç»æœ‰ `scipy.interpolate.Rbf` å¯ä»¥ç›´æ¥è¿›è¡Œæ’å€¼å’Œæ•°æ®æ‹Ÿåˆã€‚å› æ­¤ä¸Šè¿°ä»£ç å¯ä»¥ç®€åŒ–ä¸º:  
```python
from scipy.interpolate import Rbf
import numpy as np
import matplotlib.pyplot as plt

# å®šä¹‰æ ·æœ¬ç‚¹
x1 = np.random.rand(50) * 6 - 3
x2 = np.random.rand(50) * 6 - 3
X = np.array([x1, x2]).T
Y = peaks(x1, x2)  # åŸå§‹æ•°æ®çš„å€¼

# ä½¿ç”¨ Rbf ç±»è¿›è¡Œæ’å€¼
rbf = Rbf(x1, x2, Y, function='gaussian', epsilon=1.0)  # é»˜è®¤ä½¿ç”¨é«˜æ–¯æ ¸

# ç”Ÿæˆç½‘æ ¼ç‚¹
x_grid = np.linspace(-3, 3, 100)
y_grid = np.linspace(-3, 3, 100)
x_mesh, y_mesh = np.meshgrid(x_grid, y_grid)

# é¢„æµ‹ç½‘æ ¼ç‚¹ä¸Šçš„å€¼
z_mesh = rbf(x_mesh, y_mesh)

# ç»˜åˆ¶ç»“æœ
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.plot_surface(x_mesh, y_mesh, z_mesh, cmap='viridis')
plt.show()
```

å¸¸ç”¨çš„æ ¸å‡½æ•°æœ‰ `multiquadric`, `inverse`, `gaussian`, `linear` ç­‰ 


## äºŒã€å²­å›å½’å’Œ Lasso å›å½’
### (1) å¤šé‡å…±çº¿æ€§çš„æ¦‚å¿µä¸éªŒè¯
åœ¨æœ€å°äºŒä¹˜æ³•å’Œ RBF é«˜æ–¯æ ¸å‡½æ•°é¢„æµ‹çš„, **æœ‰ä¸€ä¸ªé™åˆ¶è¦æ±‚**, å³æ ·æœ¬çŸ©é˜µçš„è¡Œåˆ—å¼ä¸èƒ½ä¸º0, æˆ–è€…<b><mark style="background: transparent; color: blue">æ ·æœ¬çš„å„ä¸ªç‰¹å¾ä¹‹é—´ä¸è¡¨ç°ä¸ºå¼ºçƒˆçš„çº¿æ€§ç›¸å…³æ€§</mark></b>, **å¹¶å°†æ­¤ç§°ä¸ºå¤šé‡å…±çº¿æ€§**. è¿™ç§ä¼šå¯¼è‡´æ¨¡å‹ä¼°è®¡å¤±çœŸ, è€Œå¯¹äºå¤§å¤šæ•°æ•°æ®, æœ‰å¯èƒ½æœ‰è¿‘ä¼¼å…±çº¿æ€§å‡ºç°ã€‚

å²­å›å½’(Ridge Regression)æ˜¯ä¸€ç§æœ€å°äºŒä¹˜çš„æ”¹è¿›ç­–ç•¥,  æ˜¯å¯¹äºçº¿æ€§ç³»ç»Ÿå¸¸ç”¨çš„ä¼°è®¡æ–¹å¼ã€‚
é¦–å…ˆ, æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µå¾—åˆ°æ¯ä¸€åˆ—çš„ç›¸å…³æ€§ã€‚è¿™ä¸ªå¯ä»¥é€šè¿‡ np.corrcoef å¾—åˆ° Pearson ç›¸å…³ç³»æ•° 
```python
import numpy as np
data = np.random.rand(10, 2) # éšæœºç”Ÿæˆ 10x2 æ•°æ®
correlation_matrix = np.corrcoef(data, rowvar=True)
# å¾—åˆ°æ¯ä¸€è¡Œçš„å‘é‡
np.linalg.det(x.T @ x)  # è®¡ç®—å…±çº¿æ€§ç¨‹åº¦
```

é¦–å…ˆ, çŸ©é˜µçš„éšæœºå˜é‡ä¹‹é—´å­˜åœ¨å¤šé‡å…±çº¿æ€§, å³æœ‰:
$$\left| X^{T}  X\right| \approx 0 $$
æˆ‘ä»¬åŠ ä¸Šä¸€ä¸ªå‚æ•°çŸ©é˜µ $kI$, æ­¤æ—¶ $X^{T}  X+ KI$ çš„å¥‡å¼‚ç¨‹åº¦æ˜¾ç„¶ä¼šæ˜¾è‘—å‡å°, åˆ™æ ‡å‡†åŒ–åä»ç„¶é‡‡ç”¨ $X$ è¡¨ç¤º, å¹¶å®šä¹‰å¯¹äºåŸå§‹æ–¹ç¨‹:
$$Y = X \alpha $$
å– $\alpha$ çš„è¿‘ä¼¼:
$$\Large\boxed{\alpha(k) = (X^{T} X +  \lambda I)^{-1}  X^{T }Y}$$
ç§°ä¸º $\alpha(k)$ çš„å²­å›å½’ä¼°è®¡; å¹¶ç§°**å…¶ä¸­ $\lambda$ ä¸ºå²­å‚æ•°**;

`````ad-note
title: è¡¥å……: å²­å›å½’å’Œå¤å…¸å›å½’çš„è¡¨è¾¾ä»¥åŠç›¸å…³æ¨å¯¼
collapse: open
ç›¸è¾ƒäºå¤å…¸å›å½’çš„æŸå¤±å‡½æ•°, æœ‰ $Y= X \theta$, è®¤ä¸ºæŸå¤±å‡½æ•°ä¸º:
$$\hat{\theta} = \arg\min_{\theta} \left\| Y - X\theta \right\|_2^2$$
å…¶ä¸­, $\theta$ ä¸ºè¡¨ç¤ºæƒé‡çš„å‚å˜é‡

ç›¸æ¯”äºå¤å…¸å›å½’, <b><mark style="background: transparent; color: orange">å²­å›å½’</mark></b>é€šè¿‡åœ¨**æ™®é€šæœ€å°äºŒä¹˜æ³•çš„ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹æ¥çº¦æŸå›å½’ç³»æ•°**ã€‚æ­£åˆ™åŒ–é¡¹é€šå¸¸æ˜¯ 1 èŒƒæ•°æˆ–è€… 2 èŒƒæ•° (æ­¤å¤„å–ä¸º2èŒƒæ•°å¹³æ–¹)ï¼Œæ—¨åœ¨**ç¼©å°å›å½’ç³»æ•°çš„ç»å¯¹å€¼ï¼Œä»è€Œæé«˜æ¨¡å‹çš„ç¨³å®šæ€§**ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å¤šé‡å…±çº¿æ€§çš„æƒ…å†µä¸‹ã€‚

å²­å›å½’å¯¹åº”çš„ç›®æ ‡å‡½æ•°å¦‚ä¸‹:
$$\hat{\theta} = \arg\min_{\theta} \left( \left\| Y - X\theta \right\|_2^2 + \lambda \left\| \theta\right\|_2^2 \right)$$

å±•å¼€åˆå¹¶, åˆ™ $J(\theta)= \hat{\theta}$ ä¸ºæˆ‘ä»¬çš„ç›®æ ‡å‡½æ•° :
$$\hat{\theta} = \text{arg}\min_{\theta} (Y - X\theta)^{T} (Y - X\theta) = Y^{T} Y - 2 \theta^{T} X^{T} Y + \theta^{T} X^{T} X \theta + \lambda \theta^{T}  \theta = J(\theta)$$
æ­¤æ—¶, å¯¹ç›®æ ‡å‡½æ•°æ±‚æ¢¯åº¦,  å˜ä¸º 
$$\nabla_{\theta} J(\theta) = -2 v^{T}\cdot X^{T}Y  + 2 v^{T} X^{T} X \theta +  2 \lambda v^{T} \theta$$
å– $\nabla_{\theta} = 0$ å…¶ä¸­ $v = (1,1,1,\dots 1)^T$, åˆ™æœ€å°åŒ–æ—¶, å°† $v^{T}$ æå‡º,  æœ‰:
$$X^{T } Y =  X^{T} X \theta  + \lambda I \theta$$
å®¹æ˜“å¾—åˆ°æœ€å°äºŒä¹˜çš„è§’åº¦æ¥çœ‹çš„æœ€å°å€¼å¯¹åº”$\theta$å…¬å¼: 
$$\theta =  (X^{T} X +  \lambda I)^{-1} X ^{T} Y$$
å› æ­¤ä¸Šé¢çš„ä¸¤ç§è¡¨è¾¾å½¢å¼æ˜¯ç­‰ä»·çš„ã€‚
`````

### (2) å²­è¿¹åˆ†æ
é¦–å…ˆ, å‡è®¾ X å·²ç»æ ‡å‡†åŒ–, åˆ™ $X^T X$ ä¸ºè‡ªå˜é‡æ ·æœ¬ç›¸å…³é˜µ, <b><mark style="background: transparent; color: orange">å¦‚æœ Y ä¹Ÿæ ‡å‡†åŒ–è¿‡, åˆ™è®¡ç®—ç»“æœæ˜¯æ ‡å‡†åŒ–åçš„å²­å›å½’ä¼°è®¡</mark></b>ã€‚

æ˜¾ç„¶ï¼Œ$A(\lambda)$ æ˜¯ $\lambda$ çš„å‡½æ•°, è€Œ $A(\lambda)$ éš  çš„è½¨è¿¹ç§°ä¸ºå²­è¿¹, éœ€è¦è¯´æ˜, $A (\lambda)$ æ˜¯å›å½’å‚æ•° A çš„æœ‰åä¼°è®¡ 
![[Excalidraw/6. é¢„æµ‹å›å½’åˆæ­¥å’Œ RBF ç½‘ç»œ 2024-11-25 18.44.39|450]]

å®é™…ä¸Š $A(\lambda)$ ä½œä¸º A çš„æœ‰åä¼°è®¡æ¯”æœ€å°äºŒä¹˜ä¼°è®¡æ›´åŠ ç¨³å®šã€‚

å¯¹äº**å›¾1  æ‰€ç¤ºçš„å²­è¿¹**, å½“å²­è¿¹æœ‰æ˜¾è‘—çš„ä¸Šå‡æˆ–è€…ä¸‹é™æ—¶, åˆ™ $X^{T} X$ å¯¹ $Y$ çš„å½±å“æ˜¯æ¯”è¾ƒå¤§çš„, åªè¦åœ¨å…¶ä¸Šé™„åŠ ä¸€å®šéƒ¨åˆ†, å°±ä¼šäº§ç”Ÿæ˜¾è‘—å½±å“(ä¸»è¦æŒ‡æ­£è´Ÿæ”¹å˜)ã€‚

ä¾‹å¦‚å›¾4ä¸­, $A_1(\lambda)$ å’Œ $A_{2}(\lambda)$, è™½ç„¶ $A_2(\lambda)$ ä¸ç¨³å®š, è€Œæ€»ä½“ä¸Šæ˜¯ç¨³å®šçš„, <b><mark style="background: transparent; color: orange">è¿™ç§æƒ…å†µå¾€å¾€å‡ºç°äºç›¸å…³æ€§å¾ˆå¤§çš„åœºåˆ, æ­¤æ—¶ä»å˜é‡é€‰æ‹©è§’åº¦è€Œè¨€, A, B ä¿ç•™ä¸€ä¸ªå°±å¤Ÿäº†</mark></b>ã€‚(å¾€å¾€æ˜¯ç¬¦å·ä¸åˆç†é€ æˆçš„)

å¦‚æœå°†æŸä¸€å…·ä½“å®ä¾‹ä¸­, åˆ¤æ–­æœ€å°äºŒä¹˜æ˜¯å¦é€‚ç”¨,  æˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰çš„å›å½’ç³»æ•°å²­è¿¹ç”»å‡º, é’ˆå¯¹äºå›¾ 5 ä¸­, åˆ™å¯èƒ½æœ€å°äºŒä¹˜æ˜¯ä¸é€‚ç”¨çš„; è€Œé’ˆå¯¹å›¾ 6, æœ€å°äºŒä¹˜ä¸€èˆ¬æ˜¯é€‚ç”¨çš„ã€‚

ä¸€èˆ¬çš„ $\lambda$ å€¼é€‰å–åŸåˆ™: 
1. æ‰€æœ‰å›å½’ç³»æ•°çš„å²­ä¼°è®¡éƒ½æ˜¯è¶‹å‘äºç¨³å®šçš„;
2. æœ€å°äºŒä¹˜ä¼°è®¡ä¸­ï¼Œ ç¬¦å·ä¸åˆç†çš„å›å½’ç³»æ•°, åœ¨å²­å›å½’ä¸­å˜ä¸ºåˆç†
3. æ®‹å·®å¹³æ–¹å’Œå¢å¤§ä¸å¤ªå¤š 
4. å¯å‰”é™¤è¾ƒå°æˆ–è€…è¶‹äºé›¶çš„ä¸åˆç† $\lambda$ å€¼

æœ€å¸¸ç”¨çš„åŒ…æ‹¬ **==Horel-Kennard å…¬å¼, å²­è¿¹æ³•å’Œäº¤å‰éªŒè¯æ³•==** 

### (3) Lasso å›å½’æ¨¡å‹
å¯¹äºçº¿æ€§æ¨¡å‹, ä¸Šé¢æˆ‘ä»¬å·²ç»æ¨å¯¼å‡º:
$$\alpha = (X^{T} X )^{-1 } X^{T} y$$
è€ŒæŸå¤±å‡½æ•°ä¸º $J(\alpha) = (y - \alpha X)^{T}(y - \alpha X)$

Lasso å›å½’æ˜¯é‡‡ç”¨ 1 èŒƒæ•°ä»£æ›¿å²­å›å½’ä¸­æ·»åŠ çš„ 2 èŒƒæ•°:
$$\hat{\theta} = \text{arg}\min_{\theta} || y - X \theta ||^{2} + \lambda ||\theta||_{1}$$
å¯¹äºå‘é‡, æœ‰ $||\theta||_{1} = \sum_{i = 1}^{n}  |\theta_{i}|$ 

Lass æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯å¯ä»¥è§£å†³é«˜ç»´æ•°æ®çš„ç¨€ç–æ€§ï¼Œé‡‡ç”¨ Lasso æ–¹æ³•å¯ä»¥å°†ä¸é‡è¦çš„å˜é‡ç³»æ•°å‹ç¼©ä¸º 0, ä»è€Œå®ç°äº†**å˜é‡é™ç»´**å’Œå‚æ•°ä¼°è®¡ã€‚

éœ€è¦è¯´æ˜,  Lasso æœ€ä¼˜è§£ç”±äºçº¦æŸæ¡ä»¶ä¸å¯å¯¼ï¼Œæ— æ³•ç›´æ¥æ±‚è§£æ¢¯åº¦; å› æ­¤å¸¸ç”¨
1. åæ ‡è½´ä¸‹é™æ³•
2. æœ€å°è§’å›å½’æ³•
(å…·ä½“å‚è€ƒ[çŸ¥ä¹æ–‡ç« ](https://www.zhihu.com/collection/971827377))

### (4) æ³¢å£«é¡¿æˆ¿ä»·å²­å›å½’é¢„æµ‹å®æˆ˜
è¦æ±‚: **å¯¹äº Boston æˆ¿ä»·éƒ¨åˆ†, å»ºç«‹å²­å›å½’å’Œ Lasso å›å½’æ¨¡å‹, åŒæ—¶ç»˜åˆ¶å²­è¿¹å›¾, æ ¹æ®  RMSE (root mean squared error) å’Œ R2 åˆ†æ•°, æµ‹è¯•æ¨¡å‹çš„æ€§èƒ½**, å¹¶ä¸”<mark style="background: transparent; color: red">åˆ©ç”¨äº¤å‰éªŒè¯å’Œæ­£åˆ™å‚æ•°è°ƒä¼˜</mark>æ¥è·å–æœ€ä¼˜çš„å²­å›å½’æ­£åˆ™åŒ–å‚æ•° $\lambda$. 

å¯ä»¥ç›´æ¥è°ƒç”¨ `from sklearn.linear_model import Ridge` å»ºç«‹å²­å›å½’æ¨¡å‹, è€Œä¹Ÿå¯ä»¥é‡‡ç”¨, å¹¶é‡‡ç”¨
`coef_ : ndarray of shape (n_features,) or (n_targets, n_features): Weight vector(s).`
å‚æ•°ç›´æ¥è·å– $A(\lambda)$, å³ $\alpha$ï¼Œæƒé‡å‘é‡.

`coef_` å‚æ•°è¡¨å¾äº†å„ä¸ª $\lambda$ çš„å–å€¼æ˜¯ä¸€ä¸ª 8 å¤§å°çš„æ•°ç»„;

1. å²­è¿¹å›¾ç»˜åˆ¶: å– $\lambda$ çš„èŒƒå›´ä¸º 0-5000, ç»˜åˆ¶å‡ºå²­è¿¹å›¾å¦‚ä¸‹æ‰€ç¤º, è¯´æ˜å¯¹äºè¯¥é—®é¢˜å²­è¿¹å›¾æœ‰è¾ƒå¥½çš„ç°è±¡, å¯ä»¥é‡‡ç”¨æœ€å°äºŒä¹˜æ–¹æ³•ç±»ä¼¼çš„è¿›è¡Œå›å½’åˆ†æã€‚(ä¸€èˆ¬åœ°, å½“ç³»æ•°å˜åŒ–å¹³ç¨³æ—¶ï¼Œå¯é€‰æ‹©å¯¹åº”çš„ $\lambda$ ä½œä¸ºå€™é€‰å€¼)
![[attachments/Pasted image 20241126011042.png|350]]
éœ€è¦è¯´æ˜çš„æ˜¯, å¾€å¾€æˆ‘ä»¬ä¼šä»¥ logspace æµ‹è¯•å‚æ•° $\lambda$ çš„é€‰å–. 
åˆæ­¥ç¡®å®šå‡ºæˆ‘ä»¬éœ€è¦çš„ $\lambda$ èŒƒå›´å¤§æ¦‚åœ¨ 1000-4000 ä¹‹é—´,

2. åŸºäº RidgeCV çš„äº¤å‰éªŒè¯
å¯¹äº Python ä¸­å·²ç»é›†æˆäº† RidgeCV æ¨¡å—ä¾¿äºäº¤å‰éªŒè¯, å¯ä»¥å®ç°è‡ªåŠ¨ä¼˜åŒ–:
å¯ä»¥ç›´æ¥é€‰æ‹© mean_squared_error ç­‰æ ‡å‡†ç›´æ¥äº¤å‰éªŒè¯(å‚è€ƒ[scoring parameter](https://scikit-learn.org/dev/modules/model_evaluation.html#scoring-parameter)éƒ¨åˆ†)

```python fold title:å²­å›å½’é¢„æµ‹æ³¢å£«é¡¿æˆ¿ä»·
import matplotlib.pyplot as plt  
import numpy as np  
import pandas as pd  
from sklearn.linear_model import Ridge, RidgeCV  
from sklearn.model_selection import train_test_split  
from sklearn.datasets import fetch_california_housing  
from sklearn.preprocessing import StandardScaler  
from sklearn.pipeline import Pipeline  
from sklearn.metrics import r2_score, root_mean_squared_error  
# RMSE , R2 score for evaluation  
  
class California_housing():  
    def __init__(self):  
        housing = fetch_california_housing(download_if_missing=True)  
        self.features = housing.feature_names  
        self.data = pd.DataFrame(housing.data, columns=self.features)  
        self.target = housing.target   # the housing price  
  
        self.model = Pipeline([  
            ('scalar', StandardScaler()),  
            ('ridge', Ridge(alpha=1.0, fit_intercept=True, max_iter=None, solver='svd')),  
            #  solver{â€˜autoâ€™, â€˜svdâ€™, â€˜choleskyâ€™, â€˜lsqrâ€™, â€˜sparse_cgâ€™, â€˜sagâ€™, â€˜sagaâ€™, â€˜lbfgsâ€™}, default=â€™autoâ€™  
        ])  
        # use 80% training and 20% testing  
        x_train, x_test, y_train, y_test = train_test_split(self.data, self.target, test_size=0.2, random_state=42)  
  
        print("============= first predict: ================")  
        self.build_ridge_model(x_train, y_train, 1)  
        self.do_predict(x_test, y_test)  
  
        self.draw_ridge_figue(x_train, y_train, np.logspace(-3, 3, 100))  # from  -1000 to 1000 with 500 steps  
  
        best_lmd = self.search_best_param(1000, 4000, step= 1)  
        self.build_ridge_model(x_train, y_train, best_lmd)  
        print("============ optimized results: =================")  
        self.do_predict(x_test, y_test)  
        print("best lambda value: ", best_lmd)  
  
    def build_ridge_model(self, x_train, y_train, lmda = 1):  
        self.model.set_params(ridge__alpha=lmda) # set the lambda needed  
        self.model.fit(x_train, y_train)  
  
    def do_predict(self, x_test, y_test):  
        y_pred  = self.model.predict(x_test)  
        r2sc = r2_score(y_test, y_pred, sample_weight=None)  
        rmse = root_mean_squared_error(y_test, y_pred, sample_weight=None)  
        print("R2 score: ", r2sc)  
        print("RMSE: ", rmse)  
  
    def draw_ridge_figue(self, x_train, y_train, lmds):  
        w_arr = np.empty((len(self.features) , 0))  
        for i in range(len(lmds)):  
            l = lmds[i]  
            self.model.set_params(ridge__alpha=l)  
            self.model.fit(x_train, y_train)  
            w = np.array(self.model.named_steps['ridge'].coef_).reshape(1,-1)  
            w_arr = np.concatenate((w_arr, w.T), axis=1)  
        fig = plt.figure()  
        fig.add_subplot(111, title='Ridge Map')  
        plt.plot(lmds, w_arr.T)  
        plt.legend(self.features)  
        plt.show()  
  
    def search_best_param(self, lmd_min, lmd_max, step=0.2, k=5):  
        """  
        é‡‡ç”¨äº¤å‰éªŒè¯æ–¹æ³•, ç›´æ¥è·å–æœ€ä½³çš„ lambda å–å€¼  
        :param lmds:        :return:  
        """        lmds = np.arange(lmd_min, lmd_max, step)  
        ridgeCV = RidgeCV(alphas=lmds, scoring='neg_mean_squared_error', cv=k)  # neg_mean_squared_error  
        ridgeCV.fit(self.data, self.target)  
        return ridgeCV.alpha_  
  
if __name__ == "__main__":  
    a = California_housing()
```

### (5) äº¤å‰éªŒè¯, ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢å¯»ä¼˜ 
`````ad-note
title: è¡¥å……: ä¸€èˆ¬å¸¸ç”¨çš„äº¤å‰éªŒè¯ä»£ç 
collapse: open 
é¦–å…ˆï¼Œåœ¨ä¸€èˆ¬æ•°æ®é›†ä¸­,  å¸¸ç”¨çš„æ˜¯ **K æŠ˜äº¤å‰éªŒè¯**ï¼Œå°†æ•°æ®åˆ†æˆ K ä»½ï¼Œè½®æµä½¿ç”¨æ¯ä¸€ä»½æ•°æ®ä½œä¸ºéªŒè¯é›†ï¼Œå‰©ä½™æ•°æ®ä½œä¸ºè®­ç»ƒé›†ã€‚

æ­¤å¤–æœ‰ç•™ä¸€æ³•äº¤å‰éªŒè¯(å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æ·±åº¦å­¦ä¹ ç®—æ³•åŸç†(sklearn)/è¡¥å……çŸ¥è¯†/5.SVMæ”¯æŒå‘é‡æœº|5.SVMæ”¯æŒå‘é‡æœº]])

å¯¹äº sklearn.cross_val_score å·²ç»å†™å¥½äº†äº¤å‰éªŒè¯ä»£ç , åªéœ€è¦ä½¿ç”¨ `cv` å‚æ•°æŒ‡å®š k æŠ˜äº¤å‰éªŒè¯å³å¯, å¦‚ä¸‹:
```python
from sklearn.model_selection import cross_val_score
# äº¤å‰éªŒè¯
cv_scores = []
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    score = np.mean(cross_val_score(ridge, X, y, cv=10, scoring='neg_mean_squared_error'))
    cv_scores.append(score)

best_alpha_cv = alphas[np.argmax(cv_scores)]
print(f"æœ€ä½³æ­£åˆ™åŒ–å‚æ•°ï¼ˆäº¤å‰éªŒè¯ï¼‰ï¼š{best_alpha_cv}")
```
`````

éšæœºæ£®æ—åˆ†ç±»ç®—æ³•(å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/2. å†³ç­–æ ‘å’Œå›å½’æ ‘|å†³ç­–æ ‘]]å’Œ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/8. éšæœºæ£®æ—|8. éšæœºæ£®æ—]])éƒ¨åˆ†, ç½‘æ ¼æœç´¢æ–¹æ³•å’Œéšæœºæœç´¢ç®—æ³•é‡‡ç”¨ `from sklearn.model_selection import GridSearchCVï¼Œ RandomizedSearchCV  `, å‚è€ƒ[sklearnè¶…å‚æ•°ä¼˜åŒ–éƒ¨åˆ†](https://scikit-learn.org/dev/api/sklearn.model_selection.html#module-sklearn.model_selection) 
![[attachments/Pasted image 20241126145507.png|550]]

```python
from sklearn.datasets import load_iris  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.model_selection import GridSearchCV  

# åŠ è½½æ•°æ®  
data = load_iris()  
X, y = data.data, data.target  

# å®šä¹‰æ¨¡å‹å’Œå‚æ•°ç½‘æ ¼  
model = RandomForestClassifier()  
param_grid = {  
    'n_estimators': [10, 50, 100],  
    'max_depth': [None, 10, 20]  
}  

# ç½‘æ ¼æœç´¢  
grid_search = GridSearchCV(model, param_grid, cv=5)  
grid_search.fit(X, y)  

# è¾“å‡ºæœ€ä½³å‚æ•°  
print("æœ€ä½³å‚æ•°:", grid_search.best_params_)
```


```python
from sklearn.datasets import load_iris  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.model_selection import RandomizedSearchCV  
from scipy.stats import randint  

# åŠ è½½æ•°æ®  
data = load_iris()  
X, y = data.data, data.target  

# å®šä¹‰æ¨¡å‹å’Œå‚æ•°åˆ†å¸ƒ  
model = RandomForestClassifier()  
param_distributions = {  
    'n_estimators': randint(10, 200),  
    'max_depth': [None, 10, 20, 30]  
}  

# éšæœºæœç´¢  
random_search = RandomizedSearchCV(model, param_distributions, n_iter=10, cv=5, random_state=42)  
random_search.fit(X, y)  

# è¾“å‡ºæœ€ä½³å‚æ•°  
print("æœ€ä½³å‚æ•°:", random_search.best_params_)
```
