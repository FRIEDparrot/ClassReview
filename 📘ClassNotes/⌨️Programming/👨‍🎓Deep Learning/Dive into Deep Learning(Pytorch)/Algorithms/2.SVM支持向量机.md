## 一、基本介绍
### (1) Logistic 函数
支持向量机(Suppported Vector Machine, SVM), 是一种分类算法,其一般目标是在空间中找到一个超平面, 并表示方程为
![[attachments/Pasted image 20240327152229.png|300]]
$$w^{T} x + b = 0$$
我们首先介绍Logistic函数, 即将数据从$(-\infty, +\infty)$ 映射到(0,1)的函数，其形式如下:
$$\boxed {h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1 + e^{-\theta^{T}x}}}$$
显然,当$\theta^{T}x > 0$时,$h_{\theta}(x) >0.5$, 获得的图像如下图:
![[attachments/Pasted image 20240327152635.png|400]]
其中上式中的$\theta^{T} x$即对应一个对输入参数(n维)的线性变换:
$$\theta^{T} x = \theta_{0}x_{0} + \theta_{1} x_{1}  + \theta_{2}x_{2} + \dots  + \theta_{n}x_{n}\qquad  (x_{0} = 1)$$
另外其中$x_1,\dots x_n$代表了多个不同训练样本值（x_0=1）也可以使用不同的基函数进行代替. 
对应的分类目的, 就是求解对应的超参数$\theta_{1}, \theta_{2}, \dots$,使得训练数据中对应的 y=1 部分, $\theta^{T} x >>0$, 而 y=0 部分, $\theta^{T} x << 0$，  

### (2)最大边缘超平面
Maximal Margin Hyperplane
![[attachments/Pasted image 20240327154353.png|300]]
由于将训练样本点分开的超平面很多，所以往往必须有最大边缘超平面作为决策边界。我们考虑和每个超平面平行的一对仍然可以将样本点分开的超平面,而其中两平面间距称为边缘(margin)

最大边缘超平面即距离最长的一组超平面(B1), 而我们往往以其为分类依据, 以获得更好的泛化误差。这种方法所对应的即为**线性支持向量机方法**。（同时适用于线性可分和线性不可分）

另外，数据可能需要非线性最优超平面进行分类。对应方法为**非线性支持向量机方法** -> 此时往往使用**核方法**获得不同的非线性支持向量机特性。
![[attachments/Pasted image 20240327154556.png|300]]

> [!NOTE] 核方法原理
> 核方法实际上是基于Mercer核展开定义通过内积函数定义的非线性变换将输入空间映射到高位特征空间， 并在高维空间中寻找相应的关系。

![[Excalidraw/2.SVM支持向量机 2024-03-30 10.49.21]]
## 二、线性支持向量机
### (1) 问题引入 
对于线性超平面, **分类决策边界可以写成**(其中w, b未知):
$$w^{T}  x + b  =0$$
决策边界由参数w,b共同决定。首先假设决策编辑将训练样本($x_1, x_2,\dots$)正确分类， 则显然有
$$\begin{cases}
w^{T} x_{i} + b \geq  \varepsilon \qquad y_{i} = 1 \\
w^{T} x_{i} + b \leq  \varepsilon \qquad y_{i} = -1
\end{cases}$$
此时，**显然按照比例调整(扩大)决策边界的参数w,b(可以按比例调整)可得到**: 
$$\begin{cases}
w^{T} x_{i} + b \geq  1 \qquad y_{i} = 1 \\
w^{T} x_{i} + b \leq  -1 \qquad y_{i} = -1
\end{cases}$$
为了计算两个超平面之间的边缘$\gamma$, 则在两个**边缘超平面上各取一个样本点**，使得:
$$w^{T} x_{1} + b = 1 \qquad  w^{T}x_{2} +b =-1$$
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/第八章 向量代数和空间解析几何|第八章 向量代数和空间解析几何]], 此时容易说明**超平面距离显然计算为**：
$$\boxed{\gamma =\frac{2}{||w||}\qquad  ||w|| = w_{1}^{2} + w_{2}^{2} + \dots + w_{n}^{2}}$$
只需寻找对应的参数w,b使得 $\gamma$ 最大即可, 可以转换为下面的二次目标函数优化问题:
$$\text{argmax} \frac{2}{||w||} \leftrightarrows \text{argmin}\frac{1}{2}||w||^{2}$$
### (2) 优化求解
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/👆重要定理/拉格朗日乘数法求条件极值|拉格朗日乘数法求条件极值]], 使用拉格朗日乘子$\alpha_{i} \geq  0$, 条件是**点均在边界上**，即
$$\boxed{\forall  i , \qquad  y_{i}(w^{T} x + b) \geq  1}\tag{2.1}$$
显然 $y_{i}(w^{T} x_{i} + b) -1 \geq 0$, 获得拉格朗日函数($\alpha_i \geq 0$): 
$$L(w, b, \alpha) = \frac{1}{2}||w||^{2}- \sum^{n}_{i=1} \alpha_{i}(y_{i}(w^{T} x_{i} + b) -1 )$$
我们取$L(w,b, \alpha)$==对$w$和$b$的偏导为0==，获得:
$$||w|| =\sum^{n}_{i=1}  \alpha_{i} y_{i}x_{i}\qquad  \sum^{n}_{i=1} a_{i}y_{i}= 0$$
此时代入， 得到优化问题为**使下面的式子最大**(由于||w||二次项有负号导致的):
$$L(w, b, \alpha) = -\frac{1}{2}\sum^{n}_{i=1} \sum^{n}_{j=1} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} + \sum^{n}_{i=1} \alpha_{i}$$
变形得到最终优化问题
$$\Large\boxed{\min_{\alpha} \frac{1}{2}\sum^{n}_{i=1} \sum^{n}_{j=1}  \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} -\sum^{n}_{i=1} \alpha_{i} }$$
只需求解相应的$\alpha$即可 

### (3) 不等式约束优化与松弛变量引入
支持向量机的损失形式有L1损失的支持向量机和L2损失的支持向量机两种形式。参考UQLab 中的推导

我们再次考虑式(2.1), 可以写为: 
$$g_{i} (w) = 1 -  y_{i}(w^{T} x + b ) \leq 0$$
显然不是所有的点都满足$= 1$的约束条件, 即**部分有$>1$成立**。 此时我们补充引入松弛变量 $\delta_{i}^{2}$, 使得不等式约束转化为等式约束:
$$1 -  y_{i} (w^{T} x + b)  + \delta_{i}^{2}  = 0$$
此时Lagrange函数(求解最大)变为:
$$\Large\boxed{L (w, b, a ) = \frac{1}{2}||w||^{2} + \sum^{n}_{i=1} \alpha_{i}(1 + \delta_{i}^{2} - y _{i}(w^{T} x + b)) }$$
其中$\alpha_{i}\geq 0$; 对上式中的 $w,  \delta_{i}$ 以及 $\alpha_i$ 进行求导, 得到:
$$\Large \begin{cases}
||w_{i}||  - \sum^{n}_{j=1}  \alpha_{j} y_{j} x_{j} = 0  \\
2  \alpha_{i} \delta_{i} = 0    \\
1 + \delta_{i}^{2} - y_{i} (w^{T}  x + b) = 0  \\
\alpha_{i} \geq  0
\end{cases}$$
### (4) 对偶优化与 KKT 条件
#### 1. 对偶优化
问题:求解$g_{i}(x) \leq 0, h_{j}(x) = 0$ 约束条件下的$f(x)$极小值
我们参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/👆重要定理/拉格朗日乘数法求条件极值|拉格朗日乘数法求条件极值]], 可使用下式:
$$L(x, \alpha, \beta) =  f(x) + \sum^{m}_{i=1} \alpha_{i}g_{i}(x) + \sum^{k}_{j=1} \beta_{j} h_{j}(x)$$
我们**只需要取不同的x值让这个式子最小即可** 
$$\min f(x) \rightarrow   \min_{x}  L (x, \alpha, \beta)$$
显然，这个式子对于任何的$\alpha, \beta$ 都需要取到最小值, 我们不放设$\alpha， \beta$让右侧的式子取到最大值，则问题转化为:
$$\boxed{\min f(x) \rightarrow \min_{x} \max_{\alpha \geq 0, \beta} L (x, \alpha, \beta)\overset{转化为对偶问题}{\longrightarrow}  \max_{\alpha \geq 0, \beta}\min_{x} L (x, \alpha, \beta)}$$
上式即为==对偶优化问题的转换式==。

#### 2. KKT 条件 
KKT条件(**Karush-Kuhn-Tucker 条件**)
对于上式中, 求解$\max_{\alpha , \beta\geq 0}\min_{x} L (x, \alpha, \beta)$, 其中(显然前面一项是取小的, 而后面两项是使得取更小值的,但是由于$\alpha$存在使得最终为0)可以化为无条件极值;
$$\max_{\alpha, \beta \geq  0}  L(x, \alpha, \beta) =  f(x) + \sum^{m}_{i=1} \alpha_{i}g_{i}(x) + \sum^{k}_{j=1} \beta_{j} h_{j}(x)\qquad (g_{i} \leq   0, h_{j} = 0)$$
为了保证==对偶优化问题和初始的问题具有相同的解==， 参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/🚧结构可靠性设计基础/推导部分/对偶优化一致解条件推导过程.pdf|对偶优化一致解条件推导过程.pdf]] 和 [KKT条件详解](https://zhuanlan.zhihu.com/p/26514613) 得到一致解条件(KKT条件)，KKT 条件给出了判断解 $x^*$ 是否为最优解的<b><mark style="background: transparent; color: blue">必要条件</mark></b>:
$$\Large \begin{cases}
\left.\frac{\partial L(x ,\alpha^{*}, \beta^{*})}{\partial x}  \right|= 0 ,  \\
\alpha_{i}^{*} g_{i}(x^{*}) = 0  \\
h_{j} (x^{*}) = 0,  g_{i} (x^{*}) \leq  0 \\
\alpha_{i} (x^{*})  \geq  0
\end{cases}\tag{8.3.4.1}$$
最优解必然满足上述所有条件。 

## 三、支持向量机的常用核函数
#### 1. 点积形式
获得的是线性分类器
$$K(x,y) = x \cdot  y$$
#### 2. 多项式形式的核函数
获取d阶多项式分类器:
$$K(x,y) = \left\{ (x \cdot y) + 1\right\}^{d}$$
#### 3. 径向基函数核函数(高斯核)
径向基函数分类器
$$\Large K(x,y) = \exp\left(-\frac{||x - y||^{2}}{2\sigma^{2}}\right)$$
其中, $\sigma$ 为高斯核带宽。
#### 4. Sigmoid 形式核函数
$$K(x,y) = \tanh (kx \cdot y - \delta)$$
其中, $k > 0$, $\delta <  0$; 
#### 5. 指数核函数
$$k(x_{i}, x_{j}) = \exp \left(- \frac{||x_{i} - x_{j}||}{\sigma}\right)$$
需要说明的是，参考 [Global sensitivity analysis using support vector regression](https://pdf.sciencedirectassets.com/271589/1-s2.0-S0307904X17X00061) 部分, 根据 Mercer 理论, 核函数必须满足条件:
$$\boxed{\iint k(x_{i}, x_{j}) \phi (x_{i}) \phi (x_{j} ) dx_{i} dx_{j} = 0}$$
其中$k(x_{i}, x_{j}) = \varphi(x_{i}) \cdot  \varphi(x_{j})$为核函数， 其中$\phi(x)$是任意的不等于0且满足$\int \phi^{2}(x)< \infty$的函数; 具体证明参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/🚧结构可靠性设计基础/推导部分/mercer's theorem.pdf|mercer's theorem.pdf]] 

> [!caution] 说明
> 实际使用中, 由于核函数的投射方式不同, 所以**往往不同的核函数会导致结果的很大差别**。

## 四、非线性支持向量机与带有松弛变量的SVM方法
首先, 我们对于线性可分的样本
$$D = \left\{(x_{1}, y_{1}), \dots , (x_{l}, y_{l}) \right\},  \quad  y_{i}  \in \{-1 , +1\}$$
假设线性超平面为:
$$\Large\boxed{w \cdot  X + b  = 0}\tag{8.4.1.1}$$
分类目标是最大化超平面距离 $\gamma$, 即最小化$\frac{1}{2} ||w||^{2}$, 同时, <b><mark style="background: transparent; color: blue">考虑到一些样本不能够被正确分类, 引入松弛变量</mark></b>$\xi_{i}$ , 此时最小化问题成为寻找:
$$\text{argmin}  \frac{1}{2}||w||^{2}  + C \sum^{l}_{i=1} \xi_{i}\qquad  s.t \quad  y(w\cdot  \varphi(x_{i}) + b) \geq  1 - \xi_{i} \quad (\xi_{i} \geq 0, i = 1,2, \dots l)\tag{8.4.1.2}$$
移项得到Lagrange的约束关系:
$$g(x) = -( y(w \cdot  \varphi (x_{i} )+ b ) - 1  + \xi_{i}) \leq  0 \qquad  \xi_{i} \geq 0\tag{8.4.1.3}$$
> [!NOTE] 说明
> 其中对于每个变量都对应一个$\xi_{i}$, 描述不满足约束$\geq 1$的程度。而$C$表示**正则化常数**。取$C = \infty$时,则约束变为所有的点都必须明确分类 $y_{i}(w \cdot \varphi(x_{i}) + b)\geq 1$, 而C取有限值时, 一些样本点则可以不满足约束。

此时， 引入拉格朗日函数, 得到下面的**拉格朗日函数**(寻求最小值), 其中**最后一项是由于$\xi_{i} \geq 0$构造的**$-\gamma_{i}\xi_{i} \leq  0$: 
$$L(w, b, \xi , \alpha, \gamma) = \frac{1}{2}||w||^{2} + C \sum^{l}_{i=1} \xi_{i} - \sum^{l}_{i=1} \alpha_{i} \left\{y_{i} [w \cdot  \varphi (x_{i}) + b \right\}  - 1 + \xi_{i}) - \sum^{l}_{i=1} \gamma_{i} \xi_{i}$$
上述的优化问题变为:
$$\min_{w, b, \xi} \max_{\alpha \geq 0, \gamma\geq  0} L (w, b, \xi, \alpha, \gamma\geq  0 ) \overset{对偶问题}{\longrightarrow}  \max_{\alpha \geq   0, \gamma\geq  0}\min_{w, b, \xi} L(w, b, \xi , \alpha , \gamma)$$
代入[[#(4) 对偶优化|KKT条件]], 并经过[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/🚧结构可靠性设计基础/推导部分/支持向量机SVM分类与回归算法推导过程.pdf|支持向量机SVM分类与回归算法推导过程]], 将上式化为如下问题:
$$\Large\boxed{\max_{\alpha \geq  0}  \frac{1}{2}\sum^{l}_{i=1} \sum^{l}_{j=1} \alpha_{i}\alpha_{j} \mathcal{K}(x_{i}, x_{j}) -  \sum^{l}_{i=1} \alpha_{i}  \quad \left(\sum^{n}_{i=1} \alpha_{i} y_{i} = 0,  \alpha_{i}\in  [0, C]\right)}\tag{8.4.1.4}$$
其中, $\mathcal{K}$表示核函数。而支持向量的==**分类预测函数**==可以计算为:
$$\Large \boxed{g_{SVM }(x) = \text{sgn}\left(\sum^{l}_{i=1} \alpha_{i} y_{i}\mathcal{K}(x_{i}, x_{j}) + b\right)}\tag{8.4.1.6}$$
其中b值可以利用下式的任一分量式求解:
$$y_{i}(w \varphi(x_{i}) + b) = 1\qquad  \text{when}  \quad \alpha_{i} \in (0,C)$$
我们**一般做法是计算出每一个$\alpha\in (0,C)$分量对应的 b 之后,取平均值**。
> [!tip] 说明
> 求解获取到的大部分$\alpha_{i} = 0$,  不为零对应的样本的是支持向量。

## 五、支持向量机回归算法
首先， 考虑训练样本集:
$$D = \left\{ (x_{1}, y_{1}), \dots, (x_{l}, y_{l} )  \right\}$$
假设样本集$D$是$\varepsilon -$线性近似的, 即**存在一个超平面表达式$g_{SVR}(x) = w\cdot x + b$使得下式成立**:
$$\left| g_{SVR} (x_{i}) - y_{i} \right|  \leq  \varepsilon\quad  \rightarrow  \quad d =   \frac{|w \cdot  x + b - y_{i}|}{\sqrt{||w||^{2} + 1}} \leq  \frac{\gamma}{2}$$
图像如下:
![[Excalidraw/第八章 响应面方法和支持向量机 2024-03-30 11.13.46|200]]

> [!important] SVM分类和回归的比较
> 区别于**分类算法的目标是保证样本点全在分类直线范围外的情况下, 最大化间隔带宽度**$\gamma$。 
> 而回归算法的目标是在所有的点都满足$|g_{SVR} (x_{i}) - y_{i}| \leq \varepsilon$的情况下, 此时最大化的容差距离为:
> $$\gamma = \frac{2|\varepsilon|}{\sqrt{||w||^{2} + 1}}$$
> 此时，我们将容差 $< \varepsilon$ 的部分不计入损失, 而大于的部分计入损失; 则**要求最大化容差距离且最小化额外损失**， 公式成为
> $$\min_{w,b} \frac{1}{2} ||w||^{2} + C \sum^{n}_{i=1} (g(x_{i} - y_{i})) * ((y-g(x))\geq \varepsilon)$$
> 后面的部分是以$\xi$表示的(对于容差距离内的点,$\xi = \xi^{*} = 0$)，优化目标是求解不同的$w,b$设置下, 最小化这个损失

与分类问题完全类似，参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/🚧结构可靠性设计基础/推导部分/支持向量机SVM分类与回归算法推导过程.pdf|支持向量机SVM分类与回归算法推导过程.pdf]] 得到非线性SVM回归的优化问题:
$$\max_{\alpha, \alpha^{*}\geq 0 } \sum^{l}_{i=1}  y_{i} (\alpha_{i}^{*} - \alpha_{i}) - \varepsilon(\alpha_{i}  + \alpha^{*}_{i}) - \frac{1}{2}\sum^{l}_{i=1} \sum^{l}_{j=1} (\alpha^{*}_{i}- \alpha_{i})(\alpha^{*}_{j}- \alpha_{j}) \mathcal{K}(x_{i}, x_{j})\qquad s.t.  \quad  \sum^{l}_{i=1}  (\alpha^{*}_{i} - \alpha_{i})= 0 , \alpha_{i}, \alpha_{i}^{*}  \in [0,C]$$
求解对应的$\alpha_i, \alpha^*_i$。最终支持向量回归表达式为:
$$\Large\boxed{g_{SVR} (x) =  w \varphi (x) + b =  \sum^{l}_{i=1}(\alpha^{*}_{i} - \alpha_{i})k (x_{i}, x) + b }$$
其中$b$可以使用以下式子取平均值进行获取:
$$b = \begin{cases}
y_{j} + \varepsilon - \sum^{l}_{i=1} (\alpha_{i} - \alpha^{*} _{i})k(x_{i}, x_{j}) \qquad  \alpha_{j}  \in (0, C)\\
y_{j} - \varepsilon - \sum^{l}_{i=1} (\alpha_{i} - \alpha^{*}_{i})k(x_{i}, x_{j}) \qquad \alpha^{*}_{j} \in (0, C)
\end{cases}$$

## 六、交叉验证误差
<mark style="background: transparent; color: red">支持向量机的推广能力</mark>即在算法对于未知的测试集上对数据进行拟合的精度。为了评估模型的推广能力，往往<mark style="background: transparent; color: red">使用测试即并来取验证误差来估计预测的精度</mark>。
### (1) 留一法交叉验证误差
一般使用 $e_{LOO}$ 指代, 并且定义分类问题的**留一法交叉验证误差**(Leave one out Cross-Validation)为:
$$e_{LOO} = \frac{1}{l} \sum^{l}_{i=1} I(- y_{i} g_{SVM}^{\sim i}(x) )$$
其中, $g_{SVM}^{\sim i}$ 表示除去第i个样本之后, <mark style="background: transparent; color: red">使用剩余的样本点构建的支持向量分类模型</mark>。 

而回归问题的留一法交叉验证误差为
$$e_{LOO}  = \frac{1}{l} \sum^{l}_{i=1} I(y_{i}- g_{SVR}^{\sim i}(x) )^{2}$$
对应地 $g_{SVM}^{\sim i}$ 表示除去第i个样本之后, <mark style="background: transparent; color: red">使用剩余的样本点构建的支持向量回归模型</mark>。 
其中右上角$\sim i$表示除去第i个样本构建的支持向量分类模型。 
$$I = \begin{cases}
0 \qquad X > 0 \\
1 \qquad  X \leq 0
\end{cases}$$
### (2) K折中交叉验证
K-fold cross validation 
