## 1. Introduction for GAN  
### (1) Generative Modeling   
For the often-used Learning to learn  the mapping from data example  to labels, we call this kind of learning as <b><mark style="background: transparent; color: orange">Discriminate learning</mark></b>. Classifier and regressors are both  discriminative learning. 

Another machine learning technique more than  just  solving  discriminative  tasks isto learn a model from the data to capture the  characteristics of it. and <b><mark style="background: transparent; color: orange">generate a not photorealistic image that looks like it from the same dataset</mark></b>. This kind of learning is called **generative modeling**. 

The [[üìòClassNotes/‚å®Ô∏èProgramming/üë®‚ÄçüéìDeep Learning/‚öìDeep Learning Basic Concepts/Chapter7 RNN|recurrent neural network]] language models are one example of using a discriminative network (trained to predict the next character) that once trained can act as a generative model (predict the complete whole sentence). 

GAN (Generative adversarial networks) is a clever new way to <b><mark style="background: transparent; color: orange">leverage the power of discriminative models to get good generative models</mark></b>.  

### (2) GAN Architecture  
The architecture of GAN is as follows : 
![[Excalidraw/3. GAN (Generative  Adversarial Networks) 2025-06-18 10.27.44|250]]

### (3) Loss of Generator and Discriminator  
The  discriminator  is a classifier  to distinguish where the input  $x$ from and  output a scalar prediction $o \in R$ for input $x$, We <b><mark style="background: transparent; color: orange">train the discriminator to minimize the following  cross-entropy loss</mark></b> : 
$$\boxed{\min_{D}  \left\{ -y \log_{} D(x)  - (1- y) \log_{}(1 - D(x)) \right\} \qquad   y \in  [0; 1]}$$
where : 
$$D = \frac{1}{1  + e^{-o}}$$
is the ==predicted probability from the  real dataset==. Note if we generate a very good model, **the above loss is 0**. 

For generator,  we often  draw some paraneters $z \in R^{d}$ from a source of randomness. which gives $z \sim N(0,1)$ is <b><mark style="background: transparent; color: orange">the latent variable</mark></b>. We generate $x' =  D(G(z))$, For generator, we **update the  generator to maximize the loss for  discriminator to consider the generated data as true** i.e. <mark style="background: transparent; color: red">when  y = 0</mark>, there should  be :
$$\boxed{\max_{G} \left\{  -(1-y) \log_{} (1-D(G(z))  )\right\}} \rightarrow  \max \left\{ -\log_{} (1- D(G(z))\right\}$$
For generator does a perfect job,  $D(x') \approx 1$, hence we often use this loss for  
$$\boxed{\min \left\{ -\log_{} (D (G(z))) \right\}}$$
(which is the loss giving  $y = 1$ when feed  $x' = G(z)$ to discriminator)
$$\Large\boxed{\min_{D} \max_{G}  \left\{ -E_{x \sim Data} \log_{} D(x)   -  E_{z \sim Noise}  \log_{} (1- D(G(z))) \right\}}$$

We note in real train process, we often define two trainer (2 different optimizer), and in that case, 
```python fold title:
def train(net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data): 
	loss = nn.BCEWithLogitsLoss(reduction='sum')
	trainer_D = torch.optim.Adam(net_D.parameters(), lr=lr_D)
	trainer_G = torch.optim.Adam(net_G.parameters(), lr=lr_G) 
	animator = d2l.Animator(xlabel='epoch', ylabel='loss',
	xlim=[1, num_epochs], nrows=2, figsize=(5, 5),
	legend=['discriminator', 'generator']) animator.fig.subplots_adjust(hspace=0.3)
```

In that case, we often use <b><mark style="background: transparent; color: orange">nn.BCELoss (binary  cross entropy loss)</mark></b>, where : 
$$BCELoss(x,y) = -w_{n} \left[ y_{n} \log_{} x_{n}  + (1 -y_{n}) \log_{}(1 - x_{n})  \right]$$
note we use `nn.BCEWithLogitsLoss()` (add sigmoid in $x$)  as the loss function (see [documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html))
Hence we may use the following method to calculate the loss : 
$$loss_{d} = nn.BCEWithLogitsLoss() (x, y)$$
$$loss_{g} = nn.BCEWithLogitsLoss(x, ones(1,n))$$
we can test if the result is accurate by the following code : 
```python 
import torch  
from torch import nn  
from torch.nn.functional import sigmoid as sigmoid_fn 

data = torch.randn(10, 1)  # Generate some random data 
print(loss_fn(data, torch.ones_like(data)) , torch.mean(-torch.log(sigmoid_fn(data))))  # should be same  
```

Also Note <b><mark style="background: transparent; color: orange"> theoretical optimum of log(2) ‚âà 0.693 for a perfectly balanced GAN.  </mark></b>

### (4) Simple Implementation 

```python fold title:simple-gan-implementation 
import torch  
from torch import nn  
import torch.nn.functional as F  
import matplotlib.pyplot as plt  
from scipy import stats  

# Define simple MLP generator  
class Generator(nn.Module):  
    def __init__(self, latent_dim):  
        super().__init__()  
        self.model = nn.Sequential(  
            nn.Linear(latent_dim, 32),  
            nn.ReLU(),  
            nn.Linear(32, 32),  
            nn.ReLU(),  
            nn.Linear(32, 1)  # output 1D sample  
        )  
      
    def forward(self, z):  
        return self.model(z)  
  
  
# Define simple MLP discriminator  
class Discriminator(nn.Module):  
    def __init__(self):  
        """  
        Four Layer MLP discriminator        """        super().__init__()  
        self.model = nn.Sequential(  
            nn.Linear(1, 32),  
            nn.ReLU(),  
            nn.Linear(32, 64),  
            nn.ReLU(),  
            nn.Linear(64, 32),  
            nn.ReLU(),  
            nn.Linear(32, 1)  
        )  
  
    def forward(self, x):  
        return self.model(x)  
  
  
# Generate real data: samples from N(2, 1)  
def sample_real_data(batch_size):  
    return torch.randn(batch_size, 1) * 2.0 + 3.0  
  
  
# Generate latent input for G  
def sample_latent(batch_size, latent_dim):  
    return torch.randn(batch_size, latent_dim)  
  
  
# Training loop  
def train(net_D, net_G, num_epochs, lr_D, lr_G, latent_dim, batch_size=64):  
    loss_fn = nn.BCEWithLogitsLoss()  
  
    # Weight initialization  
    for w in net_D.parameters():  
        if w.dim() > 1:  
            nn.init.normal_(w, 0, 0.02)  
    for w in net_G.parameters():  
        if w.dim() > 1:  
            nn.init.normal_(w, 0, 0.02)  
  
    opt_D = torch.optim.Adam(net_D.parameters(), lr=lr_D)  
    opt_G = torch.optim.Adam(net_G.parameters(), lr=lr_G)  
    scheduler_D = torch.optim.lr_scheduler.LinearLR(optimizer=opt_D, start_factor=1, end_factor=0.001,  
                                                    total_iters=num_epochs)  
    scheduler_G = torch.optim.lr_scheduler.LinearLR(optimizer=opt_G, start_factor=1, end_factor=0.001,  
                                                    total_iters=num_epochs)  
  
    d_losses, g_losses = [], []  
  
    for epoch in range(num_epochs):  
        real_data = sample_real_data(batch_size)  
        fake_input = sample_latent(batch_size, latent_dim)  
        fake_data = net_G(fake_input).detach()  # stop G gradient when training D  
  
        ### Train Discriminator ###        opt_D.zero_grad()  
  
        # combine the real data and fake data randomly  
        train_data = torch.cat([real_data, fake_data], dim=0)  
        train_label = torch.cat([torch.ones_like(real_data), torch.zeros_like(fake_data)], dim=0)  
        pred_labels = net_D(train_data)  
        loss_d = loss_fn(pred_labels, train_label)  
        loss_d.backward()  
        opt_D.step()  
        scheduler_D.step()  
  
        ### Train Generator ###  
        opt_G.zero_grad()  
        z = sample_latent(batch_size, latent_dim)  
        fake_data = net_G(z)  
        gen = net_D(fake_data)  
        # Non-saturating loss: pretend fake data is real  
        loss_g = loss_fn(gen, torch.ones_like(gen))  
        loss_g.backward()  
        opt_G.step()  
        scheduler_G.step()  
  
        d_losses.append(loss_d.item())  
        g_losses.append(loss_g.item())  
  
        if (epoch + 1) % 100 == 0:  
            print(f"Epoch {epoch + 1}, Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}")  
  
    # Plot  
    plt.figure(figsize=(6, 3))  
    plt.plot(d_losses, label='Discriminator')  
    plt.plot(g_losses, label='Generator')  
    plt.xlabel('Epoch')  
    plt.ylabel('Loss')  
    plt.legend()  
    plt.title('GAN Losses')  
    plt.grid(True)  
    plt.show()  
  
    plt.figure(figsize=(6, 3))  
    sample_true = sample_real_data(1000).numpy()  
    sample_fake = net_G(sample_latent(1000, latent_dim)).detach().numpy()  
  
    # Plot histogram of real and fake data  
    plt.hist(sample_true, bins=30, alpha=0.5, label='Real', density=True)  
    plt.hist(sample_fake, bins=30, alpha=0.5, label='Fake', density=True)  
    plt.xlabel('Value')  
    plt.ylabel('Density')  
    plt.legend()  
    plt.title('Histogram of Real and Fake Data')  
  
    plt.show()  
  
  
# Run training  
latent_dim = 20  
net_G = Generator(latent_dim)  
net_D = Discriminator()  
train(net_D, net_G, batch_size=512,  
      num_epochs=3000,  
      lr_D=1e-3, lr_G=5e-4,  
      latent_dim=latent_dim)
```

After training, we get the following image : `Epoch 3000, Loss D: 0.6926, Loss G: 0.6935` 
![[attachments/Pasted image 20250618151758.png|400]]
Also, For GAN losses :
![[attachments/Pasted image 20250618151815.png|300]]
