PINN 是通过训练神经网络最小化损失， 来近似 PDE 的求解过程，  而

### (1) 基本求解思想 
首先， 我们介绍基于强形式镜像基函数的无网格方 Kansa 法，考虑 : 
$$f''(x) = -1 \qquad  x \in  (0, 1) \qquad  f(0) = f(1) = 0$$
首先, Kansa 法假设 
$$f(x) = \sum_{i = 1}^{n} a_{i} \phi_{i}(x)$$
即可以通过基函数进行表示， 显然得到 : 
$$\sum_{i = 1}^{n} a_{i} \phi_{i}''(x_{i} ) = -1  \qquad \sum_{i = 1}^{n} a_{i}\phi_{i}(0) = \sum_{i = 1}^{n}a_{i}\phi_{i}(1) = 0 $$
其中， $\phi$ 为RBF 函数; 而 $\phi_{i}$  形成线性方程组，而由于 $\phi$  都是定义出的，只需求解 $a_{i}$

由于 $f(x)$ 对应取值较多， 实际可得到大量的方程， 从而进行误差计算和最小二乘回归分析。
### (2) 一般形式 
考虑对于一般的微分方程 : 
$$\mathcal{F} (u(z); \gamma) = f(z) \qquad  z \in  \Omega$$
$$\mathcal{B}(u(z)) = g(z) \qquad  z \in  \partial \Omega$$
$$\mathcal{D}(u(z_{i})) = d(z_{i}) \qquad  i \in D$$
其中: 
$z$ 为包含空间和时间的坐标 (定义在 $\Omega$ 上)， $u = u(z)$ 为方程的解，
第一个方程 $\mathcal{F}$ 为控制方程，$\gamma$ 为控制参数 
$\mathcal{B}$ 为初值条件和边界条件 
$\mathcal{D}$ 为<mark style="background: transparent; color: red">观测数据的方式</mark>，$D$ <b><mark style="background: transparent; color: orange">为观测数据指标集合</mark></b>

而我们以 
$$u_{\theta}(z)  = f_{L}\circ  f_{L-1}\circ \dots  \circ f_{1} (z)$$
描述最终的函数 $z$, 显然这个过程可以以神经网络输出表示， 而相应残差可以表示为 : 
$$L:=w_{F} L_{F} + w_{B }  L_{B} + w_{D}L_{D}$$
因此只需训练和最小化残差: 
$$\theta^{*} \in \text{arg}\min_{\theta} L[u_{\theta}]$$

一般而言，PINN 的<b><mark style="background: transparent; color: orange">损失函数主要由两部分组成 </mark></b> (<mark style="background: transpa\rent; color: red">这是和一般神经网络的主要区别</mark>):
$$L_{PINN} = L_{data} + L_{physics}$$
-  $L_{data}$ : 数据拟合损失，如均方误差等等 
- $L_{physics}$ : **物理约束损失**，即<b><mark style="background: transparent; color: orange">通过 PDE 计算出的残差</mark></b>

例如， 对于热传导方程 : 
$$\frac{\partial u}{\partial t} = \alpha \frac{\partial^{2} u}{\partial t^{2}}$$
则我们可以定义物理约束损失为 :
$$\left(\frac{\partial }{\partial t} - \alpha  \frac{\partial^{2} }{\partial t^{2}}\right) u $$
显然其中我们可以设 $u = u(t, x)$, 
![[attachments/Pasted image 20250608211334.png|500]]
如果具有多个边界条件， 则直接将其设为所有的部分与0 差值的平方和 :  
$$L = l_{1}+ l_{2}+ \dots  + l_{n}$$

### (2) Torch 中的导数计算 
对于 torch 中， 可以通过 autograd.grad 计算导数 :  
```python fold title:基本导数计算方法
import torch

x = torch.tensor(2.0, requires_grad=True)
y = x ** 3
# 计算一阶导数 dy/dx
dy_dx = torch.autograd.grad(y, x, create_graph=True)  # create_graph 允许高阶求导
print(dy_dx)  # 输出: (tensor(12.),)
```

> [!caution] 求导结果说明 
> 对于导数计算， 只要取 $[0]$ 就可以了， 对应元组的 $[1]$ 元素完全不会出现 


### (3) PINN 示例代码  
参考 [CSDN 博客](https://blog.csdn.net/qq_49323609/article/details/129327571), 下面给出使用一个 PINN 神经网络
求解 : 
$$u = x^{2} e^{-y}$$

```python fold title:PINN求解算例
import torch  
import torch.nn as nn  
import torch.optim as optim  
import math  
import numpy as np  
from matplotlib import pyplot as plt  
  
  
# Physics Informed Neural Network Example  
# For the simplest equation : https://blog.csdn.net/qq_49323609/article/details/129327571  
  
# solve the PDE : u(x,y) = x^2 + e^(-y)  
def gradient(y: torch.Tensor, x: torch.Tensor, order=1):  
    if order == 1:  
        grad = torch.autograd.grad(  
            y, x,  
            grad_outputs=torch.ones_like(y),  
            create_graph=True,  
            retain_graph=True,  
            only_inputs=True,  
        )[0]  
        return grad  
    else:  
        grad = torch.autograd.grad(  
            y, x,  
            grad_outputs=torch.ones_like(y),  
            create_graph=True,  
            retain_graph=True,  
            only_inputs=True,  
            allow_unused=True  
        )[0]  
        return gradient(grad, x, order=order - 1)  
  
  
def ctrl_equation_loss(model, x: torch.Tensor, y: torch.Tensor):  
    """ Control equation loss function."""  
    x.requires_grad_(True)  
    y.requires_grad_(True)  
  
    u = model(x, y)  
    # Calculate the first derivative of u with respect to t  
    u_xx = gradient(u, x, order=2)  
    u_y4 = gradient(u, y, order=4)  
    # pde loss function  
    l_physics = torch.mean((u_xx - u_y4 - (2 - x ** 2) * torch.exp(-y)) ** 2)  
    return torch.mean(l_physics + 1e-6)  # Control equation loss function  
  
  
def bnd_cond_loss(model):  
    x_low = 0  
    x_high = 1  
    y_low = 0  
    y_high = 1  
  
    # 创建边界点  
    x = torch.arange(x_low, x_high, 0.01).requires_grad_(True).to(model.device)  
    y = torch.arange(y_low, y_high, 0.01).requires_grad_(True).to(model.device)  
  
    # **为了能够计算导数，需要创建与边界点相关的y坐标**  
    y_bottom = torch.zeros_like(x).requires_grad_(True).to(model.device)  # 底边界 y=0，但需要能对y求导  
    y_top = torch.ones_like(x).requires_grad_(True).to(model.device)  # 顶边界 y=1，但需要能对y求导  
  
    u1 = model(x, y_bottom)  # (x, 0)  
    u2 = model(x, y_top)  # (x, 1)  
    u3 = model(x_low * torch.ones_like(y), y)  # (0, y)  
    u4 = model(x_high * torch.ones_like(y), y)  # (1, y)  
  
    # 计算二阶导数  
    u1_yy = gradient(u1, y_bottom, order=2)  
    u2_yy = gradient(u2, y_top, order=2)  
  
    l1 = u1 - x ** 2  
    l2 = u2 - x ** 2 / math.e  
    l3 = u3 - 0  
    l4 = u4 - torch.exp(-y)  
    l5 = u1_yy - x ** 2  
    l6 = u2_yy - x ** 2 / math.e  
  
    # Boundary condition loss function  
    return torch.mean(l1 ** 2 + l2 ** 2 + l3 ** 2 + l4 ** 2 + l5 ** 2 + l6 ** 2 + 1e-6)  
  
  
class PINN_TEST1(nn.Module):  
    """Physics Informed Neural Network (PINN) for the simplest equation."""  
  
    def __init__(self, hidden_dim=32, num_layers=5):  
        super(PINN_TEST1, self).__init__()  
        # input : x, y  
        self.fc1 = nn.Linear(2, 10)  
        self.act1 = nn.Tanh()  # activation function  
        self.fc2 = nn.Linear(10, hidden_dim)  
        self.act2 = nn.Tanh()  # activation function  
        self.fc_output = nn.Linear(hidden_dim, 1)  
  
        # build the network  
        self.net = nn.Sequential()  # sequential container  
        self.net.add_module('fc1', self.fc1)  
        self.net.add_module('act1', self.act1)  
        self.net.add_module('fc2', self.fc2)  
        self.net.add_module('act2', self.act2)  
        for i in range(num_layers - 2):  
            self.net.add_module('fc_hidden' + str(i + 1), nn.Linear(hidden_dim, hidden_dim))  
            self.net.add_module('act_hidden' + str(i + 1), nn.Tanh())  
        self.net.add_module('fc_output', self.fc_output)  
  
        self.optimizer = optim.Adam(self.parameters(), lr=0.001)  
  
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  
        self.to(self.device)  # move the model to GPU if available  
  
    def forward(self, x: torch.Tensor, y: torch.Tensor):  
        x = x.to(self.device)  
        y = y.to(self.device)  
        """Forward pass of the PINN."""  
        u = self.net(torch.cat((x.view(-1, 1), y.view(-1, 1)), 1))  # [batch_size, 1]  
        return u.squeeze()  # [batch_size]  
  
    def loss(self, x, y):  
        loss = ctrl_equation_loss(self, x, y) + bnd_cond_loss(self)  
        return loss  
  
    def generate_test_data(self, n_test=100):  
        """Generate test data for evaluation"""  
        # Generate random test points  
        x_test = torch.rand(n_test, 1)  
        y_test = torch.rand(n_test, 1)  
  
        # Calculate analytical solution  
        def analytical_solution(x, y):  
            return x ** 2 * torch.exp(-y)  # 修正: x^2 * e^(-y)  
  
        u_true = analytical_solution(x_test, y_test)  
  
        # Get model predictions  
        self.eval()  
        with torch.no_grad():  
            u_pred = self.forward(x_test.squeeze(), y_test.squeeze())  
  
        return x_test.numpy(), y_test.numpy(), u_pred.cpu().numpy(), u_true.numpy()  
  
    def training_step(self, epochs=1000, batch_size=5):  
        """ Training step of the PINN."""  
        loss_history = []  
  
        # generate the random samples for training  
        for epoch in range(epochs):  
            x_batch = torch.rand(batch_size, 1)  
            y_batch = torch.rand(batch_size, 1)  
  
            batch_loss = torch.tensor(0.0)  
  
            for i in range(batch_size):  
                x, y = x_batch[i], y_batch[i]  
                u = self.forward(x, y)  
                self.optimizer.zero_grad()  
                loss = self.loss(x, y)  
                loss.backward()  
                batch_loss = batch_loss + loss  
                self.optimizer.step()  
  
            batch_loss /= batch_size  
            loss_history.append(batch_loss.item())  
  
            if epoch % 5 == 0:  
                print('Epoch: {}/{}, Loss: {:.4f}'.format(epoch, epochs, batch_loss.item()))  
              
            if epoch > 0 and epoch % 100 == 0:  
                # 生成测试数据用于绘图  
                x_test, y_test, u_pred, u_true = self.generate_test_data()  
                self.plot_results(loss_history=loss_history,  
                                  x_test=x_test, y_test=y_test,  
                                  u_pred=u_pred, u_true=u_true)  
                self.compare_solutions_2d()  
  
        print('Training completed!')  
        # 最终绘图也包含测试数据  
        x_test, y_test, u_pred, u_true = self.generate_test_data()  
        self.plot_results(loss_history=loss_history,  
                          x_test=x_test, y_test=y_test,  
                          u_pred=u_pred, u_true=u_true)  
        self.compare_solutions_2d()  
        return self.net  
  
    def plot_results(self, loss_history, x_test=None, y_test=None, u_pred=None, u_true=None):  
        """Plot training loss, solutions comparison, and error distribution"""  
  
        def analytical_solution(x, y):  
            """Analytical solution: u(x,y) = x^2 * e^(-y)"""  
            return x ** 2 * np.exp(-y)  # 修正: x^2 * e^(-y)  
  
        # Create a grid for surface plots        x_grid = np.linspace(0, 1, 50)  
        y_grid = np.linspace(0, 1, 50)  
        X, Y = np.meshgrid(x_grid, y_grid)  
  
        # Get model predictions on the grid  
        self.eval()  
        with torch.no_grad():  
            x_flat = torch.tensor(X.flatten(), dtype=torch.float32)  
            y_flat = torch.tensor(Y.flatten(), dtype=torch.float32)  
            u_pred_grid = self.forward(x_flat, y_flat).cpu().numpy().reshape(X.shape)  
  
        # Analytical solution on the grid  
        u_true_grid = analytical_solution(X, Y)  
  
        # Create subplots  
        fig = plt.figure(figsize=(20, 15))  
  
        # 1. Training loss  
        ax1 = fig.add_subplot(2, 3, 1)  
        ax1.plot(loss_history)  
        ax1.set_xlabel('Epoch')  
        ax1.set_ylabel('Loss')  
        ax1.set_title('Training Loss')  
        ax1.set_yscale('log')  
        ax1.grid(True)  
  
        # 2. Analytical solution (3D surface)  
        ax2 = fig.add_subplot(2, 3, 2, projection='3d')  
        surf1 = ax2.plot_surface(X, Y, u_true_grid, cmap='viridis', alpha=0.8)  
        ax2.set_xlabel('x')  
        ax2.set_ylabel('y')  
        ax2.set_zlabel('u(x,y)')  
        ax2.set_title('Analytical Solution: u = x² e^(-y)')  
        fig.colorbar(surf1, ax=ax2, shrink=0.5)  
  
        # 3. PINN prediction (3D surface)  
        ax3 = fig.add_subplot(2, 3, 3, projection='3d')  
        surf2 = ax3.plot_surface(X, Y, u_pred_grid, cmap='viridis', alpha=0.8)  
        ax3.set_xlabel('x')  
        ax3.set_ylabel('y')  
        ax3.set_zlabel('u(x,y)')  
        ax3.set_title('PINN Prediction')  
        fig.colorbar(surf2, ax=ax3, shrink=0.5)  
  
        # 4. Error distribution (3D surface)  
        ax4 = fig.add_subplot(2, 3, 4, projection='3d')  
        error_grid = np.abs(u_pred_grid - u_true_grid)  
        surf3 = ax4.plot_surface(X, Y, error_grid, cmap='Reds', alpha=0.8)  
        ax4.set_xlabel('x')  
        ax4.set_ylabel('y')  
        ax4.set_zlabel('|Error|')  
        ax4.set_title('Absolute Error Distribution')  
        fig.colorbar(surf3, ax=ax4, shrink=0.5)  
  
        # 5. Scatter plot comparison (现在有测试数据了)  
        if x_test is not None and y_test is not None and u_pred is not None and u_true is not None:  
            ax5 = fig.add_subplot(2, 3, 5)  
            ax5.scatter(u_true, u_pred, alpha=0.6, s=20)  
            # Perfect prediction line (y=x)  
            min_val = min(np.min(u_true), np.min(u_pred))  
            max_val = max(np.max(u_true), np.max(u_pred))  
            ax5.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')  
            ax5.set_xlabel('Analytical Solution')  
            ax5.set_ylabel('PINN Prediction')  
            ax5.set_title('Prediction vs Truth')  
            ax5.legend()  
            ax5.grid(True)  
        else:  
            # 如果没有测试数据，显示提示信息  
            ax5 = fig.add_subplot(2, 3, 5)  
            ax5.text(0.5, 0.5, 'No Test Data Available',  
                     ha='center', va='center', transform=ax5.transAxes, fontsize=14)  
            ax5.set_title('Prediction vs Truth')  
  
        # 6. Error histogram  
        ax6 = fig.add_subplot(2, 3, 6)  
        if x_test is not None and u_pred is not None and u_true is not None:  
            errors = u_pred - u_true  
            ax6.hist(errors, bins=30, alpha=0.7, edgecolor='black')  
            ax6.set_xlabel('Error (Prediction - Truth)')  
            ax6.set_ylabel('Frequency')  
            ax6.set_title('Error Distribution')  
            ax6.axvline(x=0, color='red', linestyle='--', label='Zero Error')  
            ax6.legend()  
            ax6.grid(True)  
        else:  
            # 如果没有测试数据，显示提示信息  
            ax6.text(0.5, 0.5, 'No Test Data Available',  
                     ha='center', va='center', transform=ax6.transAxes, fontsize=14)  
            ax6.set_title('Error Distribution')  
  
        plt.tight_layout()  
        plt.show()  
  
    def compare_solutions_2d(self):  
        """Create 2D contour plots for better comparison"""  
        # Create a finer grid for 2D plots  
        x_grid = np.linspace(0, 1, 100)  
        y_grid = np.linspace(0, 1, 100)  
        X, Y = np.meshgrid(x_grid, y_grid)  
  
        def analytical_solution(x, y):  
            """Analytical solution: u(x,y) = x^2 * e^(-y)"""  
            return x ** 2 * np.exp(-y)  # 修正: x^2 * e^(-y)  
  
        # Analytical solution        u_true_grid = analytical_solution(X, Y)  
  
        # PINN prediction  
        self.eval()  
        with torch.no_grad():  
            x_flat = torch.tensor(X.flatten(), dtype=torch.float32)  
            y_flat = torch.tensor(Y.flatten(), dtype=torch.float32)  
            u_pred_grid = self(x_flat, y_flat).cpu().numpy().reshape(X.shape)  
  
        # Create contour plots  
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))  
  
        # Analytical solution  
        im1 = axes[0].contourf(X, Y, u_true_grid, levels=20, cmap='viridis')  
        axes[0].set_title('Analytical Solution: u = x² e^(-y)')  
        axes[0].set_xlabel('x')  
        axes[0].set_ylabel('y')  
        fig.colorbar(im1, ax=axes[0])  
  
        # PINN prediction  
        im2 = axes[1].contourf(X, Y, u_pred_grid, levels=20, cmap='viridis')  
        axes[1].set_title('PINN Prediction')  
        axes[1].set_xlabel('x')  
        axes[1].set_ylabel('y')  
        fig.colorbar(im2, ax=axes[1])  
  
        # Error  
        error_grid = np.abs(u_pred_grid - u_true_grid)  
        im3 = axes[2].contourf(X, Y, error_grid, levels=20, cmap='Reds')  
        axes[2].set_title('Absolute Error')  
        axes[2].set_xlabel('x')  
        axes[2].set_ylabel('y')  
        fig.colorbar(im3, ax=axes[2])  
  
        plt.tight_layout()  
        plt.show()  
  
  
if __name__ == '__main__':  
    model = PINN_TEST1()  
    model.training_step(epochs=1000)
```

示例图像 (约 600 次结果:) 
![[attachments/Pasted image 20250610161113.png|700]]

![[attachments/Pasted image 20250610161129.png|500]]

