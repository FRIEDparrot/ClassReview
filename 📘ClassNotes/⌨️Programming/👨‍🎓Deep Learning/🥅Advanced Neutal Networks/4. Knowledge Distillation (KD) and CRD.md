## 1.  Knowledge Distillation 
### (1) Concepts 
> [!HINT] Reference 
> Also see [Knowledge Distillation Blog](https://blog.csdn.net/qq_42864343/article/details/134693835)  and [Contrast Rrepresentation Distillation (CRD)](https://arxiv.org/abs/1910.10699) also, various kinds of CRD is implemented in [Github](https://github.com/HobbitLong/RepDistiller)

The Knowledge Distillation is **a very important method that is very extensively used in the real deployment of the model and ==transfer learning==**. For example, we want to **deploy a smaller model with reasonable precision rather than directly deploying the large model in order to save the calculation resources**.  

1. The Knowledge Distillation  use <b><mark style="background: transparent; color: orange">Teacher Model</mark></b> and <b><mark style="background: transparent; color: orange">Student Model</mark></b> as the 
![[attachments/Pasted image 20250618181753.png|550]]

the condensation of the trained  model includes : 
1. Knowledge Distillation 
2. Weight Quantization  
3. Branch Trimming  
4. Attention Transfer  

### (2) Traning Process  
We use the <b><mark style="background: transparent; color: orange">soft target output by Teacher's network</mark></b> to train the Student network. And also, we use <b><mark style="background: transparent; color: orange">Distillation Temperature for enlarging</mark></b> the difference output by the teachers : 
$$q_{i} = \frac{\exp \left(\frac{z_{i}}{ T}\right)}{\sum_{j} \exp \left(\frac{z_{j}}{T}\right)}$$
in that case,  higher $T$ will lead to smoother probablity distribution. And finally like this : 
![[attachments/Pasted image 20250618183956.png|700]]

This is for avoiding overfitting and  **reduce the  effect of the label noise**. 


In Knowledge Distillation, we <b><mark style="background: transparent; color: orange">train the student model by 
both training data and teacher</mark></b>:  
![[Excalidraw/4. Knowledge Distillation (KD) and CRD 2025-06-19 09.09.33|800]]
The Loss function combines two parts : 
1. **Soft labels at $T=t$ predicted by the Teacher and Student.**  (learning from teacher)
2. **Hard Labels at T = 1 predicted by the Student**.  (learning from the textbook) 

The Student network have the following properties : 
1. Zero-Sample Recognition  
2. Reduce the overfitting    

## 2. Usage of  RepDistilller Package   
All the relevant models and essays are elucidated in [KD Dissertation replication](https://blog.csdn.net/qq_52572775/article/details/138467295)   

We use `python train_teacher.py --model wrn_40_2` for the network.  

By default, it use  cifar100 as the dataset (stored in `.data`folder )  
and just **follow the command in `./scripts`** 

We note that the above Knowledge Distiller is traditional  KD method, which is implemented in `KD.py` as follows : 
```python fold title:KD.py
from __future__ import print_function  
  
import torch.nn as nn  
import torch.nn.functional as F  
  
  
class DistillKL(nn.Module):  
    """Distilling the Knowledge in a Neural Network"""  
    def __init__(self, T):  
        super(DistillKL, self).__init__()  
        self.T = T  
  
    def forward(self, y_s, y_t):  
        p_s = F.log_softmax(y_s/self.T, dim=1)  
        p_t = F.softmax(y_t/self.T, dim=1)  
        loss = F.kl_div(p_s, p_t, size_average=False) * (self.T**2) / y_s.shape[0]  
        return loss
```
