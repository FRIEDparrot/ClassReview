The main works are as follows : 
1. the introduction of tensor, and how the  model expect the tensor to be shaped. 
2. gradient descent method.  
3. fully connected  model for image classification  
4. end -to -end strategy  
5. metrics to  for identify the  weaknesses in training as 
6. deploy the python into c++ web program service 

- Popular Deeplearning FreameWork relationships : 
1. Theano and TensorFlow are low-level libs (which defines the computation graph)
2. Lasagne and Kreas are high-level wrappers  
3. Caffe2 as Pytorch's backend 
4. Also support for ONNX 
JAX -> autograd and JIT capable 

the **torch original lib are based on C++ librarys**. and this can be compiled to run with parallelism on GPUS. 

criterion (loss function) -> 

torch.nn.parallel and torch.distributed model can be employed. 

ONNX is a standard  format for neural network, and <mark style="background: transparent; color: red">Pytorch provides a  way to compile the  models through <b>Torch Script</b></mark>. 

most used modules are as follows : 
```python
import torch
from torch import nn
import torch.optim as optim 
import torch.nn.functional as F

class MyModule(torch.nn.Module):  
    """  
    Transform for converting video frames as a list of tensors.    
    """    
    def __init__(self):  
        super().__init__()
```

## 1. Using Pre-Trained Models 
Go to https://pytorch.org/hub/ to see more repos. 
We firstly use the video classification model for the 
If we want to load the model, just use : 
```python
import warnings
import torch  
import torchvision  
from typing import Dict  
import json  
import urllib  
from torchvision.transforms import Compose, Lambda  
from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo  
from pytorchvideo.data.encoded_video import EncodedVideo  
from pytorchvideo.transforms import  ApplyTransformToKey, ShortSideScale, UniformTemporalSubsample, UniformCropVideo  
  
model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)  
device = "cpu"  
model = model.eval().to(device)  
  
model = torch.nn.DataParallel(model)  
  
json_url = "https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json"  
json_filename = "kinetics_classnames.json"  
  
try:  
    urllib.URLopener().retrieve(json_url, json_filename)  
except:  
    warnings.warn("request failed, trying with urllib2 instead")  
    urllib.request.urlretrieve(json_url, json_filename)  
  
with open(json_filename, "r") as f:  
    kinetics_classnames = json.load(f)  
  
# Create an id to label name mapping  
kinetics_id_to_classname = {}  
for k, v in kinetics_classnames.items():  
    kinetics_id_to_classname[v] = str(k).replace('"', "")  
  
side_size = 256  
mean = [0.45, 0.45, 0.45]  
std = [0.225, 0.225, 0.225]  
crop_size = 256  
num_frames = 32  
sampling_rate = 2  
frames_per_second = 30  
slowfast_alpha = 4  
num_clips = 10  
num_crops = 3  
  s
class PackPathway(torch.nn.Module):  
    """  
    Transform for converting video frames as a list of tensors.    
    """    
    def __init__(self):  
        super().__init__()  
  
    def forward(self, frames: torch.Tensor):  
        fast_pathway = frames  
        # Perform temporal sampling from the fast pathway.  
        slow_pathway = torch.index_select(  
            frames,  
            1,  
            torch.linspace(  
                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha  
            ).long(),  
        )  
        frame_list = [slow_pathway, fast_pathway]  
        return frame_list  
  
transform = ApplyTransformToKey(  
    key="video",  
    transform=Compose(  
        [  
            UniformTemporalSubsample(num_frames),  
            Lambda(lambda x: x / 255.0),  
            NormalizeVideo(mean, std),  
            ShortSideScale(  
                size=side_size  
            ),  
            CenterCropVideo(crop_size),  
            PackPathway()  
        ]  
    ),  
)  
  
"""  
Load the video and transform it to the input format required by the model.  
"""  
# The duration of the input clip is also specific to the model.  
clip_duration = (num_frames * sampling_rate) / frames_per_second  
  
# download example video  
url_link = "https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4"  
video_path = 'archery.mp4'  
try:  
    urllib.URLopener().retrieve(url_link, video_path)  
except:  
    urllib.request.urlretrieve(url_link, video_path)  
  
# Select the duration of the clip to load by specifying the start and end duration  
# The start_sec should correspond to where the action occurs in the video  
start_sec = 0  
end_sec = start_sec + clip_duration  
  
# Initialize an EncodedVideo helper class and load the video  
video = EncodedVideo.from_path(video_path)  
  
# Load the desired clip  
video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)  
  
# Apply a transform to normalize the video input  
video_data = transform(video_data)  
  
# Move the inputs to the desired device  
inputs = video_data["video"]  
inputs = [i.to(device)[None, ...] for i in inputs]  
  
""" Get Predictions """  
# Pass the input clip through the model  
preds = model(inputs)  
  
# Get the predicted classes  
post_act = torch.nn.Softmax(dim=1)  
preds = post_act(preds)  
pred_classes = preds.topk(k=5).indices[0]  
  
# Map the predicted classes to the label names  
pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]  
print("Top 5 predicted labels: %s" % ", ".join(pred_class_names))
```

For the issue of `No module named 'torchvision.transforms.functional_tensor'`, referring to [this GitHub link](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/13985), modifying the source code from `import torchvision.transforms.functional_tensor as F_t` to `import torchvision.transforms.functional as F_t` allows the compilation to succeed.

another example is the Image recognition model **ImageNet Large Scale Recognition**, which is downloaded at https://image-net.org/ 

```
model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)
```

dataset is downloaded at https://www.kaggle.com/c/imagenet-object-localization-challenge, refer to [[üìòClassNotes/‚å®Ô∏èProgramming/üë®‚ÄçüéìDeep Learning/PytorchÊ∑±Â∫¶Â≠¶‰π†Ê°à‰æãÂÆûÊàò/implements/Download Dataset by API from kaggle|Download Dataset]] which can download by:
![[attachments/Pasted image 20241009101028.png]]
and run the code in pycharm Termial

