## 三、朴素贝叶斯方法整理
### (1) 朴素贝叶斯方法的推导
参考 https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes (上面也给出了大量其他机器学习算法的推导)

“naive” assumption of conditional independence between every pair of features given the value of the class variable. (see https://en.wikipedia.org/wiki/Naive_Bayes_classifier) 
![[attachments/Pasted image 20240905231216.png]]

朴素贝叶斯方法假设<mark style="background: transparent; color: red">y值和x值是一一对应的, 即在一个确定的x下仅有一个y可以出现, 有</mark> 
$$\Large\boxed{P(x_{i} | y , x_{1},  x_{i-1} , \dots x_{i+1}, \dots  x_{n}) = P(x_{i}| y)}$$
同时, 将 $P (x_1, x_2, \dots  x_{n} | y)$ 以边缘概率密度乘积表示, 由于相互独立,将$P(x_i, x_i+1, \dots x_{n}| y )$采用乘积进行表示, 同时, 代入上式, 并采用 [MAP estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) 得到: 
$$P (y | x_{1}, x_{2} , \dots x_{n})  \propto P(y) \prod^{n}_{i=1} P(x_{i}|y)$$
代入上式, 有:
$$\hat{y} = \text{arg }\max_{y}P(y) \prod^{n}_{i=1}  P(x_{i}|y)$$
这个实际上类似于[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/深度学习算法原理(sklearn)/补充知识/2. MAP estimation(最大单后验可能性估计)|2. MAP estimation(最w大单后验可能性估计)]], 同时, 不同朴素贝叶斯模型的区别主要是 $P(x_i|y)$ 不相同。

> [!NOTE] 朴素贝叶斯算法的文本分类编程和预测原理
> 在过程中, 会统计每一项在数据集中出现的概率$P(y_i)$,  然后创建一个 n * m 的稀疏矩阵, 其中 n 为文档个数d, m 为词汇个数; (最简单的办法是先用一个稀疏矩阵存各个词的出现次数矩阵), 然后根据公式计算 tf-idf 值。
> 此时我们获得了 tf-idf 矩阵, 但仍然无法预测。
> 预测方法是构建一个 $P(x|y_i)$ 的矩阵, 其中行数为分类的个数($y_{j$), 列数为词语个数(x_i), 并先采用<mark style="background: transparent; color: red"> tf-idf 值在各个同一类文章中的平均值</mark>, 作为出现的概率值。最后进行归一化得到 P(x|y)矩阵。P(y)可以另行计算。
> 按照朴素贝叶斯方法获取其中的 $\hat{y}$ 作为预测值

### (2) 高斯朴素贝叶斯方法
高斯朴素贝叶斯方法, 将似然函数均视为高斯函数, 并且变量独立同分布:
$$P(x_{i}| y) = \frac{1}{\sqrt{2 \pi  \sigma_{y}^{2}} }\exp\left(- \frac{(x_{i} - \mu )^{2}}{2 \sigma_{y}^{2}}\right)$$

> [!CAUTION] 说明
> 高斯朴素贝叶斯认为, 样本的取值是连续的, 同时服从近似的高斯分布。因此可用于 iris 数据集中。

使用方法直接采用 
```python 
from sklearn.datasets import load_iris  
from sklearn.naive_bayes import GaussianNB  
from sklearn.model_selection import train_test_split  
  
[data, target] = load_iris(return_X_y=True)  
  
X_train, X_test, y_train, y_test = train_test_split(data, target, train_size= 0.8,test_size=0.2, random_state=0)  
gnb = GaussianNB()  
predictor = gnb.fit(X_train, y_train)  
y_pred = gnb.predict(X_test)
print(y_pred, y_test)
print("failed to predict", (y_test!=y_pred).sum(), "samples")
```

### (3) MNB, CNB, BNB 及其说明
1. **多项式朴素贝叶斯** : 针对于<mark style="background: transparent; color: red">多项式分布的数据</mark>, 总计特征数量为 $n$ 个, $\theta_{y}= (\theta_{y1}, \theta_{y2}, \dots  \theta_{yn})$ ,   其中$\theta_{yi} = P(x_{i}|y)$，为<mark style="background: transparent; color: red">特征出现在样本中的数量或者概率</mark>。因此，<b><mark style="background: transparent; color: blue">feature 为频率时, 才能考虑采用多项式朴素贝叶斯方法</mark></b>(features are counts or frequencies, i.e.  non-negative integers) 
```python 
from sklearn.naive_bayes import MultinomialNB
``` 
参数 $\theta_y$ 通过<mark style="background: transparent; color: red">相对频率计数</mark>进行计算: 
$$\Large \boxed{\hat{\theta}_{yi} = \frac{N_{yi} + \alpha}{N_{y} + \alpha n }}$$
其中, $N_{yi} = \sum_{x \in T} x_{i}$ 是特征 i 在训练集 T 中 分类y下出现的次数, 而 $N_{y}= \sum^{n}_{i=1} N_{yi}$ 为分类y下所有特征的总量; $\alpha$ 为平滑因子, 并满足 $\alpha \geq  0$ 以防止 $\hat{\theta}_{yi}$ 出现0的计算结果导致预测错误。当 $\alpha = 1$ 时为 Laplace 平滑, $\alpha < 1$ 为 Lidstone 平滑。<mark style="background: transparent; color: red">然后将上述函数作为最大似然函数</mark>进行预测,  

以频率为基础的训练数据示意如下: 其中特征数n为3, 

| Class\\features       | buy             | now           | free           | Total Words($N_y$) |
| --------------------- | --------------- | ------------- | -------------- | ------------------ |
| Spam                  | ==20  (N_yi)==      | ==5==             | ==10==             | 35                 |
| Not Spam              | ==5==               | ==15==            | ==5==              | 25                 |
| total count $N_{yi}$  | 25              | 20            | 15             | 60                 |
|                       |                 |               |                |                    |
| theta_yi(possibility) |                 |               |                |                    |
| spam                  | 20 +  a/35 + 3a | 5 + a/35 + 3a | 10 + a/35 + 3a |                    |
| Not spam              | ..              | ..            | ..             |                    |

可以参考 https://www.geeksforgeeks.org/multinomial-naive-bayes/ 得到<mark style="background: transparent; color: red">最终采用的最大似然估计预测公式</mark>:
$$P(D|y) = \prod^{n}_{i=1}   \hat{\theta}_{y, i}^{x_{i}}\qquad   \hat{c} = \text{arg} \max P(D|y)$$
例如: 对于 buy = 5, now = 14 , free = 3 的部分, 取laplace 平滑可以计算出 Not Spam 分类概率为:
$$\left(\frac{6}{28}\right)^{5} * \left(\frac{15}{28}\right)^{14} * \left(\frac{5}{28}\right)^{3} $$
为 Spam 的分类概率为:
$$\left(\frac{21}{38}\right)^{5} * \left(\frac{5}{38}\right)^{14}  * \left(\frac{10}{38}\right)^{3}$$

2. <b><mark style="background: transparent; color: blue">互补朴素贝叶斯</mark></b>(Complement naive bayes,CNB) : 
调用方法是 `from sklearn.naive_bayes import ComplementNB` 
其中 BernoulliNB <mark style="background: transparent; color: red">认为特征是二值量, 即均为 0 或者 1</mark>,

CNB <mark style="background: transparent; color: red">仍然是以频率为样本特征 feature 的</mark>。采用每个类的**补集数据**,计算模型权重, (一般CNB的参数估计比MNB更加稳定). 对于CNB, 权重计算为:
$$\hat{\theta}_{ci} = \frac{\alpha_{i}  + \sum^{n}_{j: y_{j}\neq c} d_{ij} }{\alpha + \sum_{j: y_{j}\neq c } \sum _{k}d_{kj}}$$
其中 $\alpha = \sum_{i} \alpha_{i}$, d_ij 可以视为上例中的数量(用于文本识别时也可以使用tf-idf代替), 然后采用如下规范化使得 $w_{ci}$ 变为权重形式(c为 class, 即上文的y):
$$w_{ci} = \log \hat{\theta}_{ci} \rightarrow  w_{ci} = \frac{w_{ci}}{\sum_{j} |w_{cj}|}$$
此时,可以采用如下公式计算所预测的分类类型:
$$\hat{c} = \text{arg} \min_{c} \sum_{i} t_{i} w_{ci}$$
其中$t_i$ 为样本中特征 i 的数量。即<mark style="background: transparent; color: red">互补朴素贝叶斯采用所计算出的权重中, 最小的值作为相应的预测类</mark>

3. 伯努利朴素贝叶斯
 `from sklear.n.naive_bayes import BernoulliNB`, 这种方法假设数据(特征)服从多变量的伯努利分布(参考[[📘ClassNotes/📐Mathmatics/🎣Probability Theory/第二章 随机变量及其分布#2. 二项分布(伯努利分布)|伯努利分布]]), 即非0即1的特性。
可以采用 `binarize` 参数决定是否将输入进行二值化. 同时必须将所有的输入全部转化为二值特征向量。(每个特征也应当为{0,1}值)

基本原理是
$$P(x_{i} |y) = P(x_{i} = 1 | y) x_{i} + (1 - P(x_{i} = 1| y)) (1 -x_{i})$$

4. CagoricalNB 分类朴素贝叶斯 
认为特征符合 [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution), 并采用下列来估计分类分布的概率:
$$P(x_{i} = t | y = c; \alpha)=  \frac{N_{tic} + \alpha }{N_{c}+ \alpha n_{i}}$$
其中$N_{tic}$为类别c中, 样本$x_i$中的分类 t 出现的次数; $n_i$ 为可用特征i中可用的分类数。

`````ad-note
title: 分类分布 (categorical distribution)
collapse: open

$$\Large\boxed{f (x = i | p ) = p_{i}}$$
其中, 元素 i 出现的概率为 $p_i$ , 同时 $\sum_{i =1}^{k} p_i = 1$, 上式也可以表达为:
$$f (x|\boldsymbol{p}) = \prod^{k}_{i=1} p_{i}^{[x = i]}$$
其中 x=i 时, $[x=i] = 1$, 否则为0 (Iverson bracket). 
`````

