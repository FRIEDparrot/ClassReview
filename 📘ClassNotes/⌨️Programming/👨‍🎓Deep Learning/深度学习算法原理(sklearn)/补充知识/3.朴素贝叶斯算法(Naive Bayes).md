## ä¸‰ã€æœ´ç´ è´å¶æ–¯æ–¹æ³•æ•´ç†
### (1) æœ´ç´ è´å¶æ–¯æ–¹æ³•çš„æ¨å¯¼
å‚è€ƒ https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes (ä¸Šé¢ä¹Ÿç»™å‡ºäº†å¤§é‡å…¶ä»–æœºå™¨å­¦ä¹ ç®—æ³•çš„æ¨å¯¼)

â€œnaiveâ€ assumption of conditional independence between every pair of features given the value of the class variable. (see https://en.wikipedia.org/wiki/Naive_Bayes_classifier) 
![[attachments/Pasted image 20240905231216.png]]

æœ´ç´ è´å¶æ–¯æ–¹æ³•å‡è®¾<mark style="background: transparent; color: red">yå€¼å’Œxå€¼æ˜¯ä¸€ä¸€å¯¹åº”çš„, å³åœ¨ä¸€ä¸ªç¡®å®šçš„xä¸‹ä»…æœ‰ä¸€ä¸ªyå¯ä»¥å‡ºç°, æœ‰</mark> 
$$\Large\boxed{P(x_{i} | y , x_{1},  x_{i-1} , \dots x_{i+1}, \dots  x_{n}) = P(x_{i}| y)}$$
åŒæ—¶, å°† $P (x_1, x_2, \dots  x_{n} | y)$ ä»¥è¾¹ç¼˜æ¦‚ç‡å¯†åº¦ä¹˜ç§¯è¡¨ç¤º, ç”±äºç›¸äº’ç‹¬ç«‹,å°†$P(x_i, x_i+1, \dots x_{n}| y )$é‡‡ç”¨ä¹˜ç§¯è¿›è¡Œè¡¨ç¤º, åŒæ—¶, ä»£å…¥ä¸Šå¼, å¹¶é‡‡ç”¨ [MAP estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) å¾—åˆ°: 
$$P (y | x_{1}, x_{2} , \dots x_{n})  \propto P(y) \prod^{n}_{i=1} P(x_{i}|y)$$
ä»£å…¥ä¸Šå¼, æœ‰:
$$\hat{y} = \text{arg }\max_{y}P(y) \prod^{n}_{i=1}  P(x_{i}|y)$$
è¿™ä¸ªå®é™…ä¸Šç±»ä¼¼äº[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/æ·±åº¦å­¦ä¹ ç®—æ³•åŸç†(sklearn)/è¡¥å……çŸ¥è¯†/2. MAP estimation(æœ€å¤§å•åéªŒå¯èƒ½æ€§ä¼°è®¡)|2. MAP estimation(æœ€wå¤§å•åéªŒå¯èƒ½æ€§ä¼°è®¡)]], åŒæ—¶, ä¸åŒæœ´ç´ è´å¶æ–¯æ¨¡å‹çš„åŒºåˆ«ä¸»è¦æ˜¯ $P(x_i|y)$ ä¸ç›¸åŒã€‚

> [!NOTE] æœ´ç´ è´å¶æ–¯ç®—æ³•çš„æ–‡æœ¬åˆ†ç±»ç¼–ç¨‹å’Œé¢„æµ‹åŸç†
> åœ¨è¿‡ç¨‹ä¸­, ä¼šç»Ÿè®¡æ¯ä¸€é¡¹åœ¨æ•°æ®é›†ä¸­å‡ºç°çš„æ¦‚ç‡$P(y_i)$,  ç„¶ååˆ›å»ºä¸€ä¸ª n * m çš„ç¨€ç–çŸ©é˜µ, å…¶ä¸­ n ä¸ºæ–‡æ¡£ä¸ªæ•°d, m ä¸ºè¯æ±‡ä¸ªæ•°; (æœ€ç®€å•çš„åŠæ³•æ˜¯å…ˆç”¨ä¸€ä¸ªç¨€ç–çŸ©é˜µå­˜å„ä¸ªè¯çš„å‡ºç°æ¬¡æ•°çŸ©é˜µ), ç„¶åæ ¹æ®å…¬å¼è®¡ç®— tf-idf å€¼ã€‚
> æ­¤æ—¶æˆ‘ä»¬è·å¾—äº† tf-idf çŸ©é˜µ, ä½†ä»ç„¶æ— æ³•é¢„æµ‹ã€‚
> é¢„æµ‹æ–¹æ³•æ˜¯æ„å»ºä¸€ä¸ª $P(x|y_i)$ çš„çŸ©é˜µ, å…¶ä¸­è¡Œæ•°ä¸ºåˆ†ç±»çš„ä¸ªæ•°($y_{j$), åˆ—æ•°ä¸ºè¯è¯­ä¸ªæ•°(x_i), å¹¶å…ˆé‡‡ç”¨<mark style="background: transparent; color: red"> tf-idf å€¼åœ¨å„ä¸ªåŒä¸€ç±»æ–‡ç« ä¸­çš„å¹³å‡å€¼</mark>, ä½œä¸ºå‡ºç°çš„æ¦‚ç‡å€¼ã€‚æœ€åè¿›è¡Œå½’ä¸€åŒ–å¾—åˆ° P(x|y)çŸ©é˜µã€‚P(y)å¯ä»¥å¦è¡Œè®¡ç®—ã€‚
> æŒ‰ç…§æœ´ç´ è´å¶æ–¯æ–¹æ³•è·å–å…¶ä¸­çš„ $\hat{y}$ ä½œä¸ºé¢„æµ‹å€¼

### (2) é«˜æ–¯æœ´ç´ è´å¶æ–¯æ–¹æ³•
é«˜æ–¯æœ´ç´ è´å¶æ–¯æ–¹æ³•, å°†ä¼¼ç„¶å‡½æ•°å‡è§†ä¸ºé«˜æ–¯å‡½æ•°, å¹¶ä¸”å˜é‡ç‹¬ç«‹åŒåˆ†å¸ƒ:
$$P(x_{i}| y) = \frac{1}{\sqrt{2 \pi  \sigma_{y}^{2}} }\exp\left(- \frac{(x_{i} - \mu )^{2}}{2 \sigma_{y}^{2}}\right)$$

> [!CAUTION] è¯´æ˜
> é«˜æ–¯æœ´ç´ è´å¶æ–¯è®¤ä¸º, æ ·æœ¬çš„å–å€¼æ˜¯è¿ç»­çš„, åŒæ—¶æœä»è¿‘ä¼¼çš„é«˜æ–¯åˆ†å¸ƒã€‚å› æ­¤å¯ç”¨äº iris æ•°æ®é›†ä¸­ã€‚

ä½¿ç”¨æ–¹æ³•ç›´æ¥é‡‡ç”¨ 
```python 
from sklearn.datasets import load_iris  
from sklearn.naive_bayes import GaussianNB  
from sklearn.model_selection import train_test_split  
  
[data, target] = load_iris(return_X_y=True)  
  
X_train, X_test, y_train, y_test = train_test_split(data, target, train_size= 0.8,test_size=0.2, random_state=0)  
gnb = GaussianNB()  
predictor = gnb.fit(X_train, y_train)  
y_pred = gnb.predict(X_test)
print(y_pred, y_test)
print("failed to predict", (y_test!=y_pred).sum(), "samples")
```

### (3) MNB, CNB, BNB åŠå…¶è¯´æ˜
1. **å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯** : é’ˆå¯¹äº<mark style="background: transparent; color: red">å¤šé¡¹å¼åˆ†å¸ƒçš„æ•°æ®</mark>, æ€»è®¡ç‰¹å¾æ•°é‡ä¸º $n$ ä¸ª, $\theta_{y}= (\theta_{y1}, \theta_{y2}, \dots  \theta_{yn})$ ,   å…¶ä¸­$\theta_{yi} = P(x_{i}|y)$ï¼Œä¸º<mark style="background: transparent; color: red">ç‰¹å¾å‡ºç°åœ¨æ ·æœ¬ä¸­çš„æ•°é‡æˆ–è€…æ¦‚ç‡</mark>ã€‚å› æ­¤ï¼Œ<b><mark style="background: transparent; color: blue">feature ä¸ºé¢‘ç‡æ—¶, æ‰èƒ½è€ƒè™‘é‡‡ç”¨å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æ–¹æ³•</mark></b>(features are counts or frequencies, i.e.  non-negative integers) 
```python 
from sklearn.naive_bayes import MultinomialNB
``` 
å‚æ•° $\theta_y$ é€šè¿‡<mark style="background: transparent; color: red">ç›¸å¯¹é¢‘ç‡è®¡æ•°</mark>è¿›è¡Œè®¡ç®—: 
$$\Large \boxed{\hat{\theta}_{yi} = \frac{N_{yi} + \alpha}{N_{y} + \alpha n }}$$
å…¶ä¸­, $N_{yi} = \sum_{x \in T} x_{i}$ æ˜¯ç‰¹å¾ i åœ¨è®­ç»ƒé›† T ä¸­ åˆ†ç±»yä¸‹å‡ºç°çš„æ¬¡æ•°, è€Œ $N_{y}= \sum^{n}_{i=1} N_{yi}$ ä¸ºåˆ†ç±»yä¸‹æ‰€æœ‰ç‰¹å¾çš„æ€»é‡; $\alpha$ ä¸ºå¹³æ»‘å› å­, å¹¶æ»¡è¶³ $\alpha \geq  0$ ä»¥é˜²æ­¢ $\hat{\theta}_{yi}$ å‡ºç°0çš„è®¡ç®—ç»“æœå¯¼è‡´é¢„æµ‹é”™è¯¯ã€‚å½“ $\alpha = 1$ æ—¶ä¸º Laplace å¹³æ»‘, $\alpha < 1$ ä¸º Lidstone å¹³æ»‘ã€‚<mark style="background: transparent; color: red">ç„¶åå°†ä¸Šè¿°å‡½æ•°ä½œä¸ºæœ€å¤§ä¼¼ç„¶å‡½æ•°</mark>è¿›è¡Œé¢„æµ‹,  

ä»¥é¢‘ç‡ä¸ºåŸºç¡€çš„è®­ç»ƒæ•°æ®ç¤ºæ„å¦‚ä¸‹: å…¶ä¸­ç‰¹å¾æ•°nä¸º3, 

| Class\\features       | buy             | now           | free           | Total Words($N_y$) |
| --------------------- | --------------- | ------------- | -------------- | ------------------ |
| Spam                  | ==20  (N_yi)==      | ==5==             | ==10==             | 35                 |
| Not Spam              | ==5==               | ==15==            | ==5==              | 25                 |
| total count $N_{yi}$  | 25              | 20            | 15             | 60                 |
|                       |                 |               |                |                    |
| theta_yi(possibility) |                 |               |                |                    |
| spam                  | 20 +  a/35 + 3a | 5 + a/35 + 3a | 10 + a/35 + 3a |                    |
| Not spam              | ..              | ..            | ..             |                    |

å¯ä»¥å‚è€ƒ https://www.geeksforgeeks.org/multinomial-naive-bayes/ å¾—åˆ°<mark style="background: transparent; color: red">æœ€ç»ˆé‡‡ç”¨çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡é¢„æµ‹å…¬å¼</mark>:
$$P(D|y) = \prod^{n}_{i=1}   \hat{\theta}_{y, i}^{x_{i}}\qquad   \hat{c} = \text{arg} \max P(D|y)$$
ä¾‹å¦‚: å¯¹äº buy = 5, now = 14 , free = 3 çš„éƒ¨åˆ†, å–laplace å¹³æ»‘å¯ä»¥è®¡ç®—å‡º Not Spam åˆ†ç±»æ¦‚ç‡ä¸º:
$$\left(\frac{6}{28}\right)^{5} * \left(\frac{15}{28}\right)^{14} * \left(\frac{5}{28}\right)^{3} $$
ä¸º Spam çš„åˆ†ç±»æ¦‚ç‡ä¸º:
$$\left(\frac{21}{38}\right)^{5} * \left(\frac{5}{38}\right)^{14}  * \left(\frac{10}{38}\right)^{3}$$

2. <b><mark style="background: transparent; color: blue">äº’è¡¥æœ´ç´ è´å¶æ–¯</mark></b>(Complement naive bayes,CNB) : 
è°ƒç”¨æ–¹æ³•æ˜¯ `from sklearn.naive_bayes import ComplementNB` 
å…¶ä¸­ BernoulliNB <mark style="background: transparent; color: red">è®¤ä¸ºç‰¹å¾æ˜¯äºŒå€¼é‡, å³å‡ä¸º 0 æˆ–è€… 1</mark>,

CNB <mark style="background: transparent; color: red">ä»ç„¶æ˜¯ä»¥é¢‘ç‡ä¸ºæ ·æœ¬ç‰¹å¾ feature çš„</mark>ã€‚é‡‡ç”¨æ¯ä¸ªç±»çš„**è¡¥é›†æ•°æ®**,è®¡ç®—æ¨¡å‹æƒé‡, (ä¸€èˆ¬CNBçš„å‚æ•°ä¼°è®¡æ¯”MNBæ›´åŠ ç¨³å®š). å¯¹äºCNB, æƒé‡è®¡ç®—ä¸º:
$$\hat{\theta}_{ci} = \frac{\alpha_{i}  + \sum^{n}_{j: y_{j}\neq c} d_{ij} }{\alpha + \sum_{j: y_{j}\neq c } \sum _{k}d_{kj}}$$
å…¶ä¸­ $\alpha = \sum_{i} \alpha_{i}$, d_ij å¯ä»¥è§†ä¸ºä¸Šä¾‹ä¸­çš„æ•°é‡(ç”¨äºæ–‡æœ¬è¯†åˆ«æ—¶ä¹Ÿå¯ä»¥ä½¿ç”¨tf-idfä»£æ›¿), ç„¶åé‡‡ç”¨å¦‚ä¸‹è§„èŒƒåŒ–ä½¿å¾— $w_{ci}$ å˜ä¸ºæƒé‡å½¢å¼(cä¸º class, å³ä¸Šæ–‡çš„y):
$$w_{ci} = \log \hat{\theta}_{ci} \rightarrow  w_{ci} = \frac{w_{ci}}{\sum_{j} |w_{cj}|}$$
æ­¤æ—¶,å¯ä»¥é‡‡ç”¨å¦‚ä¸‹å…¬å¼è®¡ç®—æ‰€é¢„æµ‹çš„åˆ†ç±»ç±»å‹:
$$\hat{c} = \text{arg} \min_{c} \sum_{i} t_{i} w_{ci}$$
å…¶ä¸­$t_i$ ä¸ºæ ·æœ¬ä¸­ç‰¹å¾ i çš„æ•°é‡ã€‚å³<mark style="background: transparent; color: red">äº’è¡¥æœ´ç´ è´å¶æ–¯é‡‡ç”¨æ‰€è®¡ç®—å‡ºçš„æƒé‡ä¸­, æœ€å°çš„å€¼ä½œä¸ºç›¸åº”çš„é¢„æµ‹ç±»</mark>

3. ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯
 `from sklear.n.naive_bayes import BernoulliNB`, è¿™ç§æ–¹æ³•å‡è®¾æ•°æ®(ç‰¹å¾)æœä»å¤šå˜é‡çš„ä¼¯åŠªåˆ©åˆ†å¸ƒ(å‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/ğŸ£Probability Theory/ç¬¬äºŒç«  éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ#2. äºŒé¡¹åˆ†å¸ƒ(ä¼¯åŠªåˆ©åˆ†å¸ƒ)|ä¼¯åŠªåˆ©åˆ†å¸ƒ]]), å³é0å³1çš„ç‰¹æ€§ã€‚
å¯ä»¥é‡‡ç”¨ `binarize` å‚æ•°å†³å®šæ˜¯å¦å°†è¾“å…¥è¿›è¡ŒäºŒå€¼åŒ–. åŒæ—¶å¿…é¡»å°†æ‰€æœ‰çš„è¾“å…¥å…¨éƒ¨è½¬åŒ–ä¸ºäºŒå€¼ç‰¹å¾å‘é‡ã€‚(æ¯ä¸ªç‰¹å¾ä¹Ÿåº”å½“ä¸º{0,1}å€¼)

åŸºæœ¬åŸç†æ˜¯
$$P(x_{i} |y) = P(x_{i} = 1 | y) x_{i} + (1 - P(x_{i} = 1| y)) (1 -x_{i})$$

4. CagoricalNB åˆ†ç±»æœ´ç´ è´å¶æ–¯ 
è®¤ä¸ºç‰¹å¾ç¬¦åˆ [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution), å¹¶é‡‡ç”¨ä¸‹åˆ—æ¥ä¼°è®¡åˆ†ç±»åˆ†å¸ƒçš„æ¦‚ç‡:
$$P(x_{i} = t | y = c; \alpha)=  \frac{N_{tic} + \alpha }{N_{c}+ \alpha n_{i}}$$
å…¶ä¸­$N_{tic}$ä¸ºç±»åˆ«cä¸­, æ ·æœ¬$x_i$ä¸­çš„åˆ†ç±» t å‡ºç°çš„æ¬¡æ•°; $n_i$ ä¸ºå¯ç”¨ç‰¹å¾iä¸­å¯ç”¨çš„åˆ†ç±»æ•°ã€‚

`````ad-note
title: åˆ†ç±»åˆ†å¸ƒ (categorical distribution)
collapse: open

$$\Large\boxed{f (x = i | p ) = p_{i}}$$
å…¶ä¸­, å…ƒç´  i å‡ºç°çš„æ¦‚ç‡ä¸º $p_i$ , åŒæ—¶ $\sum_{i =1}^{k} p_i = 1$, ä¸Šå¼ä¹Ÿå¯ä»¥è¡¨è¾¾ä¸º:
$$f (x|\boldsymbol{p}) = \prod^{k}_{i=1} p_{i}^{[x = i]}$$
å…¶ä¸­ x=i æ—¶, $[x=i] = 1$, å¦åˆ™ä¸º0 (Iverson bracket). 
`````

