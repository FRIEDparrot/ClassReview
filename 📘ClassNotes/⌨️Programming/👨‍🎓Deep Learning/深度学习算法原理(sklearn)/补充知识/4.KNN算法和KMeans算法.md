### 一、KNN算法
KNN(K-Nearest Neighbor Classification)算法, 也叫K最邻近分类算法, 是一种常用的识别，分类算法。
KNN算法要求分成的类别是已知的, 且训练样本点的类别均已知。

基本过程如下:
1. **对于每个点, 求解数据点到其余各个训练样本点的距离d**。
2. **求解对应的最邻近的K个训练样本点**。
3. 判断样本点中最多的点的类别， 并将该点与其他归为一类。 
优点是简单方便, 但是对噪声数据比较敏感(k不同，可以起到一定抗噪声作用)

需要说明的是, KNN算法<mark style="background: transparent; color: red">可以采用不同的距离度量公式</mark>, 就文本分类, **一般采用夹角余弦**(即和原点的夹角余弦值)作为距离公式。
$$\cos \theta = \frac{A\cdot B}{|A| |B|}$$
```python 
from numpy import linalg as la
def dist_cos(vector1, vector2):
	return dot(vector1, vector2)/(la.norm(vector1) * la.norm(vector2))
```

### 二、K-Means算法(K均值算法)
聚类要求： <mark style="background: transparent; color: red">类内距离小，类间距离大, 目标是未知</mark> 
1. 随机取K个点作为初始的聚类中心，代表各个聚类(一般选取数据点(注意是不同的数据点)，有时随机)
2. 计算求解各个样本相对于各个数据的距离。并将点归类到与其距离最近的点
3. 重新求解新类别的均值，再次调整中心位置
4. 重新求解，迭代至中心点近似不动为止

- K-Means算法没有训练样本数据，因此为无监督学习
- 影响: 初始中心，输入数据K的选择，距离的度量
- 改进：二分K-Means算法--> 避免收敛于局部的问题   划分的标准是最大限度降低SE值





