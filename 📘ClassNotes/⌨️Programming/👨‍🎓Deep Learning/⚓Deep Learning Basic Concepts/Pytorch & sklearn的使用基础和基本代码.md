# 1. Sklearn åº“çš„ä½¿ç”¨ 
sklearn æ˜¯æœ€å¸¸ç”¨çš„æœºå™¨å­¦ä¹ åº“, å…¶åŒ…å«åˆ†ç±», å›å½’ç®—æ³•, èšç±»ç®—æ³•, ç»´åº¦åŒ–ç®€, æ¨¡å‹é€‰æ‹©, äº¤å‰éªŒè¯, æ•°æ®é¢„å¤„ç†ç­‰ç­‰åŠŸèƒ½. ä¾‹å¦‚å²­å›å½’ï¼Œ æ”¯æŒå‘é‡æœº, KNN , æœ´ç´ è´å¶æ–¯, å†³ç­–æ ‘, ç‰¹å¾é€‰æ‹©, ä¿åºå›å½’ç­‰ç­‰ç®—æ³•

ç›¸æ¯”äº Pytorch,`sklearn` ä¸­çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ `AdaBoostClassifier`ï¼‰å¹¶ä¸ç›´æ¥æ”¯æŒé€šè¿‡ `torch.DataLoader` è¿›è¡Œå¤šæ¬¡è¾“å…¥å’Œè®­ç»ƒï¼Œå› ä¸º `sklearn` <mark style="background: transparent; color: red">æ¨¡å‹é€šå¸¸æ˜¯åŸºäºæ‰¹é‡è®­ç»ƒçš„</mark>ï¼Œè€Œä¸åƒ PyTorch é‚£æ ·é‡‡ç”¨é€æ‰¹ï¼ˆmini-batchï¼‰è®­ç»ƒçš„æ–¹å¼ã€‚ä»è€Œæä¾›æ›´å¥½çš„æ–¹æ³•ã€‚
##  åŸºç¡€éƒ¨åˆ†
### (1) sklearn ä¸­çš„ Bunch ç±»
Bunch æ˜¯ sklearn ä¸­æœ€å¸¸ç”¨çš„ç»“æ„, ç±»ä¼¼äºå­—å…¸ï¼Œå…·ä½“å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸPython/ğŸŒŸPython åŸºç¡€éƒ¨åˆ†/2. Python åŸºæœ¬æ•°æ®ç»“æ„å’Œå¯è§†åŒ–æ–¹æ³•|2. Python åŸºæœ¬æ•°æ®ç»“æ„å’Œå¯è§†åŒ–æ–¹æ³•]] 
```python 
from sklearn.datasets._base import Bunch
```
ä¸€èˆ¬åœ°, å¯ä»¥é€šè¿‡
```python
print(bunch.keys())
```
è·å–å…¶ä¸­çš„keysé”®, å¯¹äºbunchçš„keyséƒ¨åˆ†å‡å¯ä»¥é€šè¿‡ `.` è¿›è¡Œè®¿é—®ã€‚

> [!CAUTION] scipy ä¸­çš„ toarray æ–¹æ³•
> éœ€è¦æ³¨æ„çš„æ˜¯, scipy.spare.toarray() å’Œ numpy.array å¹¶ä¸ç›¸é€š, ç¨€ç–çŸ©é˜µå¯ä»¥é€šè¿‡ toarray() æ–¹æ³•è¿›è¡Œè½¬æ¢ä¸ºæ ‡å‡†çš„ numpy æ•°ç»„
### (2) æ•°æ®é›†è®­ç»ƒå’Œæµ‹è¯•éƒ¨åˆ†
sklearn å¯ä»¥ç›´æ¥è·å–å¤§é‡çš„å­¦ä¹ æ•°æ®é›†ï¼Œ åŒæ—¶æœ‰åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†åŠŸèƒ½, ä¸‹é¢ç®€ä»‹
<mark style="background: transparent; color: red">train_test_split åˆ†å‰²æ•°æ®é›†</mark> å’Œ <mark style="background: transparent; color: red">StandardScaler æ ‡å‡†åŒ–çš„æ–¹æ³•</mark> 

```python 
from sklearn.dataset import load_iris 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler   # æ ‡å‡†åŒ–å™¨ç±»å‹ 

X_train, X_test, y_train, y_test =  train_test_split(iris_data, iris_target, test_size=0.2, random_state=None)  
stdsca = StandardScaler.fit(X_train)
X_train_new = stdsca.transform(X_train)
X_test_new = stdsca.transform(X_test)
```

å¯¹äºæµ‹è¯•å‡†ç¡®åº¦, å¯ä»¥é‡‡ç”¨ [sklearn.metrics æ¨¡å—](https://scikit-learn.org/stable/api/sklearn.metrics.html)éƒ¨åˆ†: 
```python
from sklearn.metrics import accuracy_score, classification_report 
from sklearn.metrics import precision score, recall_score 
from sklearn.metrics import f1_score  # F1æ ‡å‡†, å‚è€ƒsklearn éƒ¨åˆ†
```

[`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score "sklearn.metrics.accuracy_score") ,[`average_precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score "sklearn.metrics.average_precision_score"), [`balanced_accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score "sklearn.metrics.balanced_accuracy_score"), [`brier_score_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss "sklearn.metrics.brier_score_loss") , 
æ­¤å¤–, æµ‹è¯• MSE, MAE ä¹Ÿå¯ä»¥é‡‡ç”¨ sklearn,metrics ä¸­ç›´æ¥import:

| [`mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error "sklearn.metrics.mean_squared_error")                 | Mean squared error regression loss.(MSE)        |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
| [`mean_squared_log_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error "sklearn.metrics.mean_squared_log_error") | Mean squared logarithmic error regression loss. |
| [`mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error "sklearn.metrics.mean_absolute_error")             | Mean absolute error regression loss.(MAE)       |

### (3) ç‰¹å¾æå–å’Œåˆ†ç±» 
```python
from sklearn.linear_model import LogisticRegression
```

å¦‚æœç”Ÿæˆåˆ†ç±»æ•°æ®ç¤ºä¾‹,  å¯ä»¥é‡‡ç”¨ `from sklearn.datasets import make_classification`  å¾—åˆ°
```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# ç”Ÿæˆç¤ºä¾‹æ•°æ® (1000 ä¸ªæ ·æœ¬, æ¯ä¸ªæ ·æœ¬ 20 ä¸ªç‰¹å¾)
X, y = make_classification(
    n_samples=1000,    # æ ·æœ¬æ•°é‡
    n_features=20,     # ç‰¹å¾æ•°é‡
    n_informative=15,  # æœ‰ç”¨ç‰¹å¾æ•°é‡
    n_redundant=5,     # å†—ä½™ç‰¹å¾æ•°é‡
    random_state=42
)

# åˆ’åˆ†æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# åˆå§‹åŒ– AdaBoostClassifier
clf = AdaBoostClassifier(n_estimators=50, random_state=42)

# è®­ç»ƒæ¨¡å‹
clf.fit(X_train, y_train)

# é¢„æµ‹æµ‹è¯•é›†
y_pred = clf.predict(X_test)

# è¾“å‡ºåˆ†ç±»æŠ¥å‘Š
print(classification_report(y_test, y_pred))
```

#### 1. å­—ç¬¦ä¸²æ ‡ç­¾ç¼–ç (LabelEncoder)
Label ç”¨äºè¿›è¡Œå­—ç¬¦ä¸²æ ‡ç­¾ç¼–ç ä¸ºæ•°å­—, ä¾‹å¦‚å¯¹äºå¦‚ä¸‹å†³ç­–æ ‘è¡¨æ ¼(å…¶ä¸­ç¬¬ä¸€åˆ—ä¸éœ€è¦ç¼–ç ): 
![[attachments/Pasted image 20240912111152.png]]
å…¶ä¸­é‡‡ç”¨ apply_map çš„æ–¹æ³•, å»é™¤äº†æ¯ä¸ªå­—ç¬¦ä¸²å…ƒç´ çš„ç©ºæ ¼; åŒæ—¶é‡‡ç”¨stripå»é™¤äº†æ¯ä¸ªæ ‡ç­¾çš„ç©ºæ ¼ã€‚  
```python
import pandas as pd  
from sklearn.preprocessing import LabelEncoder  # encoder labels  
  
data_raw = pd.DataFrame(pd.read_excel("seals_data.xlsx"))  
data_proceed = pd.DataFrame()  
label_encoder = LabelEncoder()  
  
# eliminate all the white space 
data_raw = data_raw.applymap(lambda x: x.strip() if isinstance(x, str) else x)  
data_proceed['è®¡æ•°'] = data_raw.iloc[:,0]  

# ç¼–ç å¹´é¾„, æ”¶å…¥, å­¦ç”Ÿ, ä¿¡èª‰, æ˜¯å¦è´­ä¹°å‡ åˆ— 
for column in data_raw.columns[1:]:
    data_proceed[str(column).strip()] = label_encoder.fit_transform(data_raw[column])  
print(data_proceed)
```
å¾—åˆ°å†³ç­–æ ‘ç¼–ç ç»“æœå¦‚ä¸‹:
![[attachments/Pasted image 20240912113356.png|200]]
#### 2. åˆ†ç±»å™¨: ç‰¹å¾é€‰æ‹©, æå–å’Œä¸»æˆåˆ†åˆ†æ
å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/1.æœºå™¨å­¦ä¹ ç®—æ³•å’Œæ–‡æœ¬åˆ†ç±»æŒ–æ˜(Naive Bayes)|æœºå™¨å­¦ä¹ ç®—æ³•å’Œæ–‡æœ¬åˆ†ç±»æŒ–æ˜(Naive Bayes)]], ç‰¹å¾æå–å¯ä»¥é€šè¿‡ `from sklearn import feature_extraction` å¾—åˆ°
å…¶ä¸­ç‰¹å¾æå–ä¸­æœ‰ image, text ç­‰ç­‰å‡ ä¸ªæ¨¡å—; å¸¸ç”¨åˆ° tf-idf æ¨¡å‹ç­‰æ–‡æœ¬ tf-idf ç‰¹å¾å‘é‡è®¡ç®—æ¨¡å‹ã€‚
![[attachments/Pasted image 20240910160939.png|300]]

```python
# ç‰¹å¾é€‰æ‹©å’Œç‰¹å¾æå–éƒ¨åˆ†
from sklearn import feature_extraction
from sklearn import feature_selection 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer 
vectorizer.fit_transform(contents_dictionary)
```

å¯¹äºå¤šæ ‡ç­¾åˆ†ç±»éƒ¨åˆ†, éœ€è¦é‡‡ç”¨å¤šè¾“å‡º
```python
from sklearn.multioutput import MultiOutputClassifier, MultiOutputEstimator
# å°†ä¸€ä¸ªéƒ¨åˆ†è½¬æ¢ä¸ºå¤šè¾“å‡º:
model = MultiOutputClassifier(MultinomialNB(alpha=0.01))
```

2. ä¸»æˆåˆ†åˆ†æ(PCAæ¨¡å—) å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/3. æ¨èç³»ç»Ÿå’Œéœ€æ±‚æœå¯»ç®—æ³•(CF,PCA,SVD)|3. æ¨èç³»ç»Ÿå’Œéœ€æ±‚æœå¯»ç®—æ³•(CF,PCA,SVD)]] éƒ¨åˆ†
```python 
from sklearn.decomposition import PCA
```

3. KNN, KMeans åˆ†ç±»å™¨
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
```


## 2. PipeLine çš„ä½¿ç”¨
PipeLine æ˜¯ sklearn ä¸­çš„ä¸€ä¸ªå¾ˆæ–¹ä¾¿çš„å®¹å™¨ç±»,  å¯ä»¥ç›´æ¥åœ¨ PipeLine ä¸­å®šä¹‰ Steps, ç„¶åè¿›è¡Œ fit æ“ä½œã€‚ (ç±»ä¼¼çš„æ˜¯ `torchvision.transforms.v2` çš„ `v2.Compose` å’Œ `nn.Sequential`)
è€Œé€šè¿‡ set_params æ–¹æ³•å¯ä»¥å¯¹ pipeline ä¸­çš„æŸä¸ªåç§°å¯¹è±¡å±æ€§è¿›è¡Œè®¾ç½®, å…·ä½“æ–¹æ³•æ˜¯é‡‡ç”¨ `__` è¿›è¡Œå‘½ååŒºåˆ†: 
å³ `set_params(name__param = new_param)` 

éœ€è¦åˆ†ç±»è¾¹ç•Œæ˜¾ç¤ºæ—¶, åˆ™å¯ä»¥é‡‡ç”¨  sklearn.inspection ä¸­çš„ DecisionBoundaryDisplay æ–¹æ³•ã€‚
xlabel å’Œ ylabel é€‰é¡¹å’Œç»˜å›¾ plt è®¾ç½®å®Œå…¨ç›¸åŒã€‚
ä¸‹å›¾é‡‡ç”¨ pipeline è¿›è¡Œåˆ†ç±»è¾¹ç•Œæ˜¾ç¤º
```python
from sklearn.pipeline import Pipeline
from sklearn.inspection import DecisionBoundaryDisplay

clf = Pipeline(  
    steps=[("scaler", StandardScaler()), ("kmeans", KMeans(n_clusters = 3))]  
)
clf.set_params(kmeans__max_iter = max_iter).fit(X_train, y_train)
disp = DecisionBoundaryDisplay.from_estimator(  
    clf,  
    X_test,  
    response_method="predict",  
    plot_method="pcolormesh",  
    xlabel=iris.feature_names[0],  
    ylabel=iris.feature_names[1],  
    shading="auto",  
    alpha=0.5,  
    ax=ax,  
)
```

éœ€è¦è¯´æ˜,  PipeLineä¸­çš„æ¯ä¸ªéƒ¨åˆ†å¿…é¡»æ¥å— x,y ä½œä¸º fit_predict çš„å‚æ•°, å› æ­¤å½“ fit ç­‰ä»…æ¥å—ä¸€ä¸ªå‡½æ•° x æ—¶, éœ€è¦é‡å®šä¹‰ç±»å‹ (é‡å®šä¹‰ SOM ç±»), å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/5. BP ç¥ç»ç½‘ç»œ, SOM ç¥ç»ç½‘ç»œå’Œ Boltzmannæœº#(2) é‡‡ç”¨ç¬¬ä¸‰æ–¹åº“å»ºç«‹ SOM ç½‘ç»œ|5. BP ç¥ç»ç½‘ç»œ, SOM ç¥ç»ç½‘ç»œå’Œ Boltzmannæœº]] 
```python
from sklearn_som.som import SOM
clf = Pipeline(steps=[  
    ('scaler', StandardScaler()),  
    ('som', CustomSOM(m=1, n=3, dim=4))  
]) 
```

### 3. è¶…å‚æ•°è°ƒä¼˜é—®é¢˜æ–¹æ³•

ä½¿ç”¨GridSearchCVå’ŒKFoldè¿›è¡Œäº¤å‰éªŒè¯
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.datasets import load_iris
 
# åŠ è½½ç¤ºä¾‹æ•°æ®é›†ï¼ˆè¿™é‡Œä»¥é¸¢å°¾èŠ±æ•°æ®é›†ä¸ºä¾‹ï¼‰
data = load_iris()
X = data.data
y = data.target
 
# å®šä¹‰Kæœ€è¿‘é‚»æ¨¡å‹
knn = KNeighborsClassifier()
 
# å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'n_neighbors': [3, 5, 7],
    'weights': ['uniform', 'distance']
}
#weightsï¼šæŒ‡å®šåœ¨é¢„æµ‹æ—¶å¦‚ä½•å¯¹é‚»å±…çš„æƒé‡è¿›è¡ŒåŠ æƒï¼Œå¸¸è§å–å€¼æœ‰uniformï¼ˆæ‰€æœ‰é‚»å±…æƒé‡ç›¸ç­‰ï¼‰å’Œdistanceï¼ˆæƒé‡ä¸è·ç¦»æˆåæ¯”ï¼‰ã€‚
 
# ä½¿ç”¨GridSearchCVå’ŒKFoldè¿›è¡Œäº¤å‰éªŒè¯
kf = KFold(n_splits=5, shuffle=True, random_state=0)
grid_search = GridSearchCV(knn, param_grid, cv=kf, scoring='accuracy')
grid_search.fit(X, y)
 
# è¾“å‡ºæœ€ä½³å‚æ•°å’Œæœ€ä½³å¾—åˆ†
print("æœ€ä½³å‚æ•°ï¼š", grid_search.best_params_)
print("æœ€ä½³å¾—åˆ†ï¼š", grid_search.best_score_)
```

# 2. PyTorch  
## 1. æ•°æ®ç±»å‹è½¬æ¢
å– tensor å€¼é‡‡ç”¨  item(): 
`label_item = torch.Tensor(label).item()`

ä¸€èˆ¬ç©ºç½‘ç»œé‡‡ç”¨å¦‚ä¸‹æ–¹æ³•å®šä¹‰:
```python
net = nn.Sequential(); 
net.add(nn.Dense(256, activation='relu'), nn.Dense(10))  
```

```python
# detach å–æ¶ˆæ±‚å¯¼è¿æ¥,  è€Œ numpy è½¬æ¢ä¸º numpy æ•°ç»„
y.detach.numpy(); 
```

## 2. åŸºæœ¬æ•°æ®æ“ä½œ
#### 1. unsqueeze,squeezeå’Œreshapeä½¿ç”¨
å¢åŠ ç»´åº¦æˆ–è€…å‡å°‘ç»´åº¦: 
torch.squeeze(input,dim=0) **ç§»é™¤æŸä¸ªç»´åº¦**
torch.unsqueeze(input,dim=0) **åœ¨æŸä¸ªç»´åº¦ä¸Šå¢åŠ ç»´åº¦** 
reshape çš„ä½¿ç”¨: åœ¨ pytorch ç­‰ä¸­, æˆ‘ä»¬å¸¸å¸¸ä½¿ç”¨ reshape å°†æ•°æ®é›†è½¬æ¢ä¸ºæŒ‡å®šçš„å½¢çŠ¶, ä¾‹å¦‚å¯¹äº (60000, 28,28) çš„æ•°æ®é›†, æˆ‘ä»¬ä¼šä½¿ç”¨ `x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)` å¾—åˆ° (60000,28,28,1) çš„4dæ•°æ®é›†, å³å°†æœ€åçš„28ä¸ªä¸­, æ¯ä¸ªå‡ä½œä¸ºè¾“å…¥æ‹†åˆ†å¼€;

```python title:squeezeä½¿ç”¨ç¤ºä¾‹
train[1].unsqueeze(0).shape
Out[12]: torch.Size([1, 28, 28])
A = train[1].unsqueeze(0).unsqueeze(0)
A.shape
Out[13]: torch.Size([1, 1, 28, 28]) 
A.squeeze(0).shape
Out[17]: torch.Size([1, 28, 28])
A.squeeze(0).squeeze(0).shape
Out[19]: torch.Size([28, 28])
A.squeeze([0,1]).shape
Out[22]: torch.Size([28, 28])
B.unsqueeze(dim=2).shape
Out[28]: torch.Size([28, 28, 1])
```

å¦å¤–ï¼Œpytorchä¸­å¤§å¤šæ•°çš„æ“ä½œéƒ½æ”¯æŒÂ `inplace`Â æ“ä½œï¼Œä¹Ÿå°±æ˜¯å¯ä»¥ç›´æ¥å¯¹ tensor è¿›è¡Œæ“ä½œè€Œä¸éœ€è¦å¦å¤–å¼€è¾Ÿå†…å­˜ç©ºé—´ï¼Œæ–¹å¼éå¸¸ç®€å•ï¼Œä¸€èˆ¬éƒ½æ˜¯åœ¨æ“ä½œçš„ç¬¦å·åé¢åŠ `_` 

#### 2. å¸¸ç”¨å‡½æ•°é›†å’Œè¿ç®— 
**torch.nn.functional** åŒ…å«äº†**ç»å¤§å¤šæ•°çš„å¸¸ç”¨å‡½æ•°**: ä¾‹å¦‚ reluï¼Œ softmax, sigmoid ç­‰ç­‰ã€‚

```python
def SoftmaxRegression(nn.Module):
	self.w = torch.normal(0,simga, size =(num_inputs, num_outpus), requires_grad=True)
	self.b = torch.zeros(num_outputs, requires_grad = True)
	self.optim = torch.optim.SGD(self.parameters(), lr = 0.01)
	
	 def parameters(self) : 
		return [self.w, self.b]
	 def forward(self):
		 return softmax(torch.matmul(X.reshape((-1,  self.W.shape[0])),  self.W) + self.b)
```

å¯¹äº minist æ•°æ®é›†, ç”±äºxçš„ç»´åº¦ä¸º 784, num_inputs = 784, num_outputs = 10. å³ w ä¸ºä¸€ä¸ª 784 x 10 çš„æƒé‡çŸ©é˜µã€‚

torch.gather(): å–æŸä¸ªtensorä¸­æŒ‡å®šä¸‹æ ‡çš„å…ƒç´ , ä½†æ˜¯éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•æŒ‡å®š:
```Python
For a 3-D tensor the output is specified by: 
out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
```

#### 3. å–æŸäº›è¡Œæˆ–è€…æŸäº›åˆ—çš„å…ƒç´ (torch.gather)
ä¾‹å¦‚gather(dim = 0) æ˜¯æŒ‰è¡Œå–å…ƒç´ , 
```python
a = tensor([[ 1,  2,  3,  4,  5], [ 6,  7,  8,  9, 10]])
a.gather(0, torch.tensor([[0, 1],[0,1]]))   # ç¬¬ä¸€åˆ—è¿›è¡Œgather, 
# tensor([[1, 7], [1, 7]]) 
a.gather(1, torch.tensor([[0, 1],[0,1]]))
```
å½“ dim = 0 æ—¶, å–æ³•ä¸ºå–æŒ‡å®šè¡Œç›¸åº”åˆ—çš„å…ƒç´ 
$$\left[\begin{matrix}
0  &|&1 \\  0&  |  & 1
\end{matrix}\right]$$
dim = 0 æ—¶, å–æ³•ä¸ºå–æŒ‡å®šåˆ—ç›¸åº”è¡Œçš„å…ƒç´ ã€‚ç¬¬ä¸€åˆ—å– 0 è¡Œ, 0è¡Œ, ç¬¬äºŒåˆ—å–1è¡Œ, 1è¡Œ; è€Œ
$$\left[\begin{matrix}
0  &1   \\   -  &  -  \\  0  & 1
\end{matrix}\right]$$
æ­¤æ—¶å¾—åˆ°çš„æ˜¯`[[1,2],[6,7]]`ã€‚ 
**è¯´æ˜: æœ€å¸¸è§çš„åº”ç”¨æ˜¯åœ¨äº¤å‰ç†µçš„è®¡ç®—ä¸­, è·å–åˆ°çš„æ¦‚ç‡çŸ©é˜µ y_pred(ä¾‹å¦‚784* 10), æ­£ç¡®ç›®æ ‡æ˜¯ç¦»æ•£çš„ç‚¹ y(0,1,2,3..9), æ­¤æ—¶éœ€è¦y_predä¸­,yæ‰€å¯¹åº”åˆ—çš„éƒ¨åˆ†**, å³å¯é‡‡ç”¨:
```python 
y_pred.gather(1, y.view(-1, 1)) 
```

ä½†å®é™…ä¸Šç”¨çš„å¤šçš„è¿˜æ˜¯å¦‚ä¸‹çš„æ–¹æ³•: 
```python
criterion =  nn.CrossEntropyLoss()
y_pred = model(X)
loss = criterion(y_pred, y)
```

## 3. åŸºæœ¬æ±‚å¯¼æ–¹æ³•
#### 1. åå‘ä¼ æ’­ç®—æ³•æ±‚å¯¼
ä»¥ä¸‹ä¸ºåŸºæœ¬çš„æ±‚å¯¼æ–¹å¼ä»£ç , å®é™…ä¸Šä»…éœ€è¦ torch.tensor ä¸­å®šä¹‰ `requires_grad = True` ä¹‹å, è®¡ç®—å¹¶é‡‡ç”¨ backward æ±‚å¯¼ :  
```python
m = torch.tensor([[2, 3]], dtype=torch.float, requires_grad=True) # æ„å»ºä¸€ä¸ª 1 x 2 çš„çŸ©é˜µ
# æ³¨æ„ï¼š[[]]æ˜¯å®šä¹‰è¡Œå‘é‡ï¼Œè€Œ[]å®šä¹‰åˆ—å‘é‡
n = torch.zeros(1, 2) # æ„å»ºä¸€ä¸ªç›¸åŒå¤§å°çš„ 0 çŸ©é˜µ
print(m)
print(n)
# é€šè¿‡ m ä¸­çš„å€¼è®¡ç®—æ–°çš„ n ä¸­çš„å€¼
print(m[0,0])
n[0, 0] = m[0, 0] ** 2
n[0, 1] = m[0, 1] ** 3
print(n)

n.backward(torch.ones_like(n)) # å°† (w0, w1) å–æˆ (1, 1), è¿›è¡Œè‡ªåŠ¨æ±‚å¯¼
print(m.grad)
# tensor([[ 4., 27.]], grad_fn=<CopySlices>)
```
å°†ä¸Šé¢çš„å¼å­å†™æˆæ•°å­¦å…¬å¼ï¼Œå¯ä»¥å¾—åˆ°: 
$$
n = (n_0,\ n_1) = (m_0^2,\ m_1^3) = (2^2,\ 3^3) 
$$
ä¸‹é¢æˆ‘ä»¬ç›´æ¥å¯¹ `n` è¿›è¡Œåå‘ä¼ æ’­ï¼Œä¹Ÿå°±æ˜¯æ±‚ `n` å¯¹ `m` çš„å¯¼æ•°ã€‚
è¿™æ—¶æˆ‘ä»¬éœ€è¦æ˜ç¡®è¿™ä¸ªå¯¼æ•°çš„å®šä¹‰ï¼Œå³å¦‚ä½•å®šä¹‰
$$
\frac{\partial n}{\partial m} = \frac{\partial (n_0,\ n_1)}{\partial (m_0,\ m_1)}
$$
$$
\frac{\partial n}{\partial m_0} = w_0 \frac{\partial n_0}{\partial m_0} + w_1 \frac{\partial n_1}{\partial m_0} = 2 m_0 + 0 = 2 \times 2 = 4
$$
$$
\frac{\partial n}{\partial m_1} = w_0 \frac{\partial n_0}{\partial m_1} + w_1 \frac{\partial n_1}{\partial m_1} = 0 + 3 m_1^2 = 3 \times 3^2 = 27
$$
#### 2. å¤šæ¬¡è‡ªåŠ¨æ±‚å¯¼
é€šè¿‡è°ƒç”¨ backward æˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€æ¬¡è‡ªåŠ¨æ±‚å¯¼ï¼Œ<mark style="background: transparent; color: red">å¦‚æœæˆ‘ä»¬å†è°ƒç”¨ä¸€æ¬¡ backwardï¼Œä¼šå‘ç°ç¨‹åºæŠ¥é”™ï¼Œæ²¡æœ‰åŠæ³•å†åšä¸€æ¬¡ã€‚è¿™æ˜¯å› ä¸º PyTorch é»˜è®¤åšå®Œä¸€æ¬¡è‡ªåŠ¨æ±‚å¯¼ä¹‹åï¼Œè®¡ç®—å›¾å°±è¢«ä¸¢å¼ƒäº†ï¼Œæ‰€ä»¥ä¸¤æ¬¡è‡ªåŠ¨æ±‚å¯¼éœ€è¦æ‰‹åŠ¨è®¾ç½®, å³è®¾å®š backward æ–¹æ³•ä¸­ retain_graph = True ä¿ç•™è®¡ç®—å›¾</mark> 
```python
x = torch.tensor([3], dtype=torch.float, requires_grad=True)
y = x * 2 + x ** 2 + 3
print(y)
y.backward(retain_graph=True) # è®¾ç½® retain_graph ä¸º True æ¥ä¿ç•™è®¡ç®—å›¾
print(x.grad)
```

> [!CAUTION] æ¢¯åº¦å½’é›¶
> æ³¨æ„ï¼šå¦‚æœæ˜¯å¾ªç¯è¿­ä»£æ±‚è§£æ¢¯åº¦çš„æƒ…å†µä¸‹, æˆ‘ä»¬å¸Œæœ›æ¯ä¸€æ¬¡è®¡ç®—å¹¶ä»…æ±‚è§£ä¸€æ¬¡æ¢¯åº¦ï¼Œåˆ™æ±‚å¯¼å®Œæ¯•ä¹‹å, éœ€è¦ä½¿ç”¨grad.data.zero_()æ¥å½’é›¶æ¢¯åº¦ï¼Œå¦åˆ™å°†ä¼šä½¿æ¢¯åº¦ç´¯åŠ ã€‚å…·ä½“å¦‚ä¸‹: 
>```python
>while (1) : 
>	w.requires_grad_(True);  # ask for the space for gradient.  
>	b.requires_grad_(True);  #  
>	try:  
>	    w.grad.zero_()  
>	    b.grad.zero_()  
>	except:  
>	    pass
>```

## äºŒã€åŸºæœ¬å›å½’æ¨¡å‹å’Œè®­ç»ƒæ­¥éª¤
### (1) åŸºæœ¬æ¨¡å‹è®­ç»ƒæ­¥éª¤
åœ¨ Pytorch ä¸­, æœ€åŸºæœ¬çš„éƒ¨åˆ†åŒ…æ‹¬å¦‚ä¸‹å‡ æ­¥(å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸPython/ğŸŒŸPython åŸºç¡€éƒ¨åˆ†/6. nn åº“ä½¿ç”¨ SGD è®­ç»ƒMINIST æ•°æ®é›†|6. nn åº“ä½¿ç”¨ SGD è®­ç»ƒMINIST æ•°æ®é›†]]): 

å¯¹äºè®­ç»ƒæ•°æ®é›†, ä¸€èˆ¬éœ€è¦è½¬æ¢ä¸º 4 ç»´å¼ é‡ï¼Œ å¦‚ (5000, 3, 32, 32),  åˆ†åˆ«å¯¹åº” (N, C, W, H)  
Nï¼šè¿™ä¸€ç»´é€šå¸¸ä»£è¡¨æ‰¹é‡å¤§å° (batch size)ï¼Œå³ä¸€æ¬¡æ€§è¾“å…¥åˆ°ç½‘ç»œä¸­çš„æ ·æœ¬æ•°é‡ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä¸€æ¬¡è¾“å…¥ 5000 ä¸ªæ ·æœ¬ã€‚
Cï¼šè¿™ä¸€ç»´è¡¨ç¤ºè¾“å…¥é€šé“æ•°ã€‚åœ¨å›¾åƒå¤„ç†ä¸­ï¼Œè¾“å…¥é€šé“é€šå¸¸å¯¹åº”äºé¢œè‰²é€šé“ï¼Œ`3`Â è¡¨ç¤º RGB å›¾åƒï¼ˆçº¢ã€ç»¿ã€è“ä¸‰ç§é¢œè‰²)
W, Hï¼šè¿™ä¸¤ä¸ªç»´åº¦åˆ†åˆ«è¡¨ç¤ºå›¾åƒçš„é«˜åº¦å’Œå®½åº¦ã€‚ 
#### 1. å®šä¹‰ç½‘ç»œæ¨¡å‹
1. ç»§æ‰¿ç»§æ‰¿ `nn.Module`, å¹¶è°ƒç”¨ `super().init()` 
2. **å®šä¹‰å˜æ¢ (forward) å‡½æ•°**
ä¸€èˆ¬åœ¨ forward å‡½æ•°ä¸­å®šä¹‰æ¯ä¸€å±‚å˜æ¢,  è€Œ 
self.net ç­‰ä¸€èˆ¬ä¹Ÿåœ¨   forward ä¸­è¢«è°ƒç”¨, å³å¯ä»¥ä¸ä½¿ç”¨ net, è€Œé‡‡ç”¨ forward å¾—åˆ°ç»“æœ, å®é™…ä¸Šéƒ½æ˜¯ä¸€ä¸ªè¾“å…¥ X 
```python
class CIFIAR_CNN(nn.Module):  
    def __init__(self, learning_rate = 0.02):  
        super(CIFIAR_CNN, self).__init__()  
  
        self.net = nn.Sequential(  
            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # é¦–å…ˆ, ç”±äºæ˜¯ RGB æ•°æ®, è¾“å…¥é€šé“æ•°ä¸º 3, å°ºå¯¸ä¸º (batch_size, 3, 32, 32)            nn.Conv2d( 32, 64, kernel_size=3, padding=1), # å½¢æˆ 64 *32 * 32 çš„è¾“å‡º  
            # nn.MaxPool2d(kernel_size=(2,2), stride=2),  # n * 64 * 16 * 16, or use the following:  
            nn.AdaptiveMaxPool2d((16, 16)) ,                # n * 64 * 16 * 16 target size  
            nn.AvgPool2d(kernel_size=(2,2), stride=(2,2)),  # n * 64 * 8 * 8  
            nn.Flatten(1, -1),    # å°† 64 * 8 * 8 çš„å¼ é‡å±•å¹³  
            nn.Linear(64 * 8 * 8, 512),  
            nn.ReLU(),         # æ”¾åœ¨ Linear ä¹‹å, æ¿€æ´»å‡½æ•°, ç”¨äºå®ç°éçº¿æ€§å­¦ä¹ , å¾—åˆ°æ›´å¤æ‚çš„æ¨¡å‹  
            nn.Linear(512, 10),  # CIFAR-10 has 10 classes, so output is 10 dimensions  
            nn.Softmax(dim=1)  # Softmax æ¿€æ´»å‡½æ•°ï¼šå°†è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ  
        )  
  
    def forward(self,X):  
        return self.net(X)
```
#### 2. åŠ è½½æ•°æ®å’Œè®­ç»ƒ
1. **åˆå§‹å˜æ¢å’ŒåŠ è½½æ•°æ®é›†**
æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ªå‡½æ•°æˆ–è€… torchvision.Compose() å®¹å™¨, ç„¶ååœ¨ transform ä¸­æŒ‡å®šå‚æ•°ï¼Œ ç„¶å
> [!NOTE] è¯´æ˜
> åœ¨åŠ è½½æ•°æ®å’Œè®­ç»ƒçš„ç±»ä¸­,  æˆ‘ä»¬ç›´æ¥å°†ç›¸åº”çš„ `self.model` æŒ‡å®šä¸ºåˆšåˆšå®šä¹‰çš„ç±»

**éœ€è¦è¯´æ˜**:  `transform` åªä¼šå½±å“åœ¨ `DataLoader` ä¸­æ‰¹é‡åŠ è½½æ•°æ®æ—¶çš„å›¾åƒã€‚å¦‚æœæ‚¨æƒ³**æŸ¥çœ‹ç»è¿‡ç¼©æ”¾å¤„ç†åçš„å›¾åƒï¼Œæ‚¨åº”è¯¥æŸ¥çœ‹ `train_loader` ä¸­çš„æ•°æ®ï¼Œè€Œä¸æ˜¯ `train_dataset.data`**ã€‚`train_loader` æ‰¹é‡åŠ è½½æ•°æ®æ—¶ï¼Œä¼šåº”ç”¨ `transform`ã€‚
```python
t = v2.Compose([  
    v2.ToDtype(torch.float32, scale=True),  
    v2.ToTensor(),  
    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  
])  
train_dataset = CIFAR10( root='./cifar10', train=True, download=True, transform=t)
```

2. **å®šä¹‰ä¸€ä¸ª DataLoader è¿­ä»£å™¨**
```python
self.train_loader = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=True)
```

3. **å®šä¹‰æŸå¤±å‡½æ•° (criterion æˆ–è€… loss) å’Œä¼˜åŒ–å™¨(optim)** 
```python
def criterion(y_true, y_pred):
	return nn.CrossEntropyLoss(y_true, y_pred)
```

å¦‚æœåˆ†ç±»æ–¹æ³•ä¾èµ–äºæ¢¯åº¦, åˆ™éœ€è¦å®šä¹‰ optim ä¼˜åŒ–å™¨: 
`optim` çš„ä½œç”¨åŒ…æ‹¬:
- **æ¢¯åº¦æ›´æ–°**ï¼šåŸºäºæ¨¡å‹å‚æ•°çš„å½“å‰æ¢¯åº¦è°ƒæ•´å‚æ•°å€¼ã€‚
- **æ§åˆ¶å­¦ä¹ ç‡**ï¼šå­¦ä¹ ç‡ï¼ˆ`learning_rate`ï¼‰æ˜¯ä¼˜åŒ–å™¨çš„å…³é”®è¶…å‚æ•°ï¼Œå†³å®šæ›´æ–°æ­¥ä¼å¤§å°ã€‚
```python
self.optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # å¸¸ç”¨: SGD, Adam, RMSprop, AdaGrad ç­‰ç­‰, 
# å…¶ä¸­ momentum ä¸ºç”¨äºåŠ é€Ÿæ”¶æ•›çš„åŠ¨é‡è®¾ç½®é¡¹
```
éœ€è¦è¯´æ˜,  å¦‚æœä¸ä½¿ç”¨ optim ä¼˜åŒ–å™¨, å¯ä»¥æ‰‹åŠ¨è¿›è¡Œå‚æ•°æ›´æ–°:
```python
with torch.no_grad():  # ç¦æ­¢è®¡ç®—å›¾æ„å»ºï¼Œç›´æ¥æ›´æ–°å‚æ•°
    for param in model.parameters():
        param -= learning_rate * param.grad  # æ‰‹åŠ¨æ›´æ–°å‚æ•°
```

4. è¿›è¡Œè®­ç»ƒ:  
å…¶ä¸­ï¼Œ è°ƒç”¨ self.model.train è·å–åˆ°
```python
def train_model(self, max_epoch = 100):  
    self.model.train()    # åªéœ€è¦è°ƒç”¨ train() å‡½æ•°å³å¯  
    # åˆå§‹æ—¶æ¨¡å‹æ˜¯æœªè®­ç»ƒçš„æ¨¡å‹  
    for epoch in range(max_epoch):  
        train_loss = 0.0  
        for data, labels in self.train_loader:  
            self.optim.zero_grad()   # åœ¨æ¯æ¬¡å°æ‰¹é‡è®­ç»ƒæ—¶, éƒ½éœ€è¦æ¸…é›¶æ¢¯åº¦  
            loss = self.criterion(self.model(data), labels)  
            loss.backward()         # åå‘ä¼ æ’­  
            self.optim.step()       # åˆ©ç”¨åå‘ä¼ æ’­å¾—åˆ°çš„æ¢¯åº¦, è¿›è¡Œå‚æ•°æ›´æ–°  
            train_loss += loss.item() * data.size(0)    # è®¡ç®—æ€»å’Œçš„è¯¯å·®
```


MaxPool æ± åŒ–çš„å‚æ•°æŒ‡å®šæ–¹æ³•: 
```python
# target output size of 5x7
m = nn.AdaptiveMaxPool2d((5, 7))
input = torch.randn(1, 64, 8, 9)
output = m(input)
# target output size of 7x7 (square)
m = nn.AdaptiveMaxPool2d(7)
input = torch.randn(1, 64, 10, 9)
output = m(input)
# target output size of 10x7
m = nn.AdaptiveMaxPool2d((None, 7))
input = torch.randn(1, 64, 10, 9)
output = m(input)
```


### (1)å¤šé¡¹å¼å›å½’æ¨¡å‹
ä¸‹é¢æ›´è¿›ä¸€æ­¥å°è¯•ä¸€ä¸‹å¤šé¡¹å¼å›å½’ï¼Œä¸‹é¢æ˜¯å…³äº x çš„å¤šé¡¹å¼ï¼š
$$
\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 
$$
è¿™æ ·å°±èƒ½å¤Ÿæ‹Ÿåˆæ›´åŠ å¤æ‚çš„æ¨¡å‹ï¼Œè¿™é‡Œä½¿ç”¨äº† $x$ çš„æ›´é«˜æ¬¡ï¼ŒåŒç†è¿˜æœ‰å¤šå…ƒå›å½’æ¨¡å‹ï¼Œå½¢å¼ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯é™¤äº†ä½¿ç”¨ $x$ï¼Œè¿˜æ˜¯æ›´å¤šçš„å˜é‡ï¼Œæ¯”å¦‚ $y$ã€$z$ ç­‰ç­‰ï¼ŒåŒæ—¶ä»–ä»¬çš„ $loss$ å‡½æ•°å’Œç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹æ˜¯ä¸€è‡´çš„ã€‚ 
```python
import numpy as np  
import torch
import matplotlib.pyplot as plt
# å®šä¹‰ä¸€ä¸ªå¤šå˜é‡å‡½æ•°
w_target = np.array([0.5, 3, 2.4]) # å®šä¹‰å‚æ•°
b_target = np.array([0.9]) # å®šä¹‰å‚æ•°

f_des = 'y = {:.2f} + {:.2f} * x + {:.2f} * x^2 + {:.2f} * x^3'.format(
    b_target[0], w_target[0], w_target[1], w_target[2]) # æ‰“å°å‡ºå‡½æ•°çš„å¼å­

print(f_des)
```

æˆ‘ä»¬ä¸‹é¢ä»¥ w0 + w1 + w2 + b = y_sample çš„å¼å­, ä»¥ randn ä½œä¸º w åˆå§‹å€¼, 0 ä½œä¸ºb åˆå§‹å€¼è¿›è¡Œå°æ‰¹é‡æ¢¯åº¦ç®—æ³•æ±‚è§£è·å–ç»“æœ(å®šä¹‰çš„ä¸¤ä¸ªå‡½æ•°ä¸­, ä¸€ä¸ªæ˜¯æ¨¡å‹, ä¸€ä¸ªæ˜¯æŸå¤±å‡½æ•°): 
```python
import numpy as np  
import matplotlib.pyplot as plt  
import torch  
  
fig = plt.figure()  
epochNum = 80  
lr = 0.001  

x_sample = np.arange(-3, 3.1, 0.1)    # å®šä¹‰ç»˜ç”»æ•°ç»„åŒºé—´  
y_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample ** 2 + w_target[2] * x_sample ** 3  
# plt.plot(x_sample,y_sample)  
# ----------------------------------------------------------  
# æ„å»ºæ•°æ® x å’Œ y# x æ˜¯ä¸€ä¸ªå¦‚ä¸‹çŸ©é˜µ [x, x^2, x^3]# y æ˜¯å‡½æ•°çš„ç»“æœ [y]x_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)  # æ„å»ºä»ç›¸åº”çš„æ•°æ®å–å¾—çš„è‡ªå˜é‡èŒƒå›´çŸ©é˜µ  
x_train = torch.from_numpy(x_train).float() # è½¬æ¢æˆ float tensory_train = torch.from_numpy(y_sample).float().unsqueeze(1) # è½¬åŒ–æˆ float tensor  
# å®šä¹‰å‚æ•° wå’Œb  
w = torch.randn((3, 1), dtype=torch.float, requires_grad=True)  # æ¯ä¸€æ¬¡runå¾—åˆ°çš„åˆå§‹fitæ›²çº¿ä¸åŒ  
b = torch.zeros((1), dtype=torch.float, requires_grad=True)  

# å®šä¹‰æ¨¡å‹
def multi_linear(x):  
    return torch.mm(x, w) + b   # æ³¨: mmå¯ä»¥ä½¿ç”¨matmulæ¥è¿›è¡Œä»£æ›¿  
  
def loss_func(y_, y):  # ä½¿ç”¨å¹³æ–¹å‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°  
    return torch.mean((y_ - y) ** 2)  
y_pred = multi_linear(x_train) # æ„å»ºå¤šé¡¹å¼å‡½æ•°  
  
ax1 = fig.add_subplot(121)  
ax1.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='curve_prefit', color='r')  
ax1.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b') 

# ç”»å‡ºæ›´æ–°ä¹‹å‰çš„æ¨¡å‹  
# ============ è®­ç»ƒæ•°æ®é›† ====================
for epoch in range (epochNum):  
    loss = loss_func(y_pred, y_train)  
    try:  
        w.grad.zero_()  
        b.grad.zero_()  
    except:  
        pass  
    loss.backward()  
    w.data = w.data - lr * w.grad.data  
    b.data = b.data - lr * b.grad.data  
    y_pred = multi_linear(x_train)  
  
ax2 = fig.add_subplot(122)  
ax2.plot(x_train.data.numpy()[:, 0], y_pred.data.numpy(), label='fitting curve-', color='r')  
ax2.plot(x_train.data.numpy()[:, 0], y_sample, label='real curve', color='b')  
  
plt.legend()  
plt.show()
```
### (2) Logistic å›å½’æ¨¡å‹
å¯¹äºç»å…¸çš„$(0,1)$æ¨¡å‹ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
$$loss = -[y * log(\hat{y}) + (1-y) *log(1-y)]$$
```python
def logistic_regression(x):
    return torch.sigmoid(torch.mm(x, w) + b)
## mm: matmul

y_pred = logistic_regression(x_data)
# è®¡ç®—loss, ä½¿ç”¨clampçš„ç›®çš„æ˜¯é˜²æ­¢æ•°æ®è¿‡å°è€Œå¯¹ç»“æœäº§ç”Ÿè¾ƒå¤§å½±å“ã€‚
def binary_loss(y_pred, y):
    logits = ( y * y_pred.clamp(1e-12).log() + \
              (1 - y) * (1 - y_pred).clamp(1e-12).log() ).mean()
    return -logits
```
åªéœ€è¦å°†lossæ”¹ä¸ºè¿™ä¸ªå³å¯ã€‚

å…¶ä¸­ï¼Œpytorchæä¾›äº†ç›¸å…³çš„å‡½æ•°ï¼Œé‡è¦çš„æ˜¯ torch.optim.SGD å¯ä»¥ç›´æ¥é‡‡ç”¨å°æ‰¹é‡æ¢¯åº¦å›å½’, å¦å¤–æ¯ä¸€æ¬¡éœ€è¦è°ƒç”¨ optimizer.zero_grad()æ¸…é›¶ä¼˜åŒ–å™¨æ¢¯åº¦å³å¯ã€‚
```python title:æœ€ç®€å•çš„å°æ‰¹é‡æ¢¯åº¦ä¸‹é™å›å½’ä»£ç (å¤–éƒ¨å®šä¹‰æ¨¡å‹y_predå’ŒæŸå¤±binary_loss)
# ä½¿ç”¨ torch.optim æ›´æ–°å‚æ•°
from torch import nn
import time
# use the neural network model for training
# å®šä¹‰ä¼˜åŒ–å‚æ•°
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

# ä¼˜åŒ–å™¨çš„è®¾å®šä»¥åŠç›¸å…³çš„ä½¿ç”¨
optimizer = torch.optim.SGD([w, b], lr=0.1) # ä½¿ç”¨SGDç®—æ³•

for e in range(1000):
    # å‰å‘ä¼ æ’­
    y_pred = logistic_regression(x_data)  # è¿™ä¸ªy_predå’Œlossæ˜¯è‡ªå·±å®šä¹‰å‡½æ•°è®¡ç®—çš„
    loss = binary_loss(y_pred, y_data)      # è®¡ç®— loss
    # åå‘ä¼ æ’­
    optimizer.zero_grad() #-----ä½¿ç”¨ä¼˜åŒ–å™¨å°†æ¢¯åº¦å½’ 0
    loss.backward()
    optimizer.step() # *** åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæ›´æ–°å‚æ•°è¢«å°è£…ï¼Œä½¿ç”¨ä¼˜åŒ–å™¨æ¥æ›´æ–°å‚æ•° **** 
    # è®¡ç®—æ­£ç¡®ç‡
    mask = y_pred.ge(0.5).float()
    acc = (mask == y_data).sum().item() / y_data.shape[0]
```


## ä¸‰ã€PyTorchæä¾›çš„æŸå¤±å‡½æ•° 
å‰é¢ä½¿ç”¨äº†è‡ªå·±å†™çš„ losså‡½æ•°ï¼Œå…¶å® PyTorch å·²ç»æä¾›äº†ä¸€äº›å¸¸è§çš„ losså‡½æ•°ï¼Œæ¯”å¦‚çº¿æ€§å›å½’é‡Œé¢çš„ loss æ˜¯ `nn.MSE()`ï¼Œè€Œ Logistic å›å½’çš„äºŒåˆ†ç±» loss åœ¨ PyTorch ä¸­æ˜¯ `nn.BCEWithLogitsLoss()`ï¼Œå…³äºæ›´å¤šçš„ lossï¼Œå¯ä»¥æŸ¥çœ‹[æ–‡æ¡£](https://pytorch.org/docs/stable/nn.html#loss-functions)

PyTorch å®ç°çš„ losså‡½æ•°æœ‰ä¸¤ä¸ªå¥½å¤„ï¼šç¬¬ä¸€æ˜¯æ–¹ä¾¿ä½¿ç”¨ï¼Œä¸éœ€è¦é‡å¤é€ è½®å­ï¼›ç¬¬äºŒå°±æ˜¯å…¶å®ç°æ˜¯åœ¨åº•å±‚ C++ è¯­è¨€ä¸Šçš„ï¼Œæ‰€ä»¥é€Ÿåº¦ä¸Šå’Œç¨³å®šæ€§ä¸Šéƒ½è¦æ¯”è‡ªå·±å®ç°çš„è¦å¥½ã€‚

å¦å¤–ï¼ŒPyTorch å‡ºäºç¨³å®šæ€§è€ƒè™‘ï¼Œå°†æ¨¡å‹çš„ Sigmoid æ“ä½œå’Œæœ€åçš„ loss éƒ½åˆåœ¨äº† `nn.BCEWithLogitsLoss()`ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ PyTorch è‡ªå¸¦çš„ loss å°±ä¸éœ€è¦å†åŠ ä¸Š Sigmoid æ“ä½œäº†
```python
# ä½¿ç”¨è‡ªå¸¦çš„loss
criterion = nn.BCEWithLogitsLoss() # å°† sigmoid å’Œ loss å†™åœ¨ä¸€å±‚ï¼Œæœ‰æ›´å¿«çš„é€Ÿåº¦ã€æ›´å¥½çš„ç¨³å®šæ€§
w = nn.Parameter(torch.randn(2, 1))
b = nn.Parameter(torch.zeros(1))

def logistic_reg(x):
    return torch.mm(x, w) + b
    
optimizer = torch.optim.SGD([w, b], 1.)

y_pred = logistic_reg(x_data)
loss = criterion(y_pred, y_data)  # è¿™æ ·å®šä¹‰æŸå¤±å‡½æ•°
# ä¹‹åè°ƒç”¨
for ......
	loss.backward()
	optimizer.step()_
print(loss.data)
```

å¸¸è§çš„æŸå¤±å‡½æ•°ï¼š
æµ…å±‚ï¼š LRï¼ŒSCM, ELm
æ·±å±‚ï¼šCNN ..

## å››ã€ç¥ç»ç½‘ç»œæ± åŒ–å±‚çš„æ¦‚å¿µç®€ä»‹
æ± åŒ–å±‚ä¹Ÿæ˜¯ä¸€ä¸ªç±»ä¼¼äºŒç»´çš„å±‚;

æ± åŒ–å±‚åœ¨CNNä¸­,ç”¨äºå‡å°‘ç©ºé—´ç»´æ•°è¿›è¡Œé™ç»´ä½¿ç”¨, å®é™…ä¸Šæ˜¯ä½¿ç”¨ä¸€ä¸ªfeature map, å…¶å®½é«˜åˆ†åˆ«ä¸º nh å’Œ nw, ç±»ä¼¼äºå·ç§¯è¿‡ç¨‹, åœ¨ map çš„æŒ‡å®šèŒƒå›´å†…è¿›è¡Œæ‰«æ; ç„¶åè¿‡æ»¤å‡ºç¬¦åˆè¦æ±‚çš„éƒ¨åˆ†ã€‚
ä¸€èˆ¬æ± åŒ–éœ€è¦æŒ‡å®š feature map å¤§å°å’Œ stride, ç¤ºä¾‹å¦‚ä¸‹: 
![[attachments/1_fhdK1xJND_m1Rdmr1m_9PQ.webp]]
ä¾‹å¦‚16x16çš„è¾“å…¥åœ¨2x2çª—, æ­¥è¿›ä¸º(2x2)æ± åŒ–åæˆä¸º 8x8 çš„ç‰¹å¾; å¸¸è§çš„æ± åŒ–æœ‰å‡å€¼æ± åŒ–å’Œæœ€å¤§å€¼æ± åŒ–ç­‰ç­‰; 


# å›¾åƒå¤„ç†ä¸“é¢˜
### (1) å›¾ç‰‡ç¼©æ”¾ä»£ç  (Pytorch, sklearn å’Œ  cv2) 
#### 1.  é€šè¿‡ skimage åº“è¿›è¡Œå›¾å½¢ç¼©æ”¾ 
ä¸€èˆ¬åœ°,  å¯¹äº 32 x 32 çš„å›¾åƒæ•°æ®é›†, è¾“å…¥æ˜¯ä¸€ä¸ª (50000, 32, 32, 3) çš„å‘é‡ , å¯ä»¥é‡‡ç”¨ `cv2.resize` æˆ–è€… skimage.transform.resize è¿›è¡Œæ‰¹é‡è°ƒæ•´
```sh
pip install scikit-image
```

ä»¥ä¸‹æ˜¯ skimage çš„
```python
import numpy as np
from skimage.transform import resize

# å‡è®¾è¾“å…¥çš„å›¾åƒæ•°æ®æ˜¯ (50000, 32, 32, 3)
images = np.random.rand(50000, 32, 32, 3)

# ä½¿ç”¨ resize æ‰¹é‡è°ƒæ•´å¤§å°
resized_images = np.zeros((images.shape[0], 28, 28, 3))  # åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„æ•°ç»„ï¼Œç›®æ ‡å°ºå¯¸æ˜¯ (50000, 28, 28, 3)
for i in range(images.shape[0]):
    resized_images[i] = resize(images[i], (28, 28), mode='reflect', preserve_range=True)

print(resized_images.shape)  # è¾“å‡º (50000, 28, 28, 3)
```
å¤„ç† å¤§æ‰¹é‡å›¾åƒæ—¶, ä¹Ÿå¯ä»¥é‡‡ç”¨
```python
import numpy as np
import cv2

# å‡è®¾è¾“å…¥çš„å›¾åƒæ•°æ®æ˜¯ (50000, 32, 32, 3)
images = np.random.rand(50000, 32, 32, 3)

# ä½¿ç”¨ cv2.resize æ‰¹é‡è°ƒæ•´å¤§å°
resized_images = np.zeros((images.shape[0], 28, 28, 3), dtype=np.float32)
for i in range(images.shape[0]):
    resized_images[i] = cv2.resize(images[i], (28, 28))
print(resized_images.shape)  # è¾“å‡º (50000, 28, 28, 3)
```

#### 2. å¯¹äº  Pytorch å°æ‰¹é‡è®­ç»ƒ
é¦–å…ˆæˆ‘ä»¬é€šè¿‡ `train_dataset = CIFAR10( root='./cifar10', train=True, download=True, transform=t)  ` å¾—åˆ°äº†å®é™…çš„æ•°æ®é›†,  è€Œå®é™…ä¸Š
`self.train_dataset.data.shape` æ˜¯ (50000, 32, 32, 3) å¤§å°çš„æ•°æ®é›†(åŒ…å« RGB æ•°æ®), è€Œå°†å…¶è½¬æ¢ä¸º 28 * 28 å›¾åƒåªéœ€ `transforms.Resize((28, 28)),  # å°†å›¾åƒç¼©æ”¾åˆ° 28x28`  


å…¶ä¸­, å‚è€ƒ [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#flatten) éƒ¨åˆ†, å¦‚æœå¾—åˆ°çš„è¾“å‡ºéœ€è¦å˜æ¢ä¸ºä¸€ç»´è¾“å‡ºï¼Œåˆ™å¯ä»¥åœ¨ net ä¸­æ’å…¥ä¸€ä¸ª nn.Flatten å±‚,  ä¼šå°†åé¢çš„ä¸‰ç»´å±•å¼€æˆä¸€ç»´, 
```python
input = torch.randn(32, 1, 5, 5)
# With default parameters
m = nn.Flatten()
output = m(input)
output.size()  # torch.Size([32, 25])
# With non-default parameters
m = nn.Flatten(start_dim =0, end_dim=2) # å°† 0-2 ç»´å±•å¹³å¾—åˆ° (32 * 1 * 5 , 5)çš„å½¢çŠ¶
output = m(input)
output.size() # (160, 5)
```


### (2) æ¿€æ´»å‡½æ•°ä¸“é¢˜
åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ `ReLU`, `Softmax` ç­‰ï¼‰é€šå¸¸ç”¨äºæ·»åŠ éçº¿æ€§å˜æ¢ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å¤æ‚çš„æ¨¡å¼ã€‚ä½ å¯ä»¥å°†æ¿€æ´»å‡½æ•°æ’å…¥åˆ° `nn.Linear` å±‚ä¹‹åï¼Œé€šå¸¸æ˜¯åœ¨è¾“å‡ºå±‚ä¹‹å‰æˆ–æŸäº›ä¸­é—´å±‚ä¹‹åè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚

å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°å¦‚ä¸‹:
#### 1. **ReLU æ¿€æ´»å‡½æ•°**
$$ \text{ReLU} (x) = \max(0,x)$$
![[Excalidraw/Pytorch & sklearnçš„ä½¿ç”¨åŸºç¡€å’ŒåŸºæœ¬ä»£ç  2024-12-21 12.24.08|250]]
`ReLU` æ˜¯å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸ç”¨äº **éšè—å±‚**ï¼Œå®ƒä¼šå¯¹è¾“å…¥è¿›è¡Œéçº¿æ€§è½¬æ¢ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œ`ReLU` æ”¾åœ¨ `nn.Linear` å±‚ä¹‹åã€‚
ä¾‹å¦‚ï¼Œåœ¨ `nn.Linear` ä¹‹åæ·»åŠ  `ReLU` æ¿€æ´»å‡½æ•°çš„å†™æ³•ï¼š

```python
self.net = nn.Sequential(
	nn.Conv2d(3, 32, kernel_size=3, padding=1),   # è¾“å…¥ï¼šbatch_size, 3, 32, 32
	nn.Conv2d(32, 64, kernel_size=3, padding=1),  # è¾“å‡ºï¼šbatch_size, 64, 32, 32
	nn.AdaptiveMaxPool2d((16, 16)),               # è¾“å‡ºï¼šbatch_size, 64, 16, 16
	nn.AvgPool2d(kernel_size=(2, 2), stride=2),   # è¾“å‡ºï¼šbatch_size, 64, 8, 8
	nn.Flatten(),                                 # å±•å¹³ï¼šbatch_size, 64 * 8 * 8
	nn.Linear(64 * 8 * 8, 512),                   # è¾“å…¥ï¼šbatch_size, 64 * 8 * 8ï¼Œè¾“å‡ºï¼šbatch_size, 512
	nn.ReLU(),                                   # ReLU æ¿€æ´»å‡½æ•°
	nn.Linear(512, 10)                            # è¾“å‡ºå±‚ï¼š10ä¸ªç±»åˆ«
)
```

- åœ¨ `nn.Linear(64 * 8 * 8, 512)` åæ’å…¥ `nn.ReLU()`ï¼Œå¸®åŠ©ç½‘ç»œå­¦ä¹ æ›´å¤æ‚çš„è¡¨ç¤ºã€‚

#### 2. **Sigmoid æ¿€æ´»å‡½æ•°**
å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter3 Linear Neural  Networks for regression(back Propagation)#(3) Sigmoid function|Sigmoid function]], æœ‰å…¬å¼å…³ç³»:
$$\text{Sigmoid}(x) =  \frac{1}{1 + e^{-x}}$$
ç”¨äºå°† wx + b éƒ¨åˆ†æŠ•å½±åˆ° (0,1) ä¹‹é—´, åŸºæœ¬å›¾åƒå¦‚ä¸‹: 
![[Excalidraw/Pytorch & sklearnçš„ä½¿ç”¨åŸºç¡€å’ŒåŸºæœ¬ä»£ç  2024-12-21 12.07.40|250]]
ç›´æ¥é‡‡ç”¨å¦‚ä¸‹è®¿é—®:
```python
torch.sigmoid(x);
```

**ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ Sigmoid**:

- **è¾“å‡ºå±‚**: å¦‚æœéœ€è¦æ¦‚ç‡è¾“å‡ºï¼ˆå¦‚äºŒåˆ†ç±»é—®é¢˜ï¼‰ï¼Œ`Sigmoid` é€šå¸¸ç”¨äºæœ€åä¸€å±‚ã€‚
- **ç‰¹å®šä»»åŠ¡**: åœ¨æŸäº›éœ€è¦ä¸¥æ ¼çº¦æŸè¾“å‡ºèŒƒå›´ä¸º `[0, 1]` çš„ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚ç”Ÿæˆå™¨ç½‘ç»œï¼ˆGANsï¼‰æˆ–å›¾åƒå¤„ç†ä»»åŠ¡ä¸­åƒç´ å€¼çš„å½’ä¸€åŒ–ã€‚

#### 3. tanh æ¿€æ´»å‡½æ•°
$$\tanh  (x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}  = 1 - \frac{2\exp (-2x)}{1+ \exp (-2x)}$$
å…¶å¯¼æ•°ä¸º:
$$\boxed{\tanh '(x) = 1 - \tanh^{2} x}$$

![[Excalidraw/Pytorch & sklearnçš„ä½¿ç”¨åŸºç¡€å’ŒåŸºæœ¬ä»£ç  2024-12-21 12.12.08|250]]
#### 4. **Softmax å‡½æ•°** 
softmax å®é™…ä¸Šä¸æ˜¯æ¿€æ´»å‡½æ•°, æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å‡½æ•°, é€šå¸¸æ”¾åœ¨ **è¾“å‡ºå±‚**ï¼Œç”¨äºå¤šåˆ†ç±»ä»»åŠ¡çš„æœ€ç»ˆè¾“å‡ºå±‚ï¼Œä»¥å°†ç½‘ç»œçš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚`Softmax` å°†æ¯ä¸ªç±»çš„è¾“å‡ºè½¬æ¢ä¸º `[0, 1]` ä¹‹é—´çš„æ¦‚ç‡ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰ç±»çš„æ¦‚ç‡å’Œä¸º 1(å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/è¡¥å……çŸ¥è¯†/9. softmaxå‡½æ•°å’Œäº¤å‰ç†µæŸå¤±CrossEntropy|softmaxå‡½æ•°å’Œäº¤å‰ç†µæŸå¤±]]éƒ¨åˆ†). 

åœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­ï¼Œ`Softmax` ä¸»è¦ç”¨äº **æœ€åä¸€å±‚**ï¼Œç”¨äºè¾“å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚( å¦‚æœæ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œ åˆ™é‡‡ç”¨ `Sigmoid`ï¼Œæˆ–è€…å¤šæ ‡ç­¾é—®é¢˜)  
ç¤ºä¾‹ : 
ä½†æ˜¯éœ€è¦æ³¨æ„, å½“é‡‡ç”¨ CrossEntropyLoss æ—¶,  æˆ‘ä»¬ä¸éœ€è¦ä½¿ç”¨ Softmax, å› ä¸ºå®é™…ä¸Šå·²ç»æ–½åŠ äº† `log_softmax` éƒ¨åˆ†; 
```python
self.net = nn.Sequential(
	nn.Conv2d(3, 32, kernel_size=3, padding=1),   # è¾“å…¥ï¼šbatch_size, 3, 32, 32
	nn.Conv2d(32, 64, kernel_size=3, padding=1),  # è¾“å‡ºï¼šbatch_size, 64, 32, 32
	nn.AdaptiveMaxPool2d((16, 16)),               # è¾“å‡ºï¼šbatch_size, 64, 16, 16
	nn.AvgPool2d(kernel_size=(2, 2), stride=2),   # è¾“å‡ºï¼šbatch_size, 64, 8, 8
	nn.Flatten(),                                 # å±•å¹³ï¼šbatch_size, 64 * 8 * 8
	nn.Linear(64 * 8 * 8, 512),                   # è¾“å…¥ï¼šbatch_size, 64 * 8 * 8ï¼Œè¾“å‡ºï¼šbatch_size, 512
	nn.ReLU(),                                   # ReLU æ¿€æ´»å‡½æ•°
	nn.Linear(512, 10),                           # è¾“å‡ºå±‚ï¼š10ä¸ªç±»åˆ«
	nn.Softmax(dim=1)                             # Softmax æ¿€æ´»å‡½æ•°ï¼šå°†è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
)
```
- `Softmax` æ”¾åœ¨ `nn.Linear(512, 10)` åï¼Œé€šå¸¸ä¼šåœ¨è®­ç»ƒæ—¶ä½¿ç”¨äº¤å‰ç†µæŸå¤± (`nn.CrossEntropyLoss`)ï¼Œè¿™ä¸ªæŸå¤±å‡½æ•°ä¼šè‡ªåŠ¨åº”ç”¨ `Softmax`ã€‚
- **ReLU** é€šå¸¸åº”ç”¨äº **éšè—å±‚**ï¼Œä½äºæ¯ä¸ª `nn.Linear` åï¼Œç”¨äºå¼•å…¥éçº¿æ€§ã€‚
- **Softmax** é€šå¸¸åº”ç”¨äº **è¾“å‡ºå±‚**ï¼Œç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ï¼Œå°†ç½‘ç»œçš„è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚`Softmax` åº”æ”¾åœ¨æœ€åçš„è¾“å‡ºå±‚ä¹‹åã€‚

æ¿€æ´»å‡½æ•°çš„ä¸€èˆ¬è§„åˆ™:
- **éšè—å±‚ï¼š** `ReLU`, `LeakyReLU`, `Tanh`, `Sigmoid` ç­‰ï¼ˆæ ¹æ®ä»»åŠ¡å’Œéœ€æ±‚é€‰æ‹©ï¼‰ã€‚
- **è¾“å‡ºå±‚ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰ï¼š** `Softmax`ï¼ˆå¤šåˆ†ç±»é—®é¢˜ï¼‰æˆ– `Sigmoid`ï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼Œæˆ–è€…å¤šæ ‡ç­¾é—®é¢˜ï¼‰ã€‚

### (3) æŸå¤±å‡½æ•°ä¸“é¢˜ 
æŸå¤±å‡½æ•°å®é™…ä¸Šæ˜¯ä¸€ç§ç»éªŒé£é™©æœ€å°åŒ–çš„æ€è·¯ï¼Œ ä½†æ˜¯éœ€è¦æ³¨æ„[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#1. Basic Concept|ç»éªŒé£é™©å’Œå®é™…é£é™©çš„åŒºåˆ«]]. å¸¸è§çš„æŸå¤±å‡½æ•°æœ‰: 0-1æŸå¤±å‡½æ•°ï¼Œç»å¯¹å€¼æŸå¤±å‡½æ•°ï¼Œå¹³æ–¹æŸå¤±å‡½æ•°ï¼ŒLogå¯¹æ•°æŸå¤±å‡½æ•°ï¼ŒæŒ‡æ•°æŸå¤±å‡½æ•°ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ŒHingeæŸå¤±å‡½æ•°ç­‰ç­‰;

#### 1. MSE, MAE æŸå¤±å‡½æ•°
MSE : å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°(MSEï¼Œmean square error), ä¸€èˆ¬ç”¨äºå›å½’é—®é¢˜(most commonly used loss function for regression tasks.) 
$$L = \frac{1}{n} \sum^{n}_{i=1} (\hat{y}_i -y_i)^2$$
```python
nn.MSELoss()
```

**Mean Absolute Error (MAE)** 
$$\text{L1Loss} = \frac{1}{n} \sum_{i=1}^n |\hat{y}_i - y_i|$$
å…¶è°ƒç”¨æ–¹æ³•æ˜¯
```python
nn.L1Loss() 
```

**Huber Loss** 
å¯¹ Huber Loss å®é™…ä¸Šæ˜¯ MSE å’Œ MAE çš„ç»„åˆ,   å…·ä½“å…¬å¼å¦‚ä¸‹ : 
$$L_{\delta}(a) =
\begin{cases} 
\frac{1}{2}a^2 & \text{if } |a| \leq \delta, \\
\delta (|a| - \frac{1}{2} \delta) & \text{otherwise.}
\end{cases}$$
```python
nn.HuberLoss
``` 

#### 2. äº¤å‰ç†µæŸå¤±å‡½æ•° 
äº¤å‰ç†µæŸå¤±å‡½æ•° (å¸¸ç”¨äºåˆ†ç±»é—®é¢˜)
$$H(y^{(i)}, \hat{y}^{(i)}) = -\overset{q}{\underset{j = 1}{\sum}}y_j^{(i)}\log(y_j^{(i)})$$
å…¶ä¸­, $\hat{y}_j^{(i)}$é0å³1  ä¸ºåˆ†ç±»æ ‡ç­¾;

ä¸€èˆ¬åœ°, ä¸ºäº†é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œ å¾€å¾€ä¼šåŠ å…¥<b><mark style="background: transparent; color: blue">æ­£åˆ™åŒ–å‚æ•°</mark></b>ï¼Œ å®é™…ä¸Šæ˜¯æƒé‡æŸå¤±çš„æ¦‚å¿µ, å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æœºå™¨å­¦ä¹ ç®—æ³•(sklearn)/9. æ­£åˆ™åŒ–æ–¹æ³•, æ¦‚ç‡å›¾æ¨¡å‹å’Œè´å¶æ–¯ç½‘ç»œ|æ­£åˆ™åŒ–æ–¹æ³•, æ¦‚ç‡å›¾æ¨¡å‹å’Œè´å¶æ–¯ç½‘ç»œ]] éƒ¨åˆ†;

### 3.  Logarithmic MSE è¯¯å·® 
å¯¹äºæˆ¿ä»·ç­‰ç­‰è¾ƒå¤§çš„æ•°æ®, ä¸€èˆ¬æˆ‘ä»¬æ›´åŠ å…³å¿ƒçš„æ˜¯ç›¸å¯¹äºåŸå§‹ä»·æ ¼çš„å‡†ç¡®ç¨‹åº¦, 
$$\sqrt{\frac{1}{n} \sum_{i = 1}^{n} (\log_{} y_{i}  - \log_{} \hat{y}_{i})^{2}}$$
å…·ä½“å‚è€ƒ [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter5 Essential Concepts & Heuristics#(1) Logarithmic MSE Loss|Logarithmic MSE Loss]] éƒ¨åˆ†

Log-Cosh è¯¯å·®, å¯ä»¥é‡‡ç”¨:
$$\text{Log-Cosh}(x) = \sum_{i=1}^{n} \log(\cosh(\hat{y}_{i} - y_{i}))$$

### éªŒè¯é›†å’ŒkæŠ˜äº¤å‰éªŒè¯
- æ–¹æ³•æ˜¯å°†è®­ç»ƒæ•°æ®é›†åˆ†å‰²æˆkä¸ªä¸åŒçš„å­æ•°æ®é›†ï¼Œæ¯ä¸€æ¬¡ä½¿ç”¨1ä¸ªæ•°æ®é›†éªŒè¯ï¼Œk-1ä¸ªè®­ç»ƒ 
- åškæ¬¡æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶å°†kæ¬¡çš„è®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®åˆ†åˆ«æ±‚å¹³å‡å€¼
å‚è€ƒ  [[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter5 Essential Concepts & Heuristics#(2) K-fold Cross-Validation|K-fold Cross-Validation]] éƒ¨åˆ†.


### å®šä¹‰è®­ç»ƒçš„å±‚ 
éœ€è¦è¯´æ˜,  Pytorch  ä¸­, æ”¯æŒè‡ªå®šä¹‰ä¸åŒçš„å±‚, ä»ç„¶å¯ä»¥é‡‡ç”¨ nn.Module è¿›è¡Œå®šä¹‰; ä»ç„¶éœ€è¦ç»§æ‰¿ nn.Module ç±»å‹,  å¹¶ä¸”é‡å†™ forward æ–¹æ³•; 

forward å¿…é¡»é‡‡ç”¨ `def forward(self, X):`  ä¸ºå®šä¹‰, å¦‚æœæœ‰ forward, åˆ™å¯ä»¥è§†ä¸ºä¸€å±‚;

```python
import torch
import torch.nn as nn

class EpochDropout(nn.Module):
    def __init__(self, p=0.5):
        super(EpochDropout, self).__init__()
        self.p = p
        self.mask = None

    def forward(self, x):
        if self.training:
            # å¦‚æœ mask ä¸ºç©ºï¼Œåˆ™ç”Ÿæˆ mask
            if self.mask is None or self.mask.shape != x.shape:
                self.mask = torch.bernoulli(torch.full(x.shape, 1 - self.p)).to(x.device) / (1 - self.p)
            return x * self.mask
        else:
            return x  # æµ‹è¯•é˜¶æ®µä¸ä½¿ç”¨ Dropout

    def reset_mask(self):
        """åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶é‡ç½® mask"""
        self.mask = None
```

```python
self.dropout = EpochDropout(p=0.5)
# æˆ–è€…ç›´æ¥æ·»åŠ åˆ° net ä¸­ 
```


### Batch Norm  
Add `nn.BatchNorm2d` after convolutional layers to stabilize learning and improve convergence speed.

`nn.BatchNorm2d` æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»ç½‘ç»œå±‚normalizationæŠ€æœ¯,ä¸»è¦ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œä¸­ã€‚å®ƒé€šå¸¸è¢«æ·»åŠ åœ¨å·ç§¯å±‚ä¹‹å,èµ·åˆ°ä»¥ä¸‹ä½œç”¨:

1. **åŠ é€Ÿè®­ç»ƒæ”¶æ•›**:
   - é€šè¿‡æ ‡å‡†åŒ–è¾“å…¥æ•°æ®,å¯ä»¥æœ‰æ•ˆç¼“è§£å†…éƒ¨åå˜ç§»ç§»(internal covariate shift)çš„é—®é¢˜,åŠ å¿«æ¨¡å‹çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€‚

2. **æé«˜æ³›åŒ–èƒ½åŠ›**:
   - BatchNormèƒ½å¤Ÿä½¿å¾—æ¨¡å‹å¯¹è¾“å…¥æ•°æ®çš„åˆ†å¸ƒå˜åŒ–æ›´åŠ é²æ£’,ä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚

3. **é™ä½å¯¹åˆå§‹åŒ–çš„ä¾èµ–**:
   - ç”±äºBatchNormèƒ½å¤Ÿå¯¹æ•°æ®è¿›è¡ŒåŠ¨æ€æ ‡å‡†åŒ–,å› æ­¤æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å¯¹åˆå§‹åŒ–å‚æ•°çš„ä¾èµ–æ€§å¤§å¤§é™ä½ã€‚

4. **é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**:
   - BatchNormèƒ½å¤Ÿæœ‰æ•ˆåœ°ç¼“è§£æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é—®é¢˜,ä¸ºç½‘ç»œçš„è®­ç»ƒæä¾›æ›´åŠ ç¨³å®šçš„ç¯å¢ƒã€‚

**ä¸€èˆ¬æƒ…å†µä¸‹, `nn.BatchNorm2d` å±‚ä¼šè¢«æ·»åŠ åœ¨å·ç§¯å±‚å’Œæ¿€æ´»å‡½æ•°å±‚ä¹‹é—´, è¿™æ ·å¯ä»¥åœ¨ç‰¹å¾æå–å’Œç‰¹å¾å˜æ¢ä¹‹åå¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–**,ä»è€Œè·å¾—æ›´åŠ ç¨³å®šå’Œé²æ£’çš„ç‰¹å¾è¡¨ç¤ºã€‚


## å¸¸ç”¨çš„å±‚éƒ¨åˆ†
nn.GroupNorm,  nn.BatchNorm1d, nn.BatchNorm2d  ç­‰ç­‰

**æ­¤å¤–, `nn.BatchNorm2d` è¿˜æœ‰ä¸€äº›å…¶ä»–çš„å˜ä½“,å¦‚ `nn.BatchNorm1d` ç”¨äºå…¨è¿æ¥å±‚, `nn.InstanceNorm2d` ç”¨äºå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡**ã€‚å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å…·ä½“é—®é¢˜å’Œç½‘ç»œç»“æ„æ¥é€‰æ‹©åˆé€‚çš„normalizationæŠ€æœ¯ã€‚

 `@torch.no_grad()` æä¾›äº†å°†æ•´ä¸ªå‡½æ•°ä¸è¿›è¡Œæ¢¯åº¦çš„æ§åˆ¶ä¿®é¥°å™¨ã€‚

ResNet 


æœ‰ä¸€äº›æ¯” `AdaptiveMaxPool2d` æ›´åŠ ä¼˜è¶Šçš„æ± åŒ–æ–¹æ³•,å…·ä½“å–å†³äºå®é™…é—®é¢˜å’Œæ¨¡å‹éœ€æ±‚ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯¹æ¯”å’Œå»ºè®®:

1. **GlobalAveragePooling2d**:
   - è¿™ç§æ–¹æ³•å°†ç‰¹å¾å›¾ä¸Šæ¯ä¸ªé€šé“çš„ç‰¹å¾å€¼å–å¹³å‡,å¾—åˆ°ä¸€ä¸ªå…¨å±€ç‰¹å¾å‘é‡ã€‚
   - ç›¸æ¯” `AdaptiveMaxPool2d`ï¼ŒGlobalAveragePooling2d èƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™ç‰¹å¾ä¿¡æ¯,ä¸ä¼šä¸¢å¤±é‡è¦çš„ç‰¹å¾ã€‚
   - åœ¨ä¸€äº›ä»»åŠ¡ä¸Š,å¦‚å›¾åƒåˆ†ç±»,GlobalAveragePooling2d è¡¨ç°æ›´ä¼˜ã€‚

2. **Spatial Pyramid Pooling (SPP)**:
   - SPP åˆ©ç”¨å¤šå°ºåº¦çš„æ± åŒ–æ“ä½œ,èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ä¸åŒå°ºåº¦çš„ç‰¹å¾ä¿¡æ¯ã€‚
   - ç›¸æ¯”å›ºå®šå¤§å°çš„æ± åŒ–,SPP èƒ½å¤Ÿé€‚åº”ä¸åŒå¤§å°çš„è¾“å…¥,æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚
   - SPP æ¨¡å—å¯ä»¥ä½œä¸ºå·ç§¯ç½‘ç»œçš„ä¸€éƒ¨åˆ†,ä¸å…¶ä»–å±‚ä¸€èµ·ç«¯åˆ°ç«¯è®­ç»ƒã€‚

3. **Dilated Convolution**:
   - Dilated Convolution å¯ä»¥åœ¨ä¸é™ä½åˆ†è¾¨ç‡çš„æƒ…å†µä¸‹æ‰©å¤§æ„Ÿå—é‡,æ•è·æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
   - ç›¸æ¯”æ± åŒ–,Dilated Convolution ä¸ä¼šä¸¢å¤±ç‰¹å¾ä¿¡æ¯,èƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™åŸå§‹çš„è¯­ä¹‰ç‰¹å¾ã€‚

å…³äºæ‚¨æåˆ°çš„GPUè¶³å¤Ÿçš„æƒ…å†µä¸‹,æ˜¯å¦åº”è¯¥ä½¿ç”¨æ± åŒ–å±‚:

- å¦‚æœ GPU èµ„æºè¶³å¤Ÿå……è£•,ä¸€èˆ¬æ¥è¯´ä¸ä½¿ç”¨æ± åŒ–å±‚è®­ç»ƒæ•ˆæœä¼šæ›´å¥½ã€‚è¿™æ˜¯å› ä¸ºæ± åŒ–ä¼šä¸¢å¤±ä¸€äº›æœ‰ä»·å€¼çš„ç‰¹å¾ä¿¡æ¯ã€‚
- ä½†å¦‚æœGPUèµ„æºå—é™,ä½¿ç”¨æ± åŒ–å±‚å¯ä»¥æœ‰æ•ˆé™ä½è®¡ç®—é‡å’Œå†…å­˜å ç”¨,ä»è€Œèƒ½å¤Ÿè®­ç»ƒæ›´æ·±æ›´å¤§çš„æ¨¡å‹ã€‚è¿™ç§æƒ…å†µä¸‹,æ± åŒ–çš„ä¼˜åŠ¿å¯èƒ½ä¼šå¤§äºå…¶å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚

ç»¼åˆè€ƒè™‘,å¦‚æœGPUèµ„æºå……è¶³,å¯ä»¥å°è¯•ä½¿ç”¨ GlobalAveragePooling2d æˆ– Dilated Convolution ç­‰æ›´é«˜çº§çš„æ± åŒ–/æ„Ÿå—é‡æ‰©å¼ æŠ€æœ¯ã€‚åŒæ—¶ä¹Ÿè¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ¨¡å‹éœ€æ±‚è¿›è¡Œå®éªŒéªŒè¯,æ‰¾åˆ°æœ€åˆé€‚çš„æ–¹æ¡ˆã€‚


### GPU è®¾å¤‡æ•°æ®ä¼ è¾“ 
ä¸€èˆ¬é‡‡ç”¨ `.cuda()` , `.cpu` , `.to(self.device)`  å’Œ `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`  ç»“åˆä½¿ç”¨è¿›è¡Œ gpu æ•°æ®ç§»åŠ¨ã€‚

å¯¹äºå¤š GPU è®­ç»ƒçš„æ¨¡å‹ï¼Œ å¯èƒ½æ¶‰åŠæ•°æ®ä¸åŒ GPU ä¹‹é—´çš„ä¼ é€’ 
```python
print(torch.cuda.device_count())

num = 1
data.to(torch.device(f'cuda:{num}'))   # å°†æ•°æ®æ”¾åœ¨æŸä¸ªgpu ä¸Š
```

### ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ 
ä¸€èˆ¬å¯¹äºä»»æ„ nn.Module æ¨¡å—ï¼Œ å¯ä»¥é€šè¿‡å­˜å‚¨ state_dict è·å–å…¶ä¸­æ‰€æœ‰å‚æ•°ä¿¡æ¯ã€‚ 
```python
torch.save(self.model.state_dict(), 'cnn_models.pth')
```

ç„¶åé‡‡ç”¨ load_state_dict å¾—åˆ°åŸå§‹æ¨¡å‹å‚æ•°
```python
model = EasyModule()
model.load_state_dict(torch.load('cnn_models.pth')) 
```



##  æ¨¡å‹æ˜¾ç¤ºæ–¹æ³•: 
ä¸€èˆ¬å¯¹äº  nn.Module ç±»å‹ç»§æ‰¿çš„æ¨¡å‹ï¼Œ å¯ä»¥ç›´æ¥ä½¿ç”¨ 
`print(model)` æ‰“å°æ¨¡å‹è¾“å…¥å’Œè¾“å‡ºéƒ¨åˆ†ï¼Œ 

æˆ–è€… module.named_parameters() æ‰“å°
```python
for name, param in model.named_parameters():
    print(f"Layer: {name}, Size: {param.size()}")
```

æ­¤å¤–å¯ä»¥é‡‡ç”¨Â `torchsummary`  åº“è·å–æ›´åŠ è¯¦ç»†çš„æ¨¡å‹ç»“æ„. 
```python
from torchsummary import summary

summary(model, input_size=(10,))
```

### Lazy ç½‘ç»œ 

> [!HINT] ä¸ºä½•é‡‡ç”¨ Lazy ç½‘ç»œ
> å¸¸ç”¨çš„ Lazy ç½‘ç»œåŒ…æ‹¬ LazyLinear,  LazyConv2d ç­‰, å…¶è¾“å…¥æ•°é‡æ˜¯æ ¹æ®å‰ä¸€å±‚çš„è¾“å‡ºç¡®å®šçš„ï¼Œå› æ­¤å¯ä»¥çµæ´»å¤„ç†ä¸åŒå½¢çŠ¶çš„è¾“å…¥æ•°æ®ï¼Œ å› æ­¤èƒ½å¤Ÿæ›´å¥½åœ°è¿ç§»åˆ°ä¸åŒçš„æ•°æ®é›†ä¸Šæ¥  
