

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SoftBLEULoss(nn.Module):
    def __init__(self, ngram=4, smooth=1e-10):
        super().__init__()
        self.ngram = ngram
        self.smooth = smooth

    def forward(self, pred_logits, target_ids):
        """
        pred_logits: [batch_size, seq_len, vocab_size]
        target_ids: [batch_size, seq_len]
        """
        batch_size = pred_logits.size(0)
        bleu_loss = 0.0

        # 计算 n-gram 匹配（近似）
        for n in range(1, self.ngram + 1):
            # 获取预测的 n-gram 概率（通过滑动窗口）
            pred_probs = F.softmax(pred_logits, dim=-1)
            ngram_probs = pred_probs.unfold(1, n, 1).prod(dim=-1)  # [batch, seq_len-n+1, n]

            # 获取目标的 n-gram
            target_ngrams = target_ids.unfold(1, n, 1)  # [batch, seq_len-n+1, n]

            # 计算匹配概率（近似）
            match_prob = (ngram_probs * (target_ngrams.unsqueeze(-1) == torch.arange(vocab_size).to(device))).sum(dim=-1)
            match_prob = match_prob.mean(dim=1)  # [batch]

            bleu_loss += -torch.log(match_prob + self.smooth).mean()

        return bleu_loss / self.ngram
```

## 强化学习策略梯度 

```python
class RLBLEULoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.bleu = evaluate.load("bleu")

    def forward(self, model, input_ids, target_ids):
        # 采样生成句子（探索）
        sampled_ids = model.generate(input_ids, do_sample=True)
        sampled_text = tokenizer.batch_decode(sampled_ids, skip_special_tokens=True)
        
        # 贪婪生成句子（基线）
        greedy_ids = model.generate(input_ids, do_sample=False)
        greedy_text = tokenizer.batch_decode(greedy_ids, skip_special_tokens=True)

        # 计算 BLEU 分数
        target_text = tokenizer.batch_decode(target_ids, skip_special_tokens=True)
        sampled_bleu = self.bleu.compute(predictions=sampled_text, references=[[t] for t in target_text])["bleu"]
        greedy_bleu = self.bleu.compute(predictions=greedy_text, references=[[t] for t in target_text])["bleu"]

        # 计算奖励并归一化
        reward = sampled_bleu - greedy_bleu
        reward = torch.tensor(reward, device=input_ids.device)

        # 计算损失（负奖励加权对数概率）
        log_probs = model(input_ids, labels=sampled_ids).logits.log_softmax(dim=-1)
        selected_log_probs = log_probs.gather(-1, sampled_ids.unsqueeze(-1)).squeeze(-1)
        loss = -reward * selected_log_probs.mean()

        return loss
```

