## 一、基本介绍
### (1) Logistic 函数
支持向量机(Suppported Vector Machine, SVM), 是一种分类算法,其一般目标是在空间中找到一个超平面, 并表示方程为
![[attachments/Pasted image 20240327152229.png|300]]
$$w^{T} x + b = 0$$
我们首先介绍Logistic函数, 即将数据从$(-\infty, +\infty)$ 映射到(0,1)的函数，其形式如下:
$$\boxed {h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1 + e^{-\theta^{T}x}}}$$
显然,当$\theta^{T}x > 0$时,$h_{\theta}(x) >0.5$, 获得的图像如下图:
![[attachments/Pasted image 20240327152635.png|400]]
其中上式中的$\theta^{T} x$即对应一个对输入参数(n维)的线性变换:
$$\theta^{T} x = \theta_{0}x_{0} + \theta_{1} x_{1}  + \theta_{2}x_{2} + \dots  + \theta_{n}x_{n}\qquad  (x_{0} = 1)$$
另外其中$x_1,\dots x_n$代表了多个不同训练样本值（x_0=1）也可以使用不同的基函数进行代替. 
对应的分类目的, 就是求解对应的超参数$\theta_{1}, \theta_{2}, \dots$,使得训练数据中对应的 y=1 部分, $\theta^{T} x >>0$, 而 y=0 部分, $\theta^{T} x << 0$，  

### (2)最大边缘超平面
Maximal Margin Hyperplane
![[attachments/Pasted image 20240327154353.png|300]]
由于将训练样本点分开的超平面很多，所以往往必须有最大边缘超平面作为决策边界。我们考虑和每个超平面平行的一对仍然可以将样本点分开的超平面,而其中两平面间距称为边缘(margin)

最大边缘超平面即距离最长的一组超平面(B1), 而我们往往以其为分类依据, 以获得更好的泛化误差。这种方法所对应的即为**线性支持向量机方法**。（同时适用于线性可分和线性不可分）

另外，数据可能需要非线性最优超平面进行分类。对应方法为**非线性支持向量机方法** -> 此时往往使用**核方法**获得不同的非线性支持向量机特性。
![[attachments/Pasted image 20240327154556.png|300]]

> [!NOTE] 核方法原理
> 核方法实际上是基于Mercer核展开定义通过内积函数定义的非线性变换将输入空间映射到高位特征空间， 并在高维空间中寻找相应的关系。

![[Excalidraw/2.SVM支持向量机 2024-03-30 10.49.21]]
## 二、线性支持向量机
### (1) 问题引入 
对于线性超平面, 分类决策边界可以写成(其中w, b未知):
$$w^{T}  x + b  =0$$
决策边界由参数w,b共同决定。首先假设决策编辑将训练样本($x_1, x_2,\dots$)正确分类， 则显然有
$$\begin{cases}
w^{T} x_{i} + b \geq  \varepsilon \qquad y_{i} = 1 \\
w^{T} x_{i} + b \leq  \varepsilon \qquad y_{i} = -1
\end{cases}$$
此时，**显然按照比例调整(扩大)决策边界的参数w,b(可以按比例调整)可得到**: 
$$\begin{cases}
w^{T} x_{i} + b \geq  1 \qquad y_{i} = 1 \\
w^{T} x_{i} + b \leq  -1 \qquad y_{i} = -1
\end{cases}$$
为了计算两个超平面之间的边缘$\gamma$, 则在两个**边缘超平面上各取一个样本点**，使得:
$$w^{T} x_{1} + b = 1 \qquad  w^{T}x_{2} +b =-1$$
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/第八章 向量代数和空间解析几何|第八章 向量代数和空间解析几何]], 此时容易说明超平面距离显然计算为：
$$\boxed{\gamma =\frac{2}{||w||}\qquad  ||w|| = w_{1}^{2} + w_{2}^{2} + \dots + w_{n}^{2}}$$
只需寻找对应的参数w,b使得 $\gamma$ 最大即可, 可以转换为下面的二次目标函数优化问题:
$$\text{argmax} \frac{2}{||w||} \leftrightarrows \text{argmin}\frac{1}{2}||w||^{2}$$
### (2) 优化求解
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/👆重要定理/拉格朗日乘数法求条件极值|拉格朗日乘数法求条件极值]], 使用拉格朗日乘子$\alpha_{i} \geq  0$, 条件是**点均在边界上**，即
$$\boxed{\forall  i , \qquad  y_{i}(w^{T} x + b) \geq  1}\tag{2.1}$$
显然 $y_{i}(w^{T} x_{i} + b) -1 \geq 0$, 获得拉格朗日函数($\alpha_i \geq 0$):
$$L(w, b, \alpha) = \frac{1}{2}||w||^{2}- \sum^{n}_{i=1} \alpha_{i}(y_{i}(w^{T} x_{i} + b) -1 )$$
我们取$L(w,b, \alpha)$==对$w$和$b$的偏导为0==，获得:
$$||w|| =\sum^{n}_{i=1}  \alpha_{i} y_{i}x_{i}\qquad  \sum^{n}_{i=1} a_{i}y_{i}= 0$$
此时代入， 得到优化问题为**使下面的式子最大**(由于||w||二次项有负号导致的):
$$L(w, b, \alpha) = -\frac{1}{2}\sum^{n}_{i=1} \sum^{n}_{j=1} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} + \sum^{n}_{i=1} \alpha_{i}$$
变形得到最终优化问题
$$\Large\boxed{\min_{\alpha} \frac{1}{2}\sum^{n}_{i=1} \sum^{n}_{j=1}  \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} -\sum^{n}_{i=1} \alpha_{i} }$$
只需求解相应的$\alpha$即可。 

### (3) 不等式约束优化与松弛变量引入
我们再次考虑式(2.1), 可以写为: 
$$g_{i} (w) = 1 -  y_{i}(w^{T} x + b ) \leq 0$$
显然不是所有的点都满足$= 1$的约束条件, 即**部分有$>1$成立**。 此时我们补充引入松弛变量 $\delta_{i}^{2}$, 使得不等式约束转化为等式约束:
$$1 -  y_{i} (w^{T} x + b)  + \delta_{i}^{2}  = 0$$
此时Lagrange函数(求解最大)变为:
$$\Large\boxed{L (w, b, a ) = \frac{1}{2}||w||^{2} + \sum^{n}_{i=1} \alpha_{i}(1 + \delta_{i}^{2} - y _{i}(w^{T} x + b)) }$$
其中$\alpha_{i}\geq 0$; 
对上式中的 $w,  \delta_{i}$ 以及 $\alpha_i$ 进行求导, 得到:
$$\Large \begin{cases}
||w_{i}||  - \sum^{n}_{j=1}  \alpha_{j} y_{j} x_{j} = 0  \\
2  \alpha_{i} \delta_{i} = 0    \\
1 + \delta_{i}^{2} - y_{i} (w^{T}  x + b) = 0  \\
\alpha_{i} \geq  0
\end{cases}$$
### (4) KKT 条件
KKT条件(**Karush-Kuhn-Tucker 条件**), 是在满足某些规则的条件下， 非线性规划问题能够得到最优解的充要条件。

具体推导参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/✈️航空发动机可靠性分析/推导部分/对偶优化一致解条件推导过程.pdf|对偶优化一致解条件推导过程.pdf]] 和 [KKT条件详解](https://zhuanlan.zhihu.com/p/26514613)

我们获得的KKT问题的基本表达式为:
在$g_{i}(x) \leq  0, h_{j}  (x) = 0$条件下, 求解最小化的f(x)值:
$$\min f(x)$$
此时, 构造下式:
$$L(x, \alpha, \beta) =  f(x) + \sum^{m}_{i=1} \alpha_{i}g_{i}(x) + \sum^{k}_{j=1} \beta_{j} h_{j}(x)$$
KKT 条件给出了判断解 $x^*$ 是否为最优解的<b><mark style="background: transparent; color: blue">必要条件</mark></b>: 
$$\Large \begin{cases}
\frac{\partial L_{i}(x, \alpha^{*} , \beta^{*})}{\partial x_{l}} = \frac{\partial f}{\partial x_{l}}+ \sum^{m}_{i=1} \frac{\partial\alpha_{i}}{\partial x_{l}} g_{i}(x) + \sum^{k}_{j=1}\frac{\partial \beta_{j}}{\partial x_{l}}h_{j}(x) = 0 \\
\alpha_{i} g_{i} (x) = 0   \\
g_{i}(x) \leq  0,  h_{j} (x) = 0 \\
\alpha_{i} \geq 0 
\end{cases}$$
原理是推导中$\alpha_1 a_1 = 0$, 因此仅有$\alpha_i$起作用(不为零)时, $g_{i}(x) = 0$, 不起作用时,$g_i(x) < 0$, 因此必然有$\alpha_i g(x) = 0$成立。

最优解必然满足上述所有条件。

## 三、非线性支持向量机

`````ad-todo
title: 推导
collapse: open
`````

### 常用的核函数
#### 1. 点积形式
获得的是线性分类器
$$K(x,y) = x \cdot  y$$
#### 2. 多项式形式的核函数
获取d阶多项式分类器:
$$K(x,y) = \left\{ (x \cdot y) + 1\right\}^{d}$$
#### 3. 径向基函数核函数(高斯核)
径向基函数分类器
$$\Large K(x,y) = \exp\left(-\frac{||x - y||^{2}}{2\sigma^{2}}\right)$$
其中, $\sigma$ 为高斯核带宽。
#### 4. Sigmoid 形式核函数
$$K(x,y) = \tanh (kx \cdot y - \delta)$$
其中, $k > 0$, $\delta <  0$; 
#### 5. 指数核函数
$$k(x_{i}, x_{j}) = \exp \left(- \frac{||x_{i} - x_{j}||}{\sigma}\right)$$
