## 一、基本介绍
### (1) Logistic 函数
支持向量机(Suppported Vector Machine, SVM), 是一种分类算法,其一般目标是在空间中找到一个超平面, 并表示方程为
![[attachments/Pasted image 20240327152229.png|300]]
$$w^{T} x + b = 0$$
我们首先介绍Logistic函数, 即将数据从$(-\infty, +\infty)$ 映射到(0,1)的函数，其形式如下:
$$\boxed {h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1 + e^{-\theta^{T}x}}}$$
显然,当$\theta^{T}x > 0$时,$h_{\theta}(x) >0.5$, 获得的图像如下图:
![[attachments/Pasted image 20240327152635.png|400]]
其中上式中的$\theta^{T} x$即对应一个对输入参数(n维)的线性变换:
$$\theta^{T} x = \theta_{0}x_{0} + \theta_{1} x_{1}  + \theta_{2}x_{2} + \dots  + \theta_{n}x_{n}\qquad  (x_{0} = 1)$$
另外其中$x_1,\dots x_n$代表了多个不同训练样本值（x_0=1）也可以使用不同的基函数进行代替. 
对应的分类目的, 就是求解对应的超参数$\theta_{1}, \theta_{2}, \dots$,使得训练数据中对应的 y=1 部分, $\theta^{T} x >>0$, 而 y=0 部分, $\theta^{T} x << 0$，  

### (2)最大边缘超平面
Maximal Margin Hyperplane
![[attachments/Pasted image 20240327154353.png|300]]
由于将训练样本点分开的超平面很多，所以往往必须有最大边缘超平面作为决策边界。我们考虑和每个超平面平行的一对仍然可以将样本点分开的超平面,而其中两平面间距称为边缘(margin)

最大边缘超平面即距离最长的一组超平面(B1), 而我们往往以其为分类依据, 以获得更好的泛化误差。这种方法所对应的即为**线性支持向量机方法**。（同时适用于线性可分和线性不可分）

另外，数据可能需要非线性最优超平面进行分类。对应方法为**非线性支持向量机方法** -> 此时往往使用**核方法**获得不同的非线性支持向量机特性。
![[attachments/Pasted image 20240327154556.png|300]]

> [!NOTE] 核方法原理
> 核方法实际上是基于Mercer核展开定义通过内积函数定义的非线性变换将输入空间映射到高位特征空间， 并在高维空间中寻找相应的关系。

## 二、线性支持向量机
### (1) 问题引入 
对于线性超平面, 分类决策边界可以写成(其中w, b未知):
$$w^{T}  x + b  =0$$
决策边界由参数w,b共同决定。首先假设决策编辑将训练样本($x_1, x_2,\dots$)正确分类， 则显然有
$$\begin{cases}
w^{T} x_{i} + b > 0 \qquad y_{i} = 1 \\
w^{T} x_{i} + b < 0 \qquad y_{i} = -1
\end{cases}$$
此时，按照比例调整(扩大)决策边界的参数w,b(可以按比例调整)显然可得到:
$$\begin{cases}
w^{T} x_{i} + b \geq  1 \qquad y_{i} = 1 \\
w^{T} x_{i} + b \leq  -1 \qquad y_{i} = -1
\end{cases}$$
为了计算两个超平面之间的边缘$\gamma$, 则在两个边缘超平面上各取一个样本点，使得:
$$w^{T} x_{1} + b = 1 \qquad  w^{T}x_{2} +b =-1$$
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/第八章 向量代数和空间解析几何|第八章 向量代数和空间解析几何]], 此时容易说明超平面距离显然计算为：
$$\boxed{\gamma =\frac{2}{||w||}\qquad  ||w|| = w_{1}^{2} + w_{2}^{2} + \dots + w_{n}^{2}}$$
只需寻找对应的参数w,b使得 $\gamma$ 最大即可, 可以转换为下面的二次目标函数优化问题:
$$\text{argmax} \frac{2}{||w||} \leftrightarrows \text{argmin}\frac{1}{2}||w||^{2}$$
### (2) 优化求解
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/👆重要定理/拉格朗日乘数法求条件极值|拉格朗日乘数法求条件极值]], 使用拉格朗日乘子$\alpha_{i} \geq  0$, 条件是点均在边界上$y_{i}(w^{T} x + b) -1=0$获得拉格朗日函数:
$$L(w, b, \alpha) = \frac{1}{2}||w||^{2}- \sum^{n}_{i=1} \alpha_{i}(y_{i}(w^{T} x_{i} + b) -1 )$$
我们取$L(w,b, \alpha)$==对$w$和$b$的偏导为0==， 获得:
$$||w|| =\sum^{n}_{i=1}  \alpha_{i} y_{i}x_{i}\qquad  \sum^{n}_{i=1} a_{i}y_{i}= 0$$
此时代入， 得到优化问题为**使下面的式子最大**(由于||w||二次项有负号导致的):
$$L(w, b, \alpha) = -\frac{1}{2}\sum^{n}_{i=1} \sum^{n}_{j=1} \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} + \sum^{n}_{i=1} \alpha_{i}$$
变形得到最终优化问题
$$\Large\boxed{\min_{\alpha} \frac{1}{2}\sum^{n}_{i=1} \sum^{n}_{j=1}  \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} -\sum^{n}_{i=1} \alpha_{i} }$$
只需求解相应的$\alpha$即可。 


## 三、非线性支持向量机

`````ad-todo
title: 推导
collapse: open
`````

### 常用的核函数
#### 1. 点积形式
获得的是线性分类器
$$K(x,y) = x \cdot  y$$
#### 2. 多项式形式的核函数
获取d阶多项式分类器:
$$K(x,y) = \left\{ (x \cdot y) + 1\right\}^{d}$$
#### 3. 径向基函数核函数
径向基函数分类器
$$\Large K(x,y) = e^{-||x - y||^{2}/2\sigma^{2}}$$
#### 4. Sigmoid 形式核函数
$$K(x,y) = \tanh (kx \cdot y - \delta)$$
