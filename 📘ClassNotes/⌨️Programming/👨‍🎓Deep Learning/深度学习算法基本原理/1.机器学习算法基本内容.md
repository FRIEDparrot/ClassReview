## ä¸€ã€æœºå™¨å­¦ä¹ åŸºæœ¬å†…å®¹
ç¥ç»ç½‘ç»œçš„ä¸»çº¿åŒ…å«: 
1. åˆ†ç±», å›å½’å’Œé¢„æµ‹ 
2. è´å¶æ–¯ç†è®ºå’Œæ™ºèƒ½æ¨ç† 
3. ç®—æ³•éƒ¨åˆ†

```python
a = [1,2,3,4]
np.mat(a)
```
é€šè¿‡ numpy åº“å¯ä»¥å®ç°çŸ¢é‡åŒ–ç¼–ç¨‹, é€šè¿‡ GPU è¿›è¡Œå¹¶è¡Œè®¡ç®—å’Œå¤§è§„æ¨¡æµ®ç‚¹è¿ç®—, å¯ä»¥æä¾›è¾ƒé«˜çš„å¤„ç†æ€§èƒ½ã€‚

### (1) èŒƒæ•°, æ¬§å¼è·ç¦», åˆ‡æ¯”é›ªå¤«è·ç¦»å’Œæ±‰æ˜è·ç¦»

> [!caution] èŒƒæ•°çš„å®šä¹‰
> å¯¹äºä¸€èˆ¬çš„èŒƒæ•°, Lp èŒƒæ•°å®šä¹‰ä¸ºå„ä¸ªå…ƒç´ çš„pæ¬¡æ–¹çš„å’Œçš„ 1/p æ¬¡æ–¹, è€Œè·ç¦»ä¸€èˆ¬å®šä¹‰ä¸º:
> $$(\sum^{n}_{i=1} |p_{i} - q_{i}|^{k})^{\frac{1}{k}}$$

ä¸€èˆ¬åœ°, æˆ‘ä»¬å°†ä¸¤ä¸ªå‘é‡çš„ç‰¹å¾é‡‡ç”¨æµ®ç‚¹æ•°è¿›è¡Œè¡¨ç¤º,åˆ™å¯ä»¥é‡‡ç”¨è·ç¦»(èŒƒæ•°)æ¥è¯´æ˜æŸä¸ªæ•°æ®, è€Œä¸€èˆ¬åœ°, è·ç¦»è¶Šè¿‘çš„å‘é‡, è¶Šå®¹æ˜“å½’ä¸ºä¸€ç±»ã€‚

å¯¹äº**ä¸¤ç‚¹çš„æ¬§æ°è·ç¦»**,å¯ä»¥é‡‡ç”¨ä¸‹å¼è¿›è¡Œè®¡ç®—:
$$d_{12}  =  \sqrt{(A - B) (A- B)^{T}}$$
è€Œ**åˆ‡æ¯”é›ªå¤«è·ç¦»**ä¸º $\max(|x2 - x1|, |y2 - y1|)$, å¯ä»¥é‡‡ç”¨
$$\lim_{k \rightarrow \infty} (\sum^{n}_{i=1} |x_{1i} - x_{2i}|^{k})^{\frac{1}{k}}$$
å…¶äºŒ, æˆ‘ä»¬å¸¸å¸¸é€šè¿‡æ–¹å‘ä½™å¼¦è¡¡é‡æ ·æœ¬é‡ä¹‹é—´çš„å·®å¼‚, å‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/ğŸ“ˆAdvanced Mathematics/ç¬¬å…«ç«  å‘é‡ä»£æ•°å’Œç©ºé—´è§£æå‡ ä½•|ç¬¬å…«ç«  å‘é‡ä»£æ•°å’Œç©ºé—´è§£æå‡ ä½•]], æœ‰
$$\cos \theta = \frac{\vec{a}\cdot\vec{b}}{|a||b|}$$

> [!NOTE] æ±‰æ˜è·ç¦»
> **æ±‰æ˜è·ç¦»**å®šä¹‰ä¸ºä¸¤ä¸ªç­‰é•¿å­—ç¬¦ä¸²ä¹‹é—´çš„<mark style="background: transparent; color: red">æ±‰æ˜è·ç¦»å®šä¹‰ä¸º</mark>**å°†å…¶ä¸­ä¸€ä¸ªå˜ä¸ºå¦å¤–ä¸€ä¸ªçš„æœ€å°äº¤æ¢æ¬¡æ•°**:
> ä¸€èˆ¬å¯ä»¥é‡‡ç”¨å¦‚ä¸‹æ–¹æ³•è·å–:
> ```python
> s1 = np.mat([1,0,1,0,0,1,1])
> s2 = np.mat([0,1,0,1,1,1,0])
> len(np.nonzero(s1 - s2)[1])
> ```
> æ±‰æ˜è·ç¦»å¸¸ç”¨äºä¿¡æ¯ç¼–ç , ä¸€èˆ¬ä¼šä½¿ç¼–ç ä¹‹é—´çš„æ±‰æ˜è·ç¦»å°½å¯èƒ½å¤§ä»¥å¢åŠ å®¹é”™ç‡

**æ°å¡å¾·ç›¸ä¼¼ç³»æ•°**:å¯¹äºä¸¤ä¸ªé›†åˆ A, B ä¸­çš„äº¤é›†å…ƒç´ åœ¨ A, B å¹¶é›†ä¸­å çš„æ¯”ä¾‹ä¸ºæ°å¡å¾·ç›¸ä¼¼ç³»æ•°ã€‚ç”¨äºè¡¡é‡ä¸¤ä¸ªé›†åˆçš„ç›¸ä¼¼åº¦ã€‚
$$J(A,B) =  \frac{|A \bigcap B|}{|A\bigcup B |}$$
**æ°å¡å¾·è·ç¦»**: å®šä¹‰ä¸º 
$$J_{\delta} (A, B) = 1 - J(A, B) = \frac{|A \bigcup  B | + |A \bigcap B |}{|A \bigcup B |}$$
### (2) å¤šå…ƒç»Ÿè®¡åŸºç¡€
> [!NOTE] å¸¸è§çš„å¤šå…ƒç»Ÿè®¡ç®—æ³•
> åŒ…æ‹¬æœ´ç´ è´å¶æ–¯åˆ†æ,å›å½’åˆ†æ,ç»Ÿè®¡å­¦ä¹ åŸºç¡€, èšç±»åˆ†æ, ä¸»æˆåˆ†åˆ†æ(PCA), æ¦‚ç‡å›¾æ¨¡å‹ç­‰ç­‰ã€‚

æœ€å¸¸ç”¨çš„è´å¶æ–¯å…¬å¼è¡¨è¾¾äº†æ¡ä»¶æ¦‚ç‡çš„å…³ç³»:
$$P(B|A) =  \frac{P (A |B) P(B)}{P(A)}$$
ä»æ¦‚ç‡ç»Ÿè®¡çš„è§’åº¦, ä¸€ä¸ªå¯¹è±¡å¯ä»¥è¡¨ç¤ºä¸ºnä¸ªéšæœºå˜é‡çš„æ•´ä½“, å…¶ä¸­ $X = (X_1,X_2, \dots X_n)$ ä¸º n ç»´éšæœºå˜é‡æˆ–è€…éšæœºå‘é‡ã€‚**æ¯ä¸ªå¯¹è±¡æ˜¯éšæœºå‘é‡ä¸­çš„ä¸€ç»„å–å€¼, è€ŒçŸ©é˜µä¸­çš„æ‰€æœ‰å¯¹è±¡æ„æˆäº†éšæœºå‘é‡çš„è”åˆå’Œè¾¹ç¼˜å¯†åº¦æ¦‚ç‡åˆ†å¸ƒ**ã€‚
ä¾‹å¦‚, å¯¹äº10ä¸ªè‹¹æœ, å…¶ä¸­çº¢çš„8ä¸ª, é»„çš„2ä¸ª; è€Œæ¢¨æœ‰é»„è‰²å’Œç»¿è‰², å…¶ä¸­é»„è‰²ä¸º9ä¸ªã€‚æ­¤æ—¶, å¯ä»¥æ„å»ºå‡ºå¦‚ä¸‹çš„æ¦‚ç‡è¡¨, å¯¹åº”çš„**è”åˆæ¦‚ç‡å¯†åº¦åˆ†å¸ƒ**å’Œ**è¾¹ç¼˜æ¦‚ç‡å¯†åº¦åˆ†å¸ƒ**å¦‚å›¾:
![[Excalidraw/1.æœºå™¨å­¦ä¹ ç®—æ³•åŸºæœ¬å†…å®¹ 2024-09-05 11.09.17|650]]
æ‰€æœ‰ä»¥æ¦‚ç‡ç›¸å…³çš„ç®—æ³•**å‡ä»¥å¯¹è±¡çš„è”åˆæ¦‚ç‡å¯†åº¦åˆ†å¸ƒå’Œè¾¹ç¼˜æ¦‚ç‡å¯†åº¦åˆ†å¸ƒä¸ºè¿ç®—åŸºç¡€**ã€‚

ç›¸å…³ç³»æ•°å®šä¹‰å‚è€ƒ[[ğŸ“˜ClassNotes/ğŸ“Mathmatics/ğŸ£Probability Theory/ç¬¬å››ç«  éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾#ä¸‰ã€åæ–¹å·®åŠå…¶ç›¸å…³ç³»æ•°|éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾]], å®šä¹‰ä¸º(èŒƒå›´ä¸º -1 ~ 1):
$$\boxed{\rho_{XY} = \frac{E\{[X - E(X)][Y - E(Y)]\}}{\sqrt{D(X)} \sqrt{D(Y)}}}$$
è€Œ**ç›¸å…³è·ç¦»**å®šä¹‰ä¸º1ä¸è¯¥ç³»æ•°çš„å·®å€¼: 
$$D_{XY} = 1 - \rho_{XY}$$
æ ¹æ®éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾, æˆ‘ä»¬å¯ä»¥å°†æ¬§å¼è·ç¦»å‘å±•ä¸ºé©¬æ°è·ç¦»å…¬å¼:

**é©¬æ°è·ç¦»**:
é©¬æ°è·ç¦»(Mahalanobis Distance),å®šä¹‰ä¸ºå¯¹äºMä¸ªæ ·æœ¬å‘é‡ $X_1 \sim X_{m}$, å…¶åæ–¹å·®çŸ©é˜µè®°ä¸º$S$, å‡å€¼ä¸º $\mu$, åˆ™æ ·æœ¬å‘é‡åˆ°å‡å€¼ $\mu$ çš„é©¬æ°è·ç¦»å®šä¹‰ä¸º: 
$$D(x) = \sqrt{(\boldsymbol{x} - \mu)^{T} S^{-1} (\boldsymbol{x} - \mu)}$$
å…¶ä¸­, å¯¹äºä¸¤ä¸ªå‘é‡ $X_{i} ,X_{j}$, åˆ™å…¶é©¬æ°è·ç¦»å®šä¹‰ä¸º:
$$\Large\boxed{D(X_{i}, X_{j}) =  \sqrt{(X_{i} - X_{j})^{T }  S^{-1} (X_{i} - X_{j})}}$$
å…¶ä¸­å½“åæ–¹å·®ä¸ºå¯¹è§’çŸ©é˜µæ—¶, è½¬æ¢ä¸ºæ¬§æ°è·ç¦»ã€‚å…¶<mark style="background: transparent; color: red">ä¼˜ç‚¹æ˜¯é‡çº²æ— å…³, æ’é™¤å˜é‡ä¹‹é—´çš„ç›¸äº’å¹²æ‰°</mark>ã€‚

è®¡ç®—ä»£ç å¦‚ä¸‹, æ¯”è¾ƒç®€å•:
```python
import numpy as np
import numpy.linalg as la  

def mahalanobis_distance(x1 : np.matrix, x2:np.matrix, S):  
    """  
    Calculates the Mahalanobis distance between two vectors x1 and x2 given the covariance matrix S.  
    Parameters:    x1 (numpy.ndarray): The first vector.    x2 (numpy.ndarray): The second vector.    S (numpy.ndarray): The covariance matrix.    Returns:    float: The Mahalanobis distance between x1 and x2.    """    diff = x1 - x2  
    D = np.sqrt(diff.T * la.inv(S) * diff)  
    return D  
  
if __name__ == "__main__" :  
    x1 = np.mat([1, 2, 3])  
    x2 = np.mat([4, 5, 6])  
    S  = np.mat([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  
    print(mahalanobis_distance(x1.T, x2.T, S))
```

### (3) çŸ©é˜µçš„ç©ºé—´å˜æ¢
ç©ºé—´çš„å˜æ¢ç‰¹æ€§æ˜¯çŸ©é˜µæœ€é‡è¦çš„ç‰¹æ€§ä¹‹ä¸€, <mark style="background: transparent; color: red">ç”±ç‰¹å¾åˆ—çš„å–å€¼èŒƒå›´æ‰€æœ‰æ„æˆçš„çŸ©é˜µç©ºé—´åº”å½“å…·å®Œæ•´æ€§ï¼Œ å¹¶èƒ½å¤Ÿåäº‹ç‰©çš„ç©ºé—´å½¢å¼æˆ–è€…å˜åŒ–è§„å¾‹</mark>ã€‚
ä¸€èˆ¬è€Œè¨€, è®­ç»ƒé›†æ‰€æ„æˆçš„çŸ©é˜µå¸¸å¸¸åŒ…å«å¤§é‡çš„æ ·æœ¬, è¦æ±‚: 
1. å‘é‡å’ŒçŸ©é˜µçš„ç›¸å…³è¿ç®—æ˜¯å…·æœ‰æ„ä¹‰çš„è¿ç®—, å¹¶ä¸”èƒ½å¤Ÿè§£é‡Šå‡ºäº‹ç‰©çš„ç©ºé—´å½¢å¼ï¼Œ 
2. è®¡ç®—ç»“æœåº”å½“èƒ½å¤Ÿåæ˜ å…¶æœ¬è´¨ç‰¹å¾, ä»è€Œæ˜ å°„å‡ºå¯¹è±¡é›†åˆè¡¨è¾¾çš„å½¢å¼å’Œè§„å¾‹ã€‚
nç»´æ­£äº¤ç©ºé—´å³ä¸º n ä¸ªå½¼æ­¤æ­£äº¤çš„åŸºåœ°å‘é‡æ„æˆçš„ç©ºé—´

åœ¨åŸºåº•è¿›è¡Œä¸€å®šçš„å˜æ¢æ—¶, ç›¸åº”çš„æ‰€å¾—çš„åæ ‡ä¹Ÿä¼šå‘ç”Ÿç›¸åº”çš„å˜æ¢, ä¾‹å¦‚åœ¨ç›´è§’åæ ‡ç³»ä¸­çš„ $\alpha = (1,2)$, è®¡ç®—æ–°åæ ‡ç³»i'=(3,1), j'=(1,3)ä¸‹çš„åæ ‡, å®¹æ˜“å¾—å‡ºæ˜¯ i' * 1 + j' * 2 = (5,7):

åˆ™ä»(1,2) åˆ° (5,7) çš„è¿‡ç¨‹, åˆ™<mark style="background: transparent; color: red">å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå˜æ¢çŸ©é˜µ</mark>, ç›´æ¥å®Œæˆå˜æ¢è¿‡ç¨‹ :
```python
A = [[3,1], [1,3]]
i' =  A * i 
j' = A * j
```
$$\alpha' = A * \alpha = [\alpha_{1}, \alpha_{2}] * \alpha  =  (5,7)$$
å…¶ä¸­ `A = [[3,1], [1,3]]` ä¸ºå˜æ¢çŸ©é˜µ, å°†åæ ‡ä» i, j å˜æ¢åˆ° $[\alpha_1, \alpha_2]$ æ„æˆçš„æ–°åæ ‡ç³»ã€‚

éœ€è¦è¯´æ˜çš„æ˜¯, å‘é‡çš„å˜æ¢å¯ä»¥åˆ†è§£ä¸ºä¼¸ç¼©å’Œæ—‹è½¬, ç±»ä¼¼å¤æ•°è¿ç®—, è®¾ $\alpha = [r \cos \theta, r \sin \theta]$, åˆ™
$$  \lambda  e^{i\theta} * \alpha =  \lambda  *  \left[\begin{matrix}
\cos \Delta \theta  &  - \sin \Delta  \theta  \\ 
\sin \Delta  \theta  &  \cos \Delta  \theta  
\end{matrix}\right]  *  \left[\begin{matrix}
r \cos \theta  \\ r \sin \theta
\end{matrix}\right]$$
å¦å¤–, æ ¹æ®çŸ©é˜µçš„ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼$\lambda$, æˆ‘ä»¬å¯ä»¥è¿˜åŸåˆå§‹çš„çŸ©é˜µ:
$$A = P  \Lambda P^{-1}$$
å…¶ä¸­ P ä¸ºç‰¹å¾å‘é‡æ„æˆçš„çŸ©é˜µ, $\Lambda$ ä¸ºç‰¹å¾å€¼æ’æˆçš„å¯¹è§’çŸ©é˜µ; ç‰¹å¾å€¼ ($Av = \lambda v$)
```python 
import numpy as np  
import numpy.linalg as la  
  
A = np.mat([[8,1,6],[3,5,7],[4,9,2]], dtype=np.float64)  
lmbda, v = la.eig(A)  
  
Sigma = np.diag(lmbda)  
B = v * Sigma * la.inv(v)  
print(B)
```

é’ˆå¯¹æ•°æ®çš„å½’ä¸€åŒ–ä¸€èˆ¬çš„æ–¹æ³•æ˜¯é€šè¿‡æ¯”ä¾‹ç¼©æ”¾, å¯¹ç‰¹å¾è¿›è¡Œè§„èŒƒåŒ–å¤„ç†, æ˜ å°„åˆ°æŸä¸ªåŒºé—´ã€‚
$$X^{*} =  \frac{x - \mu}{\sigma}$$
## äºŒã€æ–‡æœ¬æŒ–æ˜æ–¹æ³•
ä¸€èˆ¬çš„åšæ³•æ˜¯ä»å¤§é‡æ–‡æœ¬æ•°æ®ä¸­æŠ½å–äº‹å…ˆæœªçŸ¥çš„, å¯ç†è§£çš„, å¯ç”¨çš„çŸ¥è¯†çš„è¿‡ç¨‹ã€‚ä¸»è¦åŒ…å«: 
1. æœç´¢å’Œä¿¡æ¯æ£€ç´¢
2. æ–‡æœ¬èšç±», æ–‡æœ¬åˆ†ç±»,æ–‡æœ¬æŒ–æ˜, ä¿¡æ¯æŠ½å–(IE), è‡ªç„¶è¯­è¨€å¤„ç†(NLP), æ¦‚å¿µæå–

ç›®å‰çš„æ–‡æœ¬åˆ†ç±»ç®—æ³•ä¸»æµåŒ…å«: 
1. åŸºäºæ¨¡å¼ç³»ç»Ÿ
2. åŸºäºåˆ†ç±»æ¨¡å‹ (æœºå™¨å­¦ä¹ , ä¸€èˆ¬éœ€è¦ä¸€ç»„é¢„åˆ†ç±»å’Œè®­ç»ƒæ¥å»ºç«‹åˆ†ç±») 

<mark style="background: transparent; color: red">å®Œå…¨è§£å†³ä¸­æ–‡åˆ†è¯ç®—æ³•çš„æ˜¯åŸºäºæ¦‚ç‡å›¾æ¨¡å‹çš„æ¡ä»¶æ¦‚ç‡éšæœºåœº(CRF)æ–¹æ¡ˆ, </mark> 

### (1) æœ´ç´ è´å¶æ–¯æ–¹æ³•çš„æ¨å¯¼
å‚è€ƒ https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes (ä¸Šé¢ä¹Ÿç»™å‡ºäº†å¤§é‡å…¶ä»–æœºå™¨å­¦ä¹ ç®—æ³•çš„æ¨å¯¼)

â€œnaiveâ€ assumption of conditional independence between every pair of features given the value of the class variable. (see https://en.wikipedia.org/wiki/Naive_Bayes_classifier) 
![[attachments/Pasted image 20240905231216.png]]

æœ´ç´ è´å¶æ–¯æ–¹æ³•å‡è®¾<mark style="background: transparent; color: red">yå€¼å’Œxå€¼æ˜¯ä¸€ä¸€å¯¹åº”çš„, å³åœ¨ä¸€ä¸ªç¡®å®šçš„xä¸‹ä»…æœ‰ä¸€ä¸ªyå¯ä»¥å‡ºç°, æœ‰</mark> 
$$\Large\boxed{P(x_{i} | y , x_{1},  x_{i-1} , \dots x_{i+1}, \dots  x_{n}) = P(x_{i}| y)}$$
åŒæ—¶, å°† $P (x_1, x_2, \dots  x_{n} | y)$ ä»¥è¾¹ç¼˜æ¦‚ç‡å¯†åº¦ä¹˜ç§¯è¡¨ç¤º, ç”±äºç›¸äº’ç‹¬ç«‹,å°†$P(x_i, x_i+1, \dots x_{n}| y )$é‡‡ç”¨ä¹˜ç§¯è¿›è¡Œè¡¨ç¤º, åŒæ—¶, ä»£å…¥ä¸Šå¼, å¹¶é‡‡ç”¨ [MAP estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) å¾—åˆ°: 
$$P (y | x_{1}, x_{2} , \dots x_{n})  \propto P(y) \prod^{n}_{i=1} P(x_{i}|y)$$
ä»£å…¥ä¸Šå¼, æœ‰:
$$\hat{y} = \text{arg }\max_{y}P(y) \prod^{n}_{i=1}  P(x_{i}|y)$$
è¿™ä¸ªå®é™…ä¸Šç±»ä¼¼äº[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/æ·±åº¦å­¦ä¹ ç®—æ³•åŸºæœ¬åŸç†/è¡¥å……çŸ¥è¯†/2. MAP estimation(æœ€å¤§å•åéªŒå¯èƒ½æ€§ä¼°è®¡)|2. MAP estimation(æœ€å¤§å•åéªŒå¯èƒ½æ€§ä¼°è®¡)]], åŒæ—¶, ä¸åŒæœ´ç´ è´å¶æ–¯æ¨¡å‹çš„åŒºåˆ«ä¸»è¦æ˜¯ $P(x_i|y)$ ä¸ç›¸åŒã€‚

### (2) é«˜æ–¯æœ´ç´ è´å¶æ–¯æ–¹æ³•
é«˜æ–¯æœ´ç´ è´å¶æ–¯æ–¹æ³•, å°†ä¼¼ç„¶å‡½æ•°å‡è§†ä¸ºé«˜æ–¯å‡½æ•°, å¹¶ä¸”å˜é‡ç‹¬ç«‹åŒåˆ†å¸ƒ:
$$P(x_{i}| y) = \frac{1}{\sqrt{2 \pi  \sigma_{y}^{2}} }\exp\left(- \frac{(x_{i} - \mu )^{2}}{2 \sigma_{y}^{2}}\right)$$

> [!CAUTION] è¯´æ˜
> é«˜æ–¯æœ´ç´ è´å¶æ–¯è®¤ä¸º, æ ·æœ¬çš„å–å€¼æ˜¯è¿ç»­çš„, åŒæ—¶æœä»è¿‘ä¼¼çš„é«˜æ–¯åˆ†å¸ƒã€‚å› æ­¤å¯ç”¨äº iris æ•°æ®é›†ä¸­ã€‚

ä½¿ç”¨æ–¹æ³•ç›´æ¥é‡‡ç”¨ 
```python 
from sklearn.datasets import load_iris  
from sklearn.naive_bayes import GaussianNB  
from sklearn.model_selection import train_test_split  
  
[data, target] = load_iris(return_X_y=True)  
  
X_train, X_test, y_train, y_test = train_test_split(data, target, train_size= 0.8,test_size=0.2, random_state=0)  
gnb = GaussianNB()  
predictor = gnb.fit(X_train, y_train)  
y_pred = gnb.predict(X_test)
print(y_pred, y_test)
print("failed to predict", (y_test!=y_pred).sum(), "samples")
```

**å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯** : é’ˆå¯¹äº<mark style="background: transparent; color: red">å¤šé¡¹å¼åˆ†å¸ƒçš„æ•°æ®</mark>, é’ˆå¯¹æ¯ä¸ªå‚æ•° $\theta_{y}= (\theta_{y1}, \theta_{y2}, \dots  \theta_{yn})$, å…¶ä¸­ç‰¹å¾æ•°é‡ä¸º $n$ ä¸ª,   è€Œ$\theta_{yi} = P(x_{i}|y)$ï¼Œä¸º<mark style="background: transparent; color: red">ç‰¹å¾å‡ºç°åœ¨æ ·æœ¬ä¸­çš„æ•°é‡æˆ–è€…æ¦‚ç‡</mark>ã€‚å› æ­¤ï¼Œ<b><mark style="background: transparent; color: blue">feature ä¸ºé¢‘ç‡æ—¶, æ‰èƒ½è€ƒè™‘é‡‡ç”¨å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æ–¹æ³•</mark></b>(features are counts or frequencies, i.e.  non-negative integers) 
```python 
from sklearn.naive_bayes import MultinomialNB
``` 
å‚æ•° $\theta_y$ é€šè¿‡<mark style="background: transparent; color: red">ç›¸å¯¹é¢‘ç‡è®¡æ•°</mark>è¿›è¡Œè®¡ç®—:
$$\Large \boxed{\hat{\theta}_{yi} = \frac{N_{yi} + \alpha}{N_{y} + \alpha n }}$$
å…¶ä¸­, $N_{yi} = \sum_{x \in T} x_{i}$ æ˜¯ç‰¹å¾ i åœ¨è®­ç»ƒé›† T ä¸­å‡ºç°çš„æ¬¡æ•°, è€Œ $N_{y}= \sum^{n}_{i=1} N_{yi}$  ä¸ºæ‰€æœ‰ç‰¹å¾çš„æ€»é‡; $\alpha$ ä¸ºå¹³æ»‘å› å­, å¹¶æ»¡è¶³ $\alpha \geq  0$ ä»¥é˜²æ­¢ $\hat{\theta}_{yi}$ å‡ºç°0çš„è®¡ç®—ç»“æœå¯¼è‡´é¢„æµ‹é”™è¯¯ã€‚å½“ $\alpha = 1$ æ—¶ä¸º Laplace å¹³æ»‘, $\alpha < 1$ ä¸º Lidstone å¹³æ»‘ã€‚

ä»¥é¢‘ç‡ä¸ºåŸºç¡€çš„è®­ç»ƒæ•°æ®ç¤ºæ„å¦‚ä¸‹:

| Class    | buy | now | free | Total Words |
| -------- | --- | --- | ---- | ----------- |
| Spam     | 20  | 5   | 10   | 35          |
| Not Spam | 5   | 15  | 5    | 25          |

<b><mark style="background: transparent; color: blue">è¡¥å……æœ´ç´ è´å¶æ–¯</mark></b>(Complement naive bayes,CNB) å’Œä¼¯åŠªåˆ©è´å¶æ–¯: 
è°ƒç”¨æ–¹æ³•æ˜¯ `from sklearn.naive_bayes import ComplementNB` å’Œ `from sklear.n.naive_bayes import BernoulliNB` 

å…¶ä¸­ BernoulliNB <mark style="background: transparent; color: red">è®¤ä¸ºç‰¹å¾æ˜¯äºŒå€¼é‡, å³å‡ä¸º 0 æˆ–è€… 1</mark>,

CNB <mark style="background: transparent; color: red">ä»ç„¶æ˜¯ä»¥é¢‘ç‡ä¸ºæ ·æœ¬ç‰¹å¾ feature çš„</mark>ã€‚é‡‡ç”¨æ¯ä¸ªç±»çš„**è¡¥é›†æ•°æ®**,è®¡ç®—æ¨¡å‹æƒé‡, (ä¸€èˆ¬CNBçš„å‚æ•°ä¼°è®¡æ¯”MNBæ›´åŠ ç¨³å®š). å¯¹äºCNB, æƒé‡è®¡ç®—ä¸º:
$$\hat{\theta}_{ci} = \frac{\alpha_{i}  + \sum^{n}_{j: y_{j}\neq c} d_{ij} }{\alpha + \sum_{j: y_{j}\neq c } \sum _{k}d_{kj}}$$
