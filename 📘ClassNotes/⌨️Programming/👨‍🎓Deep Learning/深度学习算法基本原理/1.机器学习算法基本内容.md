## 一、机器学习基本内容
神经网络的主线包含: 
1. 分类, 回归和预测 
2. 贝叶斯理论和智能推理 
3. 算法部分

```python
a = [1,2,3,4]
np.mat(a)
```
通过 numpy 库可以实现矢量化编程, 通过 GPU 进行并行计算和大规模浮点运算, 可以提供较高的处理性能。

### (1) 范数, 欧式距离, 切比雪夫距离和汉明距离

> [!caution] 范数的定义
> 对于一般的范数, Lp 范数定义为各个元素的p次方的和的 1/p 次方, 而距离一般定义为:
> $$(\sum^{n}_{i=1} |p_{i} - q_{i}|^{k})^{\frac{1}{k}}$$

一般地, 我们将两个向量的特征采用浮点数进行表示,则可以采用距离(范数)来说明某个数据, 而一般地, 距离越近的向量, 越容易归为一类。

对于**两点的欧氏距离**,可以采用下式进行计算:
$$d_{12}  =  \sqrt{(A - B) (A- B)^{T}}$$
而**切比雪夫距离**为 $\max(|x2 - x1|, |y2 - y1|)$, 可以采用
$$\lim_{k \rightarrow \infty} (\sum^{n}_{i=1} |x_{1i} - x_{2i}|^{k})^{\frac{1}{k}}$$
其二, 我们常常通过方向余弦衡量样本量之间的差异, 参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/第八章 向量代数和空间解析几何|第八章 向量代数和空间解析几何]], 有
$$\cos \theta = \frac{\vec{a}\cdot\vec{b}}{|a||b|}$$

> [!NOTE] 汉明距离
> **汉明距离**定义为两个等长字符串之间的<mark style="background: transparent; color: red">汉明距离定义为</mark>**将其中一个变为另外一个的最小交换次数**:
> 一般可以采用如下方法获取:
> ```python
> s1 = np.mat([1,0,1,0,0,1,1])
> s2 = np.mat([0,1,0,1,1,1,0])
> len(np.nonzero(s1 - s2)[1])
> ```
> 汉明距离常用于信息编码, 一般会使编码之间的汉明距离尽可能大以增加容错率

**杰卡德相似系数**:对于两个集合 A, B 中的交集元素在 A, B 并集中占的比例为杰卡德相似系数。用于衡量两个集合的相似度。
$$J(A,B) =  \frac{|A \bigcap B|}{|A\bigcup B |}$$
**杰卡德距离**: 定义为 
$$J_{\delta} (A, B) = 1 - J(A, B) = \frac{|A \bigcup  B | + |A \bigcap B |}{|A \bigcup B |}$$
### (2) 多元统计基础
> [!NOTE] 常见的多元统计算法
> 包括朴素贝叶斯分析,回归分析,统计学习基础, 聚类分析, 主成分分析(PCA), 概率图模型等等。

最常用的贝叶斯公式表达了条件概率的关系:
$$P(B|A) =  \frac{P (A |B) P(B)}{P(A)}$$
从概率统计的角度, 一个对象可以表示为n个随机变量的整体, 其中 $X = (X_1,X_2, \dots X_n)$ 为 n 维随机变量或者随机向量。**每个对象是随机向量中的一组取值, 而矩阵中的所有对象构成了随机向量的联合和边缘密度概率分布**。
例如, 对于10个苹果, 其中红的8个, 黄的2个; 而梨有黄色和绿色, 其中黄色为9个。此时, 可以构建出如下的概率表, 对应的**联合概率密度分布**和**边缘概率密度分布**如图:
![[Excalidraw/1.机器学习算法基本内容 2024-09-05 11.09.17|650]]
所有以概率相关的算法**均以对象的联合概率密度分布和边缘概率密度分布为运算基础**。

相关系数定义参考[[📘ClassNotes/📐Mathmatics/🎣Probability Theory/第四章 随机变量的数字特征#三、协方差及其相关系数|随机变量的数字特征]], 定义为(范围为 -1 ~ 1):
$$\boxed{\rho_{XY} = \frac{E\{[X - E(X)][Y - E(Y)]\}}{\sqrt{D(X)} \sqrt{D(Y)}}}$$
而**相关距离**定义为1与该系数的差值: 
$$D_{XY} = 1 - \rho_{XY}$$
根据随机变量的数字特征, 我们可以将欧式距离发展为马氏距离公式:

**马氏距离**:
马氏距离(Mahalanobis Distance),定义为对于M个样本向量 $X_1 \sim X_{m}$, 其协方差矩阵记为$S$, 均值为 $\mu$, 则样本向量到均值 $\mu$ 的马氏距离定义为: 
$$D(x) = \sqrt{(\boldsymbol{x} - \mu)^{T} S^{-1} (\boldsymbol{x} - \mu)}$$
其中, 对于两个向量 $X_{i} ,X_{j}$, 则其马氏距离定义为:
$$\Large\boxed{D(X_{i}, X_{j}) =  \sqrt{(X_{i} - X_{j})^{T }  S^{-1} (X_{i} - X_{j})}}$$
其中当协方差为对角矩阵时, 转换为欧氏距离。其<mark style="background: transparent; color: red">优点是量纲无关, 排除变量之间的相互干扰</mark>。

计算代码如下, 比较简单:
```python
import numpy as np
import numpy.linalg as la  

def mahalanobis_distance(x1 : np.matrix, x2:np.matrix, S):  
    """  
    Calculates the Mahalanobis distance between two vectors x1 and x2 given the covariance matrix S.  
    Parameters:    x1 (numpy.ndarray): The first vector.    x2 (numpy.ndarray): The second vector.    S (numpy.ndarray): The covariance matrix.    Returns:    float: The Mahalanobis distance between x1 and x2.    """    diff = x1 - x2  
    D = np.sqrt(diff.T * la.inv(S) * diff)  
    return D  
  
if __name__ == "__main__" :  
    x1 = np.mat([1, 2, 3])  
    x2 = np.mat([4, 5, 6])  
    S  = np.mat([[1, 0, 0], [0, 1, 0], [0, 0, 1]])  
    print(mahalanobis_distance(x1.T, x2.T, S))
```

### (3) 矩阵的空间变换
空间的变换特性是矩阵最重要的特性之一, <mark style="background: transparent; color: red">由特征列的取值范围所有构成的矩阵空间应当具完整性， 并能够反事物的空间形式或者变化规律</mark>。
一般而言, 训练集所构成的矩阵常常包含大量的样本, 要求: 
1. 向量和矩阵的相关运算是具有意义的运算, 并且能够解释出事物的空间形式， 
2. 计算结果应当能够反映其本质特征, 从而映射出对象集合表达的形式和规律。
n维正交空间即为 n 个彼此正交的基地向量构成的空间

在基底进行一定的变换时, 相应的所得的坐标也会发生相应的变换, 例如在直角坐标系中的 $\alpha = (1,2)$, 计算新坐标系i'=(3,1), j'=(1,3)下的坐标, 容易得出是 i' * 1 + j' * 2 = (5,7):

则从(1,2) 到 (5,7) 的过程, 则<mark style="background: transparent; color: red">可以找到一个变换矩阵</mark>, 直接完成变换过程 :
```python
A = [[3,1], [1,3]]
i' =  A * i 
j' = A * j
```
$$\alpha' = A * \alpha = [\alpha_{1}, \alpha_{2}] * \alpha  =  (5,7)$$
其中 `A = [[3,1], [1,3]]` 为变换矩阵, 将坐标从 i, j 变换到 $[\alpha_1, \alpha_2]$ 构成的新坐标系。

需要说明的是, 向量的变换可以分解为伸缩和旋转, 类似复数运算, 设 $\alpha = [r \cos \theta, r \sin \theta]$, 则
$$  \lambda  e^{i\theta} * \alpha =  \lambda  *  \left[\begin{matrix}
\cos \Delta \theta  &  - \sin \Delta  \theta  \\ 
\sin \Delta  \theta  &  \cos \Delta  \theta  
\end{matrix}\right]  *  \left[\begin{matrix}
r \cos \theta  \\ r \sin \theta
\end{matrix}\right]$$
另外, 根据矩阵的特征向量和特征值$\lambda$, 我们可以还原初始的矩阵:
$$A = P  \Lambda P^{-1}$$
其中 P 为特征向量构成的矩阵, $\Lambda$ 为特征值排成的对角矩阵; 特征值 ($Av = \lambda v$)
```python 
import numpy as np  
import numpy.linalg as la  
  
A = np.mat([[8,1,6],[3,5,7],[4,9,2]], dtype=np.float64)  
lmbda, v = la.eig(A)  
  
Sigma = np.diag(lmbda)  
B = v * Sigma * la.inv(v)  
print(B)
```

针对数据的归一化一般的方法是通过比例缩放, 对特征进行规范化处理, 映射到某个区间。
$$X^{*} =  \frac{x - \mu}{\sigma}$$
## 二、文本挖掘方法
一般的做法是从大量文本数据中抽取事先未知的, 可理解的, 可用的知识的过程。主要包含: 
1. 搜索和信息检索
2. 文本聚类, 文本分类,文本挖掘, 信息抽取(IE), 自然语言处理(NLP), 概念提取

目前的文本分类算法主流包含: 
1. 基于模式系统
2. 基于分类模型 (机器学习, 一般需要一组预分类和训练来建立分类) 

<mark style="background: transparent; color: red">完全解决中文分词算法的是基于概率图模型的条件概率随机场(CRF)方案, </mark> 

### (1) 朴素贝叶斯方法的推导
参考 https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes (上面也给出了大量其他机器学习算法的推导)

“naive” assumption of conditional independence between every pair of features given the value of the class variable. (see https://en.wikipedia.org/wiki/Naive_Bayes_classifier) 
![[attachments/Pasted image 20240905231216.png]]

朴素贝叶斯方法假设<mark style="background: transparent; color: red">y值和x值是一一对应的, 即在一个确定的x下仅有一个y可以出现, 有</mark> 
$$\Large\boxed{P(x_{i} | y , x_{1},  x_{i-1} , \dots x_{i+1}, \dots  x_{n}) = P(x_{i}| y)}$$
同时, 将 $P (x_1, x_2, \dots  x_{n} | y)$ 以边缘概率密度乘积表示, 由于相互独立,将$P(x_i, x_i+1, \dots x_{n}| y )$采用乘积进行表示, 同时, 代入上式, 并采用 [MAP estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) 得到: 
$$P (y | x_{1}, x_{2} , \dots x_{n})  \propto P(y) \prod^{n}_{i=1} P(x_{i}|y)$$
代入上式, 有:
$$\hat{y} = \text{arg }\max_{y}P(y) \prod^{n}_{i=1}  P(x_{i}|y)$$
这个实际上类似于[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/深度学习算法基本原理/补充知识/2. MAP estimation(最大单后验可能性估计)|2. MAP estimation(最大单后验可能性估计)]], 同时, 不同朴素贝叶斯模型的区别主要是 $P(x_i|y)$ 不相同。

### (2) 高斯朴素贝叶斯方法
高斯朴素贝叶斯方法, 将似然函数均视为高斯函数, 并且变量独立同分布:
$$P(x_{i}| y) = \frac{1}{\sqrt{2 \pi  \sigma_{y}^{2}} }\exp\left(- \frac{(x_{i} - \mu )^{2}}{2 \sigma_{y}^{2}}\right)$$

> [!CAUTION] 说明
> 高斯朴素贝叶斯认为, 样本的取值是连续的, 同时服从近似的高斯分布。因此可用于 iris 数据集中。

使用方法直接采用 
```python 
from sklearn.datasets import load_iris  
from sklearn.naive_bayes import GaussianNB  
from sklearn.model_selection import train_test_split  
  
[data, target] = load_iris(return_X_y=True)  
  
X_train, X_test, y_train, y_test = train_test_split(data, target, train_size= 0.8,test_size=0.2, random_state=0)  
gnb = GaussianNB()  
predictor = gnb.fit(X_train, y_train)  
y_pred = gnb.predict(X_test)
print(y_pred, y_test)
print("failed to predict", (y_test!=y_pred).sum(), "samples")
```

**多项式朴素贝叶斯** : 针对于<mark style="background: transparent; color: red">多项式分布的数据</mark>, 针对每个参数 $\theta_{y}= (\theta_{y1}, \theta_{y2}, \dots  \theta_{yn})$, 其中特征数量为 $n$ 个,   而$\theta_{yi} = P(x_{i}|y)$，为<mark style="background: transparent; color: red">特征出现在样本中的数量或者概率</mark>。因此，<b><mark style="background: transparent; color: blue">feature 为频率时, 才能考虑采用多项式朴素贝叶斯方法</mark></b>(features are counts or frequencies, i.e.  non-negative integers) 
```python 
from sklearn.naive_bayes import MultinomialNB
``` 
参数 $\theta_y$ 通过<mark style="background: transparent; color: red">相对频率计数</mark>进行计算:
$$\Large \boxed{\hat{\theta}_{yi} = \frac{N_{yi} + \alpha}{N_{y} + \alpha n }}$$
其中, $N_{yi} = \sum_{x \in T} x_{i}$ 是特征 i 在训练集 T 中出现的次数, 而 $N_{y}= \sum^{n}_{i=1} N_{yi}$  为所有特征的总量; $\alpha$ 为平滑因子, 并满足 $\alpha \geq  0$ 以防止 $\hat{\theta}_{yi}$ 出现0的计算结果导致预测错误。当 $\alpha = 1$ 时为 Laplace 平滑, $\alpha < 1$ 为 Lidstone 平滑。

以频率为基础的训练数据示意如下:

| Class    | buy | now | free | Total Words |
| -------- | --- | --- | ---- | ----------- |
| Spam     | 20  | 5   | 10   | 35          |
| Not Spam | 5   | 15  | 5    | 25          |

<b><mark style="background: transparent; color: blue">补充朴素贝叶斯</mark></b>(Complement naive bayes,CNB) 和伯努利贝叶斯: 
调用方法是 `from sklearn.naive_bayes import ComplementNB` 和 `from sklear.n.naive_bayes import BernoulliNB` 

其中 BernoulliNB <mark style="background: transparent; color: red">认为特征是二值量, 即均为 0 或者 1</mark>,

CNB <mark style="background: transparent; color: red">仍然是以频率为样本特征 feature 的</mark>。采用每个类的**补集数据**,计算模型权重, (一般CNB的参数估计比MNB更加稳定). 对于CNB, 权重计算为:
$$\hat{\theta}_{ci} = \frac{\alpha_{i}  + \sum^{n}_{j: y_{j}\neq c} d_{ij} }{\alpha + \sum_{j: y_{j}\neq c } \sum _{k}d_{kj}}$$
