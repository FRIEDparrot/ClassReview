## 一、粒子群优化算法简介
### (1) 粒子群优化算法
粒子群优化算法(Particle Swarm Optimization), 是一种通过模拟鸟类的觅食行为发展的基于群体协作的搜索优化算法。<mark style="background: transparent; color: red">粒子群算法的核心思想是通过群体中个体的协作和信息共享寻找最优解</mark>。

粒子群的最优解**通过适应度确定**, 一般适应度即为目标函数值。

首先, 我们随机生成空间中的一组解系, 而每个粒子通过考虑:
1. 粒子本身找到的最优解
2. 群体找到的最优解
来确定下一步的转移方向。另外, 也可以使用局部极值进行，即也可以:
3. 使用粒子较为邻近的粒子中的最优解代替群体最优解

我们设输入变量共有 $D$ 维, 而在这个搜索空间中, $N$ 个粒子组成一个群落, 且每个粒子为一个 D 维的向量, 记为 : 
$$X = (x_{i1} , x_{i2}, \dots x_{iD})\qquad i = 1,2, \dots  N$$
对应地, 每一个粒子都有本身的运动速度, 且初始化是随机的。
$$V  = (v_{i1} ,v_{i2}, \dots v_{iD})\qquad i = 1,2, \dots  N$$
在运动过程中, 粒子更新速度和位置的公式如下: 
$$\Large \boxed{v_{id} = w  v_{id-1}+ c_{1} r_{1}  (p_{id} - x_{id}) + c_{2} r_{2}(p_{jd} - x_{id})}$$
其中, w为惯性部分, 而 $c1, c2$ 分别为自我认知和社会认知系数(学习因子), 而r1, r2为随机数并且范围$[0,1]$; $p_{id}$ 为个体已知的最优解, $x_{jd}$ 为群体的最优解; 每一次，粒子在运动过程中, 取
$$x_{id + 1} = x_{id} + v_{id} \cdot  t$$
其中$t$为相应时间，一般取1即可。
> [!caution] 说明
> 一般<mark style="background: transparent; color: red">粒子群算法的个体和社会学习因子可以取2左右</mark>。
> 不同的惯性权重对搜索有不同的影响，其中,惯性权重较大时，利于全局搜素，较小时利于局部搜索。

下面的代码通过绘制动态图像演示了粒子群算法
```python title:粒子群优化算法示例代码如下
#================ Particles Swarm Optimization =================================  
import numpy as np  
import scipy as sc  
import matplotlib.pyplot as plt  
  
  
def F1(x1, x2):  
    return x1 ** 2 + x2 ** 2 - x1 * x2 -10 * x1 - 4 * x2 + 60;
  
def F2(X1, X2):  
    return 3 * (1 - X1) ** 2 * np.exp(-(X1 ** 2) - (X2 + 1) ** 2) - 10 * (X1 / 5 - X1 ** 3 - X2 ** 5) * np.exp( \  
        - X1 ** 2 - X2 ** 2) - 1 / 3 ** np.exp(-(X1 + 1) ** 2 - X2 ** 2)  
  
# used for show animation  
# fig = plt.figure() ; plt.ion()  
def plot_figure(fig, x_range, y_range, F, x, y):  
    ax = fig.add_subplot(1,1,1,projection='3d')  
    xx = np.arange(x_range[0], x_range[1], 0.1);  
    yy = np.arange(y_range[0], y_range[1], 0.1);  
    [X, Y] = np.meshgrid(xx, yy);  
    Z = F(X,Y)  
    ax.plot_surface(X,Y,Z, alpha = 0.8, cmap = 'rainbow')  
    ax.scatter(x, y, F(x, y), color='black', marker='^')  
    ax.view_init(45, 135 + 180)  
    plt.show(block = False)  
    plt.pause(1)  
    fig.clear()  
  
# 如果改成F2, 则将两个x上下限改为[-3, 3], v上下限改为[-1, 1]效果好  
def F(X1, X2):  
    return F1(X1, X2)  
  
  
if __name__ == "__main__":  
    fig = plt.figure()  
    plt.ion()   # don't stop when plt.show()  
    # ---------------- main function -----------------    x1_range = [-15, 15]; x2_range = [-15, 15]; v_range = [-5, 5];  
    num_PSO = 200  
    num_epoch = 50  
  
    n = 2  
    w = 0.9         # inertia weight  
    c1 = 2; c2 = 2   # learning factor  
  
    # random location sample    xp = np.random.rand(num_PSO,n) * [x1_range[1] - x1_range[0], x2_range[1] - x2_range[0]] + [x1_range[0], x2_range[0]]  
    vp = np.random.rand(num_PSO,n) * [v_range[1] - v_range[0]] + v_range[0]  # [-1, 1] in axis  
    # get the random velocity sample    best_xp = np.zeros([num_PSO,n]); best_yp = 1e5 * np.ones([num_PSO, 1])  # record the self   minimum value coordinate  
    best_glx = np.zeros(n);     best_gly  = 1e5;          # record the global minimum value coordinate  
  
    for epoch in range (num_epoch):  
        yp = F(xp[:,0], xp[:,1])   # calculate the function value of  
        plot_figure(fig, x1_range, x2_range, F, xp[:, 0], xp[:, 1])  
  
        for i in range(num_PSO):  
            if yp[i] < best_yp[i]:  # seld minimum value  
                best_yp[i] = yp[i]  
                best_xp[i,:] = xp[i,:]  
  
  
        # record the global minimum value of y ** caution --> use best array for each for judge  
        if np.min(best_yp, axis= 0) < best_gly:  
            best_gly = np.min(best_yp, axis=0)  
            best_glx = best_xp[np.argmin(best_yp),:]  
  
        # vp[i, :] = w * vp + c1 * r1 * (best_xp - xp) + c2 * r2 * (best_glx - xp)  
        # move the points to the next place  
        for i in range(num_PSO):  
            r1 = np.random.rand();  r2 = np.random.rand();  
            vp[i, :] = w * vp[i, :] + c1 * r1 * (best_xp[i, :] - xp[i, :]) + c2 * r2 * (best_glx - xp[i, :])  
  
            vp[i, :] = np.max(np.vstack((np.array([v_range[0], v_range[0]]), vp[i, :])), axis=0)  
            vp[i, :] = np.min(np.vstack((np.array([v_range[1], v_range[1]]), vp[i, :])), axis=0)  
  
            xp[i, :] = xp[i, :] + vp[i, :]  
  
            xp[i,:] = np.max(np.vstack((np.array([x1_range[0], x2_range[0]]), xp[i,:])),axis=0)  
            xp[i,:] = np.min(np.vstack((np.array([x1_range[1], x2_range[1]]), xp[i,:])),axis=0)  
  
            # use hstack or vstack for extend  
        print("epoch:", epoch, "minimum value:", best_gly, "this time best:", np.min(yp));
```

### (2) 粒子群算法的改进方案
#### 1. 线性递减惯性权重
对于在搜索过程中，可以取:
$$w^{d} = w_{start} - (w_{start} - w_{end}) \times \frac{d}{K}$$
其中$K$为迭代总次数, 而$w_{start}$一般取0.9, $w_{end}$ 一般取0.4

#### 2. 自适应惯性权重方法
取惯性权重**和迭代次数与每个粒子的适应度有关**:

假设求解最小值问题, 则可以取: 
$$\Large w_{i}^{d} = \begin{cases}
w_{\min } + (w_{\max} - w_{\min})  \frac{f(x_{i})^{d} - f_{\min}^{d}}{f_{average}^{d}  - f_{min}^{d}}\qquad  f(x_{i}^{d}) \leq f_{average}^{d} \\
w_{\max} \qquad \qquad \qquad \qquad  \qquad \qquad \qquad f(x_{i}^{d}) \geq  f_{average}^{d}
\end{cases}$$
即当求解出的目标函数值较小时, 可以尽可能将权重选取地较小, 而目标函数值较大时， 选择大的权重值。其中$f^d_{\min}$表示第 $d$ 次迭代的最小适应度。类似地，最大值可以使用$f_{max} - f(x_{i})$等代替上面的部分，不详细叙述。 

#### 3. 随机惯性权重法
$$\omega = \mu_{\min}+(\mu_{\max} - \mu_{\min}) \times  rand()  + \sigma \times  randn()$$
其中$\mu_{\min}$为随机 迭代惯性权重的最小值。 **避免了迭代前期的局部搜索能力不足的同时避免后期全局搜索能力的不足**。
最终使得权重 $\omega$ 有利于向期望权重方向进化。

#### 4. 压缩因子方法
应用较多的个体学习因子$c1$, 群体学习因子c2均取2.05; 
$$C  = C_{1} + C_{2} = 4.1 \qquad  \Phi  = \frac{2}{(2 - C - \sqrt{C^{2} - 4 C})}$$
权重仍然相同取0.9, 而速度更新公式写为:
$$v_{i}^{d} =  \Phi \times [w v_{i}^{d-1} + c_{1}r_{1} (pbest_{i}^{d} - x_{i}^{d})] + c_{2}r_{2} (gbest ^{d} - x_{i}^{d})$$
#### 5. 非对称学习因子方法
![[attachments/Pasted image 20240422175744.png|500]]
