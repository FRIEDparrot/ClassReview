## 一、决策树算法
决策树最早起源于 CLS(Concept Learning System) 学习系统, 而 ID3 算法原型在1979年才被J.R.Quinlan提出。并通过简化 ID3算法, 确立了决策树学习理论。引入节点缓冲区将ID3发展为ID4算法, 最后发展改进为 C4.5算法, 成为机器学习的十大数据挖掘算法之一。

另外一个重要的机器学习算法, <mark style="background: transparent; color: red">也是ID3的另外一个分支, 称为分类决策树算法</mark>(Classification and Regression Tree, CART算法), 其中与 C4.5 不同的是, CART决策树算法主要用于预测。
> [!NOTE] 补充: 十大数据挖掘算法
> ![[Pasted image 20221115195839.png|400]]

### (1) 决策树及其算法框架
决策树是一种基于 if-then 结构形成的分类学习方法,  以某笔记本电脑的客户的分类和销售收入调查表为例:

| 计数  | 年龄  | 收入  | 学生  | 信誉  | 是否购买 |
| --- | --- | --- | --- | --- | ---- |
| 64  | 青   | 高   | 否   | 良   | 不买   |
| 64  | 青   | 高   | 否   | 优   | 不买   |
| 128 | 中   | 高   | 否   | 良   | 买    |
| 60  | 老   | 中   | 否   | 良   | 买    |
| 64  | 老   | 低   | 是   | 良   | 买    |
| 64  | 老   | 低   | 是   | 优   | 不买   |
| 64  | 中   | 低   | 是   | 优   | 买    |
| 128 | 青   | 中   | 否   | 良   | 不买   |
| 64  | 青   | 低   | 是   | 良   | 买    |
| 132 | 老   | 中   | 是   | 良   | 买    |
| 64  | 青   | 中   | 是   | 优   | 买    |
| 32  | 中   | 中   | 否   | 优   | 买    |
| 32  | 中   | 高   | 是   | 良   | 买    |
| 64  | 老   | 中   | 否   | 优   | 不买   |


首先, 决策树部分主要分为根节点, 叶子节点和内部节点, <mark style="background: transparent; color: red">当按照某种条件进行划分时, 如果划分到子集为空,或者子集中样本已经归位同一个类别的标签, 则该子集成为叶子节点,  否则对应于内部节点， 对于内部节点均需要选择一个新的类别标签对该子集继续进行划分</mark>,直到全部为叶子节点。

例如按照年龄, 容易发现中年均是购买该产品的。以此类推可以构建如下的决策树:
![[Excalidraw/2. 决策树和回归树 2024-09-11 18.13.37|650]]
其中, 可以通过数据除以总个数 (1024) <mark style="background: transparent; color: red">获取到每个节点处的购买概率。</mark> 

决策树中, 主函数本质上是通过递归函数, 按照规则生长出决策树的各个分支节点, 其中<b><mark style="background: transparent; color: blue">决策树的不同取决于最优特征选择的标准上有差异。</mark></b> 其中 <mark style="background: transparent; color: red">ID3 的最优特征选择是信息增益, C4.5 是信息增益率, CART 是节点方差大小</mark>。
一般地, <b><mark style="background: transparent; color: blue">选择最优特征， 需要遍历整个数据集并评估特征， 找到最优的特征进行返回</mark></b>。

其中首先需输入分类的数据集和类别标签, 而关键是<b><mark style="background: transparent; color: blue">根据某种分类规则找到最优的划分特征， 创建特征的划分节点, 即计算最优的特征子函数</mark></b>, 实现数据集的划分。

决策树的**分类器**是<mark style="background: transparent; color: red">通过遍历整个决策树， 使测试数据找到决策树中的叶子节点对应的类别标签。 则这个标签即为返回的结果。</mark>

### (2) 信息熵测度
数据集的划分是将数据从无需变为有序的过程, 并采用熵(entropy)衡量数据无需的程度。 设不确定性函数I为事件的信息量,且为事件 $U$ 发生概率 $p$ 的单减函数, 并且满足 $I(p_1, p_2) = I(p_1) + I(p_2)$ 
$$I(U) = \log_{} \left(\frac{1}{p}\right) =- \log_{} p  \qquad  p = p (y|x)$$
对于一个信号源, 不能仅考虑某单一时间发生的不确定性, 而需要考虑所有可能情况的平均不确定性。

当信源事件有 $n$ 种取值 $U_1, \dots U_i,\dots ,U_n$ 且相互独立时,  记其对应概率为$p_i$, 则信源的平均不确定性应当计算为:
$$H(U) =   E[- \log_{} p_{i}] = - \sum^{n}_{i=1}  p_{i} \log_{} p_{i}$$
需要说明的是, 参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/⚓Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#(5) entropy concept of Information theory|entropy concept of Information theory]], 取 1 "nat" $=  \frac{1}{\ln 2} \approx 1.44bit$ , 编码至少需要 $H[p]$ "nat" 进行编码。 <b><mark style="background: transparent; color: blue">若上述函数中一般取 2 为底</mark></b>, 则成为一般的 bit 单位。
信息熵是不确定性的度量标准, 用于度量类别的不确定性。<mark style="background: transparent; color: red">某个特征列向量的信息熵越大， 说明该向量的不确定性程度越大， 应当从该特征向量进行着手划分。 </mark>

#### 1. 整个数据集的信息熵
第一, <mark style="background: transparent; color: red">采用信息熵度量标签对样本整体的不确定性</mark>。设S是s个数据样本的集合，在一个大分类(是否购买)下共有 m 个小类 $C_i$ (该类别标签具有m个不同的值, 即买或者不买, m = 2), 此时可取:
$$\Large p_{i} = \frac{s_{i}}{|S|}$$
则 $p_i$ 是任意样本的值为 $C_i$ 的概率。则对于给定的样本, <mark style="background: transparent; color: red">分类的信息熵为某个属于各个标签不同值的乘积</mark>: 
$$\Large\boxed{I(s_{1}, s_{2}, \dots s_{m}) = - \sum^{m}_{i=1} p_{i} \log_{2} (p_{i})}$$
考虑最简单的情况, 某个标签仅有1个值, 显然$p_{i} = 1$。则熵为0(不需要利用其他标签进行分类), <b><mark style="background: transparent; color: blue">这个称为需要分类的总体熵, 即总体不确定性。</mark></b> 

#### 2. 节点信息熵期望的计算
<mark style="background: transparent; color: red">采用信息熵度量类别标签对各个节点的不确定性, 假设对于特征 A(类似上图中的年龄), 有 v 个不同的值, 并可以通过 A 将 S 划分为 v 个子集(S_j), 其中 S 的子集 S_j 在特征 A 上具有值 a_j</mark>. 此时**如果选择 A 作为测试特征**(即用于分类C的特征)。此时, 设最终需要分的类为 $C_i$, 设 $s_{ij}$ 是子集 $S_j$ 中的类 $C_i$ 中的样本数。则<b><mark style="background: transparent; color: blue">利用 A 划分成的子集的熵的期望</mark></b>给出为(s为样本总数):
$$E(A) = \sum^{v}_{j=1}  \frac{s_{1j} + s_{2j} + \dots + s_{mj}}{s} I (s_{1j}, s_{2j},\dots s_{mj})$$
对于权重部分, 对于C各个分类不重合的情况实际上等于$a_{j}$中的样本总数除总的样本数:
$$\text{weight}  =  \frac{s_{1j} + s_{2j} + \dots + s_{mj}}{s}= \frac{|S_{j}|}{s}$$
<b><mark style="background: transparent; color: blue">其中需要注意的是,</mark></b> 单独子集各自的熵, 仅考虑某个子集 $S_{j}$ 样本中的成功划分为类i的数量:
$$I  (s_{1j}, s_{2j},\dots s_{mj}) =  -\sum^{n}_{i=1} p_{ij}\log_{2} (p_{ij}) \qquad  p_{ij} = \frac{s_{ij}}{|S_{j}|}$$
<mark style="background: transparent; color: red">前面一部分为第j个子集的权</mark>,  当子集划分纯度高时, 假设 A 可以完全划分分类 C, 则显然I=0, 同时保证了当划分更加均匀时, 所得的划分期望仍然是较小的。**平均节点熵最小的可以作为根节点使用**。

#### 3. 信息增益
最后, **使用信息增益来确定决策树的划分依据**。<mark style="background: transparent; color: red">信息增益定义为在决策树某个分支上整个数据集的信息熵与当前节点信息熵的差值</mark>。
$$\text{Gain}(A)  =  I (s_{1}, s_{2}\dots  s_{m}) -  E(A) $$
信息增益表示了<b><mark style="background: transparent; color: blue">得知属性 A 之后, 导致的熵值的压缩期望</mark></b>,  <mark style="background: transparent; color: red">此时我们可以将最高信息增益的特征作为 S 的测试属性， 即创建以此为分类依据的节点</mark>。这个就是 ID3 决策树的生成过程。

对于上述问题, 买的共 640, 不买 384 人, 因此得到总的信息熵:
$$I = I(640,384) = -p1*log2(p1) - p2*log2(p2) = 0.9544$$

### (3) ID3 决策树算法编程实例
例如年龄的信息熵为: 设老年为0, 买为1 
首先计算权重: 
$$p1 = \frac{384}{1024}=0.375\qquad  p2 = \frac{256}{1024} = 0.25,\qquad  p3 = 0.375 $$
$$p11 = \frac{128}{384} = \frac{1}{3} , p21 = \frac{2}{3}  \qquad  I_{1}=-p11*log2(p11)-p21*log2(p21)= 0.9189$$
$$p12 = 1, p22 = 0, I_{2} = 0$$
$$p13 = \frac{128}{384} = \frac{1}{3},p23=\frac{256}{384} = \frac{2}{3}\qquad  I_{3}=0.9189$$
则平均节点熵为:
$$0.375*0.9189 + 0.375 *0.9189 = 0.6891$$
信息增益为:
$$\text{Gain} = 0.9544 - 0.6891 = 0.2653$$
只需以此类推，计算出所有特征的增益, 取其中最大的作为根节点进行分类即可。
具体决策树部分参考[sklearn 决策树部分](https://scikit-learn.org/stable/modules/tree.html)

Iris, Wine 和 Breast Cancer 均可用于决策树分类或者回归算法。

ID3 算法的实现: 首先将图片等转为 excel 表格,再采用 pandas 读入:
```python 
import pandas as pd  
from pytesseract import image_to_data, image_to_string  
from PIL import Image  
  
data = pd.read_excel("seals_data.xlsx")  
print(data)
```
结果如图:
![[attachments/Pasted image 20240912105614.png]]
转化为决策树编码: 
![[attachments/Pasted image 20240912113417.png]]
此时, 0 为不购买, 1 为购买, 则按照年龄分组, 采用如下代码:
```python
def calc_entropy_mat(self,counts, decision_arr, target_arr):  
    """  
    calculate the entropy matrix    
    :param counts: the counts array of the current node    
    :param decision_arr: the decision attributes of the current node    
    :param target_arr: target array to be classified of the current node    
    :return:  
    """  
    decisions = np.unique(decision_arr)  
    targets   = np.unique(target_arr)  
  
    cnt_mat = np.zeros((targets.size, decisions.size)) # number of samples of each class for each decision attribute  
    for c in range(targets.size):  
        for d in range(decisions.size):  
            tar = targets[c]  
            dec = decisions[d]  
            cnt_mat[c,d] = np.sum(counts[(decision_arr == dec) & (target_arr == tar)])
```
计算出对应的概率矩阵如下:
![[attachments/Pasted image 20240912155654.png]]
我们采用如下的结构格式存储相应的矩阵:
由于每次节点部分分类实际上是找子集的过程。我们<mark style="background: transparent; color: red">把每个用于分类的特征称为一个decision, 而一个decision下的不同情况称为 feature </mark>, 最终需要归类的特征称为 target。
每次我们只需要得到 data  矩阵,  
![[Excalidraw/2. 决策树和回归树 2024-09-12 23.41.10|900]]
寻找下一个树建立所需的节点时, 

每一次添加节点, 只需要从 features 中找出不为0的继续添加即可。最后一个节点是 {target(买或不买) : number } 形式; 当 cnt_arr 仅有一个非零项时. 

其二, 我们是应当以 decision 作为节点的, 而不是以 feature 作为节点。同时最后的叶子节点同时记录。最终建立的树结构如下: (图中列出了三种典型的结构, 由于需要得知结果及对应的数量，所以需要建立一个节点进行记录)
![[Excalidraw/2. 决策树和回归树 2024-09-13 00.25.59|650]]

此时我可以编写如下程序运行决策树:
```python
import numpy  
import numpy as np  
import copy  
import pandas as pd  
from sklearn.preprocessing import LabelEncoder  # encoder labels  
from treelib import Tree, Node  
from sklearn.datasets._base import Bunch  
from dict_to_map import dict2map  
  
class ID3_Tree():  
    """ ID3 decision Tree Algorithm """  
    def __init__(self, counts = None, data = None, target = None, label_encoder = None):  
        if (counts == None or data == None or target == None):  
            self.load_data()  
        else:            self.train_data = Bunch(counts = counts, data = data, target = target)  
            self.label_encoder = label_encoder()  
  
        self.__Init_Tree(self.train_data.counts, self.train_data.data, self.train_data.target)  
        self.build_Tree()  
  
    def to_dict(self, *args, **kwargs):  
        return self.tree  
  
    def load_data (self):  
        """ arrange the data into the correct shapes """  
        data_raw = pd.DataFrame(pd.read_excel("seals_data.xlsx"))  
        label_encoder = LabelEncoder()  
        # eliminate all the white space  
        data_raw = data_raw.map(func=lambda x:x.strip() if isinstance(x, str) else x)  
        data_proceed = pd.DataFrame()  
        data_proceed['计数'] = data_raw.iloc[:, 0]  
        for column in data_raw.columns[1:]:  
            data_proceed[str(column).strip()] = label_encoder.fit_transform(data_raw[column])  
  
        """ split data into 3 part : counts, data and target """  
        counts = np.array(copy.deepcopy(data_proceed.iloc[:,0]))  
        data = np.array(copy.deepcopy(data_proceed.iloc[:,1:-1]))  
        target = np.array(copy.deepcopy(data_proceed.iloc[:,-1]))  
        labels = [str(column).strip() for column in data_raw.columns]  
        self.data_raw = data_raw  
        self.train_data = Bunch(counts = counts, data = data, target = target ,labels = labels) # target = data_raw.iloc[:, 0])  
        self.label_encoder = label_encoder  
  
    def __Init_Tree(self, counts:np.ndarray, data:np.ndarray, target:np.ndarray):  
        self.__check_param(counts, data, target)  
        self.nums   = counts.shape[0]              # number of the type of the samples  
        self.target_num = len(np.unique(target))   # the number of classes (C_i) , i = 1... m  
        self.decision_num = data.shape[1]          # number of decision attributes (D)  
        self.total_num = counts.sum()              # total number of samples (N)  
        self.tree = {}                             # init the tree node  
  
        # calculate the infomation entropy of the entire dataset        targets =  np.unique(target)  
        cls_cnt =  np.array([counts[target == targets[i]].sum() for i in range(targets.size)])  
        cls_prop=  cls_cnt/cls_cnt.sum()  
        self.base_entropy = -np.sum(cls_prop * np.log2(np.where(cls_prop == 0,1e-10, cls_prop)))  
  
    def build_Tree(self):  
        if (self.target_num <= 1):  
            # only 1 class, stop split and return the empty tree  
            self.tree["root"] = self.train_data.target[0]  
            return self.tree  
  
        # initialize the node decision range and target range, we use whole data set to calculate the entropy of root at first  
        dec_range = np.arange(self.decision_num)   # decision attributes  
        tar_range = np.arange(self.nums)           # targets on data  
            # recursive call the calc_entropy_mat function until the class is purely classified.  
        self.tree = self.__build_tree_node(dec_range, tar_range)  
  
    def __check_param(self, counts, data, target):  
        if len(counts.shape)!=1 or len(data.shape)!=2 or len(target.shape)!=1:  
            raise ValueError("The input data is not in the correct shape")  
        elif counts.shape[0]!= data.shape[0] or counts.shape[0] != target.shape[0]:  
            raise ValueError("The input data is not in the correct shape")  
  
    def show(self):  
        map = dict2map(self.tree)  
        tree = Tree.from_map(map)  
        tree.show(line_type="ascii-em", sorting=False)
  
    def __build_tree_node(self, dec_range, tar_range) -> dict:  
        """  
        recursive function,        counts, data, target, node_dec_range, node_tar_range        :param dec_range : decision range (in direction 1 or y)        :param tar_range : target range (in direction 0 or x)        :return: root (name of the root node is defined by decision)  
        """  
        counts = self.train_data.counts[tar_range]  
        data =   self.train_data.data[tar_range][:, dec_range]  
        target = self.train_data.target[tar_range]  
        self.__check_param(counts, data, target)  
  
        gain_list = [self.__get_node_info_gain(counts, data[:, i], target) for i in range(dec_range.size)]  
        best_dec_idx = np.argmax(gain_list)  # best decision (note : relevant to dec_range, not self.train_data)  
  
        best_decision = data[:, best_dec_idx]  
        features = np.unique(best_decision)  
        cnt_mat = self.__get_count_matrix(counts, best_decision, target)  
  
        # use best decision as root node -> delete it from dec_range  
        root_name = "decision" + str(dec_range[best_dec_idx])   # get the location of best decision  
        root = {}  
  
        d2 = numpy.delete(dec_range, best_dec_idx)  # create new dec_range object  
        for i in range(len(features)):  
            feature = features[i]  
            cnt_arr = np.array(cnt_mat[:, i]).squeeze(1)   # change to array and squeeeze to 1 dim  
  
            # record the classification result:            left_tar_idx = np.nonzero(cnt_arr)[0]          # choice leave in this feature  
  
            if (len(left_tar_idx) == 0):  
                raise ValueError("left choices is not zero here!")  
  
            if left_tar_idx.size == 1:  
                # left number can be calculated by cnt_arr[left_choices], t2 = []  
                root["target" + str(left_tar_idx[0]) ] = np.sum(cnt_arr[left_tar_idx])  
            elif d2.size == 0 :  # no available decision left  
                root["dummy"] = np.sum(cnt_arr[left_tar_idx]) # create dummy node  
            else:  
                t2 = tar_range[best_decision == feature]  
                sub_tree = self.__build_tree_node(d2, t2);  
                sub_rootname, sub_root = next(iter(sub_tree.items()))  
                root[sub_rootname] = sub_root  
        return {root_name : root}  
  
    def __get_node_info_gain(self,counts_arr, decision_arr, target_arr):  
        """  
        :param counts_arr: the counts array of the current node        :param decision_arr: the decision attributes of the current node        :param target_arr: target array to be classified of the current node        :return: gain : scalar, infomation gain of the current node  
        """        cnt_mat = self.__get_count_matrix(counts_arr, decision_arr, target_arr)  
        # calculate the information gain of the current node by cnt_mat  
        prob_mat = np.mat(cnt_mat / np.sum(cnt_mat, axis=0))  # calculate probability matrix  
        prob_mat = np.where(prob_mat == 0, 1e-10, prob_mat)   # substitute 0 with 1e-10 to avoid log calculation error  
        node_entropy = -np.sum(np.multiply(prob_mat, np.log2(prob_mat)), axis=0)  
        node_wt = cnt_mat.sum(axis=0) / np.sum(cnt_mat)  
        gain = self.base_entropy - np.multiply(node_wt, node_entropy).sum()  
        return gain  
    def __get_count_matrix(self, counts_arr, decision_arr, target_arr):  
        """  
        calculate the count matrix of the node        :param counts_arr:   the counts array of the current node        :param decision_arr: the decision attributes of the current node        :param target_arr:   target array to be classified of the current node        :return: cnt_mat : np.matrix  
        """        features = np.unique(decision_arr)  
        targets = np.unique(target_arr)  
        cnt_mat = np.array([  
            [np.sum(counts_arr[(decision_arr == dec) & (target_arr == tar)]) for dec in features]  
            for tar in targets  
        ])  
        return np.matrix(cnt_mat)  
  
  
if __name__ == "__main__":  
    tree = ID3_Tree()  
    print(tree.to_dict())  
    tree.show()
```

其中 dict2map  函数参考[[📘ClassNotes/⌨️Programming/🐍Python/🌟Python 基础部分/2. Python 基本数据结构和可视化方法#(1) Python 中的树结构|2. Python 基本数据结构和可视化方法]], 可以输出如下的树结构:
```python
{'decision0': {'target1': 256, 'decision3': {'target0': 128, 'target1': 256}, 'decision2': {'target0': 256, 'target1': 128}}}
decision0
╠══ decision0-target1 : 256
╠══ decision0-decision3
║   ╠══ decision0-decision3-target0 : 128
║   ╚══ decision0-decision3-target1 : 256
╚══ decision0-decision2
    ╠══ decision0-decision2-target0 : 256
    ╚══ decision0-decision2-target1 : 128
```
注意应当设置 Tree 的 sorting = False 来保证输出树是相应的决策部分, 不更改feature相对顺序。

对应的，决策树分类的代码也相对简单, 只需要按照相应的类进行分类即可;

### (4) C4.5 算法
对于 C4.5 算法, <b><mark style="background: transparent; color: blue">基本的程序结构与 ID3 相同, 而节点的划分标准上做了改进</mark></b>。即采用信息增益率代替信息增益。
$$\text{GainRatio}(S,A) = \frac{\text{Gain}(S, A)}{\text{SplitInfo}(S,A)}$$
其中, SplitInfo(S,A) 代表了按照特征 A 划分S的广度和均匀性。计算为:
$$\text{SplitInfo(S,A)} =  - \sum^{c}_{i=1} \frac{|s_{i}|}{|S|} \log_{2} \left(\frac{|S|}{|s_{i}|}\right)$$
其中 s_j 为 A 的 C 个不同的值构成的样本子集。

对于上述问题, 采用 C4.5算法获取其决策树, 需要 `pip install c45-decision-tree` 获取 c4.5 的决策树库。
另外，为了修改中文乱码问题, 进入C45Classifier函数源码并添加 `dot.attr(encoding='utf-8')  # Ensure UTF-8 encoding` 和 `fontname="SimHei"` 部分: 
```python
def generate_tree_diagram(self, graphviz, filename):  
    # Menghasilkan diagram pohon keputusan menggunakan modul graphviz  
    dot = graphviz.Digraph()  
    def build_tree(node, parent_node=None, edge_label=None):  
        if isinstance(node, _DecisionNode):  
            current_node_label = str(node.attribute)  
  
            dot.node(str(id(node)), label=current_node_label)  
            if parent_node:  
                dot.edge(str(id(parent_node)), str(id(node)), label=edge_label, fontname="SimHei")  
  
            for value, child_node in node.children.items():  
                build_tree(child_node, node, value)  
        elif isinstance(node, _LeafNode):  
            current_node_label = f"Class: {node.label}, Weight: {node.weight}"  
            dot.node(str(id(node)), label=current_node_label, shape="box", fontname="SimHei")  
  
            if parent_node:  
                dot.edge(str(id(parent_node)), str(id(node)), label=edge_label, fontname="SimHei")  
  
    build_tree(self.tree)  
    dot.format = 'png'  
    dot.attr(encoding='utf-8')  # Ensure UTF-8 encoding  
    return dot.render(filename, view=False)
```

然后只需要如下代码就可以显示决策树了: 
```python
from C45 import C45Classifier  
import pandas as pd  
import graphviz  
  
data_raw = pd.DataFrame(pd.read_excel("scripts/seals_data.xlsx"))  
data_raw = data_raw.map(func=lambda x:x.strip() if isinstance(x, str) else x)  
  
counts = data_raw.iloc[:,0]  
data   = data_raw.iloc[:,1:-1]  
target = data_raw.iloc[:,-1]  
  
data_new = []  
target_new = []  
for i in range(counts.size):  
    for j in range(counts[i]):  
        data_new.append(list(data.iloc[i]))  
        target_new.append(target.iloc[i])  
  
model = C45Classifier()  
model.fit(data_new, target_new)  
tree_diagram = model.generate_tree_diagram(graphviz, "tree_diagram")  
graphviz.view(tree_diagram)
print(model.predict([['老', '低', '是', '良']])) # 买
```

![[attachments/tree_diagram.png]]


此外, sklearn 中给出了决策树分类和回归模块, 但是一般用于连续数据: 
```python
from sklearn.tree import DecisionTreeClassifier 
from sklearn.tree import DecisionTreeRegressor 
```

### (5) CART 回归树算法 
CART 是一种通过决策树方法实现回归的算法, 可以用于分类和预测。
首先, 创建回归模型时, 样本取值包括观察值和输出值。 且观察值和输出值均为连续的。<b><mark style="background: transparent; color: blue">CART 采用了最小剩余方差办法判定回归树的最优划分</mark></b>(Squared Residuals Minimization)。**将数据集进行切分之后, 通过线性回归技术进行建模**。
基本思想是, **决策树将数据集分成子模型数据， 并利用线性回归技术来进行建模， 如果切分后的子集难以拟合， 则继续切分。该线性回归模型也被称为模型树**。

CART 算法的主要流程:
1. 决策树主函数: 主函数是递归函数, 且功能是根据 CART 规则生长出各个节点, 并根据终止条件结束算法。
	1. 通过最小剩余方差确定最佳划分, 并创建对应的**划分节点**(最小方差子函数)。
	2. 在划分节点处, 将数据集进行二分 
	3. 根据二分结果构建新的左右节点, 作为新的两个分支
	4. 检验递归终止条件, 下一次节点输入变为其包含的相应数据集和类标签。
2. 采用最小剩余方差函数计算数据的最优划分方差， 划分列和划分值。
3. 二分数据集
4. 剪枝策略  


划分原理是, <mark style="background: transparent; color: red">每次划分取某个 feature 列所有的大于 value 的样本行</mark>。在**每个特征feature上**进行二分数据集。然后计算划分之后的总方差(方差和)， 并**和划分之前进行比较**。最终确定最优划分。

#### 1. 模型树
CART 预测实际上是利用分段的线性函数作为叶子节点去拟合原始数据。而其中的<mark style="background: transparent; color: red">每个线性函数都被称为一颗模型树</mark>。

- 一般样本总体的重复不很高, 但是**局部模式一般会经常重复** 
- 模型给出了数据的范围。 
- 传统的回归方法包含的信息都不如模型树丰富。从而模型树的准确度更高。

#### 2. 剪枝方法
一般地, 为了<mark style="background: transparent; color: red">避免过拟合</mark>, 预测树采用了剪枝的方法。剪枝包括<mark style="background: transparent; color: red"></mark><b><mark style="background: transparent; color: blue">先剪枝和后剪枝</mark></b>，先剪枝一般方法是先给定一个预定义的划分阈值, 而当子集划分低于这个阈值之后，子集划分终止。另外也包含预剪枝部分; 预剪枝不会生成整个决策树， 且算法简单, 适合大估摸问题的粗略估计。

后剪枝是在完全生成的决策树中, 去掉不具有代表性的子树并使用叶节点代替。<mark style="background: transparent; color: red">一般方法是递归估算内部节点的误判率，低于某个值时, 可将其设置为叶子节点</mark>。
对于某个叶子节点,  覆盖 $N$ 个样本, 假设其中有 $E$ 个错误。则取错误率计算为:
$$\frac{E + 0.5}{N}$$
其中 0.5 为惩罚因子。考虑某子树, 错误分类样本记为1, 正确分类样本记为0，同时假设树的**固有误判率**为$e$, 则可以根据[[📘ClassNotes/📐Mathmatics/🎣Probability Theory/重要定理/常用的概率分布及数学期望和方差|常用的概率分布及数学期望和方差]](二项分布)估计出均值和标准差: 
$$E = e \times  N \qquad  \text{var} = \sigma  = \sqrt{N e(1 - e)}$$
对应的叶子结点, 误判率为: 
$$e = \frac{E + 0.5}{N}\qquad  E(\text{leaf err cnt}) = N \times e $$
剪枝标准取为:
$$E(\text{sub tree err})  -   var(\text{sub tree err} )  > E (\text{leaf err cnt})$$
一般由于CART是二叉树，所以一般只需要采用 .left 和 .right 来存储对应的节点就行了。整体类似于二叉树。只需要计算方差。(实际上只需要利用一个Set子集分别计算每个节点的方差即可), 当满足条件时, 则合并(使用左右子树均值代替子树)

> [!NOTE] 说明
> 在最开始的树中, 仅有叶子节点有意义， 所以我们可以任意采用中间节点进行合并叶子节点。这就是剪枝。剪枝之后中间节点的值(用于分割)成为两个叶子结点值的均值。

#### 3. CART 回归树的建立 
```python title:
import sklearn  
from sklearn import tree  
import numpy as np  
import matplotlib.pyplot as plt  
  
x = np.linspace(0, 10, 100)  
y_raw  = np.sin(x)  
y_rand = np.random.rand(100)  
y = y_raw + y_rand * 0.5  
  
fig, ax = plt.subplots()  
clf = tree.DecisionTreeRegressor(max_depth=4) # max layer of the regression tree  
clf.fit(x.reshape(-1, 1), y_raw.reshape(-1, 1))  
y_pred = clf.predict(x.reshape(-1, 1))  
  
plt.scatter(x, y, c='b', marker='.')  
plt.plot(x, y_pred, c='r', linewidth=2)  
plt.show()
```

最简单的代码如上所示, 其中 max_depth 越深, 拟合越精确。
![[attachments/Pasted image 20240914000254.png]]


