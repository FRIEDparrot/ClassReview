## 一、基于 AdaBoost 的人脸检测
### (1) Opencv 自带库人脸检测原理
opencv 的人脸识别中, **最常用的是 Haar 特征和  LBP 特征**,  对于  Haar 特征检测部分,  实际上**需要定义相应的矩阵特征, 包含边界线性特征等等**,  AdaBoost 是人脸检测的主流算法, 每一次快速排除不属于对应物体的区域, 保留其中不确定区域,  

Haair 特征是将人脸fenwei   Edge Features , Line Features, Center-Surround Features 几种,   **如果采用 Haar 特征检测人脸, 则需要采用 Haar 特征级联表作为训练集**, 实际上是 xml 文件;  而 LBP 特征也是基于特征级联表进行训练的; 

Theano 已经不再维护，也不支持 Python 的最新版本，包括 Python 3.12 等等, 仅支持 Python 3.7 或更低的版本

### (2) AdaBoost 算法说明
AdaBoost (Adaptive Boosting)算法是从[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓深度学习算法原理(sklearn)/2. 决策树和回归树|决策树]]发展出的一种算法,  实际上是针对同一训练集样本的不同特征训练处不同的弱分类器,  并级联得到实际分类器,  即集成学习算法。

主要流程包含: 
1. 初始化权重  
2. 训练弱学习器 
3. 更新样本权重和构建强学习器 
![[Excalidraw/7. 模式识别和人脸检测 2024-11-29 14.48.48|300]]
AdaBoost **关注之前被错误分类的点, 以修正模型的缺陷**.  而每一轮都会训练一个新的弱学习器。

算法流程:  
首先给定一个训练数据集 \{ (x_1 , y_1), \dots (x_n, y_n) \} 其中$y_i$ 属于  (-1,1) 

样本权重更新: 
$$w^{t + 1} = w_{i}^{(t)} \times \exp  (- \alpha  \times  y_{i} \times   h_{t}(x_{i}))$$
其中 $w_i^{(t)}$ 为 t 轮中样本权重, $\alpha$  为弱学习器权重,  $y_i$ 为真实标签。
<mark style="background: transparent; color: red">最终的模型是所有弱学习器的加权求和结果</mark>, 单层决策树以最小误差为衡量找到最优列, 
$$\left[H(x) = \text{sign}\left(\sum  _{t = 1}^{T}  \alpha_{t} \cdot  h_{t}(x)\right)\right]$$
其中 $h_t(x)$ 是第 t 个弱学习器的预测结果。


首先, AdaBoost 算法会使用权重向量 $D$ 预测输入值, 而<mark style="background: transparent; color: red">每次训练之后, 分对的样本权值下降, 分错的样本权值提高, </mark>
我们取  $\varepsilon$ 为正确分类的样本比例, 则在单层决策树中, 有:
$$\alpha = \frac{1}{2} \log_{e} \left(\frac{1-  \varepsilon}{\varepsilon}\right)= \frac{1}{2} \ln \left(\frac{1- \varepsilon}{\varepsilon}\right)$$
对于权值, 更新向量 $D$, 其中, **正确样本权值和错误样本权值分别更新**为:
$$D_{i + 1}^{corr} = \frac{D_{i}^{(i)} \varepsilon^{- \alpha}}{ \text{sum}(D)} \qquad D_{i + 1}^{err} = \frac{D_{i}^{(i)} \varepsilon\alpha}{\text{sum} (D)}$$
我们以图像分类训练为例,  其中大型数据集包含 [ImageNet](https://www.image-net.org/), CIFAR-10 等等, 其中  CIFAR-10 是 torchvision 中的一个自带数据集, 参考[CIFAR-10 Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) 部分; 
```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
``` 

应用场景: `AdaBoost` 通常用于以下类型的问题: 
1. **二分类问题**：例如垃圾邮件分类、客户流失预测等。
2. **多分类问题**：通过将多个二分类问题转化为多分类问题，可以解决更多复杂的任务。
3. **提升弱学习器的性能**：适合用于那些单个分类器表现不好的任务，尤其是当基础分类器表现较差时，通过集成方法来提高模型的准确度。

`AdaBoost` 算法特点:
1. **可以用于任何弱分类器**，最常见的是决策树。
2. **对于分类错误的样本赋予更大的权重**，从而促使下一轮学习器对这些难以分类的样本进行更准确的分类。
3. **对噪声敏感**，因为它会越来越专注于错误分类的样本，这可能会导致过拟合（尤其是在噪声数据上）。

假设我们有一个二分类问题，需要预测一个电子邮件是否为垃圾邮件。我们使用 `AdaBoost` 来提高分类性能，尤其是在单个基础分类器（如决策树）效果不理想时。

### (3) 实战:  AdaBoost 二分类垃圾邮件算法 
我们采用 sklearn 中的数据集  `fetch_20newsgroups_vectorized` 来训练二分类垃圾邮件问题
对于特征矩阵, 我们一般采用词频特征矩阵进行建立, 参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓深度学习算法原理(sklearn)/1.机器学习算法和文本分类挖掘(Naive Bayes)|1.机器学习算法和文本分类挖掘(Naive Bayes)]]  部分,然后直接从其中提取矩阵 (需要全局模式)
```python
import numpy as np  
from sklearn.model_selection import train_test_split  
from sklearn.ensemble import AdaBoostClassifier  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.datasets import fetch_20newsgroups_vectorized  
from sklearn.metrics import accuracy_score  
  
"""  
加载示例数据集，这里使用 `fetch_20newsgroups_vectorized` 作为一个文本分类问题的示例  
它提供了 20 类新闻组数据集，其中一类代表垃圾邮件，其他类代表非垃圾邮件。  
我们将其转化为一个二分类问题，1 代表垃圾邮件，0 代表非垃圾邮件  
"""  
  
# 载入数据  
newsgroups_data = fetch_20newsgroups_vectorized(subset='train', download_if_missing=True)  
X = newsgroups_data.data  # 特征矩阵
y = np.array([1 if target == 1 else 0 for target in newsgroups_data.target])  # 目标值（0 或 1)
# 切分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  
  
# 初始化 AdaBoost 模型，使用决策树作为基分类器  
# 我们使用决策树的最大深度为1来**确保它是一个弱学习器**
base_estimator = DecisionTreeClassifier(max_depth=1)  
adaboost = AdaBoostClassifier(  
    estimator=base_estimator,  
    n_estimators=50,  
    learning_rate=1,  
    algorithm="SAMME"  
)  
# 训练模型  
adaboost.fit(X_train, y_train)  
# 预测  
y_pred = adaboost.predict(X_test)  
# 输出准确率  
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
# Accuracy: 0.9540500736377026 
```

在二分类问题中, 如果传统的决策树模型不好, 则可以采用 AdaBoost 可以获取到很高的准确率。

## 二、特征提取相关算法讲解
### (1) 常用特征提取算法 
最多用的人脸识别技术是局部特征提取方法, 而通过主成分分析， 容易找到最大方差的轴; 
局部特征提取基本思想是<mark style="background: transparent; color: red">最小化同类内的方差, 而最大化不同类的方差</mark>
常用的局部特征提取方法包括 **Gabor 最小波**, **离散 Cosin 变换**和**局部 LBP(Local Binary Pattern, 局部二值模式)算法**, 往往用于特征提取部分。

## 三、 CIFAR-10 模型多分类问题
### (1) 数据集图片预览
例如显示数据集的第一张图片, 可以采用 (同时演示了 v2.Compose 如何使用, 用于显示多个变换):
```python 
from torch.utils.data import DataLoader  
from torchvision.datasets import CIFAR10  
from sklearn.ensemble import AdaBoostClassifier  
import torchvision.transforms as transforms  
import torchvision.transforms.v2 as v2
from PIL import Image

class CIFAR_practice():  
    def __init__(self):  
        self.train_batch_size = 1000  
        self.test_batch_size = 200  
        self.class_labels = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']  
        self.load_data()  
        self.show_test_images() 
	def load_data(self):
    # 定义数据预处理  transforms    t = v2.Compose([  
        v2.ToDtype(torch.float32, scale=True),  
        v2.ToTensor(),  
        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_dataset = CIFAR10( root='./cifar10', train=True, download=True, transform=t)  
    self.train_loader = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=True)  
    test_dataset = CIFAR10(root='./cifar10', train=False, download=True, transform=t)  
    self.test_loader  = DataLoader(test_dataset, batch_size=self.test_batch_size, shuffle=True)
    
	def show_test_images(self):  
	    for images, labels in self.train_loader:  
	        img = transforms.ToPILImage()(images[0])  
	        label =  self.class_labels[labels[0].item()]  
	        img.show()  
	        print(label)  
	        break
```
### (2) 基于 sklearn 的 AdaBoostClassifier 决策树分类效果
对于sklearn 模型,  由于库都已经封装好了, 所以可以直接采用 PCA 进行降维, 然后用 AdaBoost 算法直接进行图像分类预测即可。代码很简单, 如下 : 

```python fold title:sklearn的决策树图像分类
import torch  
from torchvision.datasets import CIFAR10  
from PIL import Image  
from  torch import nn  
import cv2  
import numpy as np  
  
# Used in adaboost Algorithm  
from sklearn.ensemble import AdaBoostClassifier  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.decomposition import PCA  
from sklearn.metrics import accuracy_score  
  
"""  
采用基础的 PCA 和 Adaboost 算法，对 cifar10 数据集进行分类 但是实际成功预测率极低, 需要进一步优化  
cnn 分类版本参考 image_classify_cnn.py"""  
class CIFAR_practice():  
    def __init__(self):  
        self.train_batch_size = 1000  
        self.test_batch_size = 200  
        self.load_data()  
        self.train_adaboost_model()  
  
    def load_data(self):  
        self.train_dataset = CIFAR10( root='./cifar10', train=True, download=True)  
        self.test_dataset = CIFAR10(root='./cifar10', train=False, download=True)  
  
    def get_labels(self, indices):  
        labels = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']  
        return [labels[i] for i in indices]  
  
    def getdata_adaboostModel(self):  
        def resize_images(img_data, size):  
            resized_images = np.zeros((img_data.shape[0], size[0], size[1], 3), dtype=np.float32)  
            for i in range(img_data.shape[0]):  
                resized_images[i] = cv2.resize(img_data[i], size)  
            return resized_images  
  
        new_sz = (28, 28)  
        data_train = resize_images(torch.tensor(self.train_dataset.data).numpy(), new_sz)  
        labels_train = torch.tensor(self.train_dataset.targets).numpy()  
        data_test = resize_images(torch.tensor(self.test_dataset.data).numpy(), new_sz)  
        labels_test = torch.tensor(self.test_dataset.targets).numpy()  
        return data_train, labels_train, data_test, labels_test  
    def train_adaboost_model(self):  
        """  
        直接采用 sklearn 训练出 adaboost 模型, 然后进行预测  
        :return:  
        """        data_train, labels_train, data_test, labels_test = self.getdata_adaboostModel()  
  
        data_train  = np.reshape(data_train, (-1, 28 * 28 * 3))  
        data_test   = np.reshape(data_test, (-1, 28 * 28 * 3))  
        model = PCA(n_components=50)  
        pca_data_train = model.fit_transform(data_train)  
        pca_data_test = model.transform(data_test)  
        adb = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=4), n_estimators=50, learning_rate=1,  
                                 algorithm='SAMME')  
        adb.fit(pca_data_train, labels_train)  
        label_pred_old = adb.predict(pca_data_train)  
        label_pred_new = adb.predict(pca_data_test)  
        print("accuracy on train data", accuracy_score(labels_train, label_pred_old))  
        print("accuracy on test data", accuracy_score(labels_test, label_pred_new))  
  
    @staticmethod    def criterion(cls, y_true, y_pred):  
        loss = nn.CrossEntropyLoss()  
        return loss(y_true, y_pred)  
  
    def train_cnn_model(self):  
        pass  
if __name__ == "__main__":  
    CIFAR_practice()  
    # if reaches really low
```

> [!NOTE] max_depth 参数部分
> - 较浅的树（例如 `max_depth=1`）通常被称为 **决策桩**（decision stumps），这种树只能根据一个特征做出决策, 几乎没有分类能力。
> - 较大的 `max_depth` 值允许树进行更多的分裂，这可能会导致过拟合，因为树会变得更加复杂，能够拟合训练数据中的噪声。
> - 一般对于 AdaBoost 算法采用 1 进行，来确保是一个弱学习器。

结果如下:
```python
accuracy on train data 0.36794
accuracy on test data 0.3548
```

 分析预测精度极低原因: 
 **避免使用 PCA 降维** 
- 上述代码使用了 PCA 将图像的维度从 `28 * 28 * 3 = 2352` 降到 `50`，这是一个非常激进的降维步骤。PCA 在降维过程中丢失了大量的原始图像信息，特别是在处理图像时，PCA 不一定能有效地保留特征。 
- 图像数据通常具有高度的非线性特征，PCA 是基于线性假设的降维方法，不一定适合图像数据。建议使用更为强大的模型（如 CNN），或者直接使用未降维的图像数据来训练模型。
- 如果仅用作是否为人脸的二分类, 可能实际分类精度会提高。
因此, AdaBoost 主要用于处理简单的弱分类器(如决策树),  而对于图像学习和深度学习, 我们采用卷积神经网络 (CNN) 来完成。
### (3) 基于 CNN 的图片分类方法
下面我们使用 CNN 和 `torch` 实现图像分类
CNN  基本原理可以参考 [CNN 详细介绍和原理详解](https://blog.csdn.net/IronmanJay/article/details/128689946)， 其中涵盖了 padding 的概念部分;

在 CNN 中，输出通道（也称为特征图或滤波器）是通过卷积核（filter或kernel）进行卷积运算后生成的特征。输出通道的数量通常由卷积层的参数决定。比如一个卷积层若设定有 `64` 个滤波器，那么输出通道数就是 `64`。输出的数据维度则会变成 `(5000, 64, H', W')`，其中 `H'` 和 `W'` 是经过卷积、池化等操作后图像的新高度和宽度。

代码如下: 
```python fold title:训练CNN卷积神经网络进行图像10分类问题
import random
import torch
from torch.nn.modules.module import T
from torchvision.datasets import CIFAR10
from PIL import Image
from  torch import nn
import numpy as np
from torch.utils.data import DataLoader
import torch.optim as optim
import torchvision.transforms as transforms
from torch.nn import AdaptiveMaxPool2d
from torchvision.transforms import v2
from sklearn.metrics import accuracy_score

class CIFIAR_CNN(nn.Module):
    def __init__(self, learning_rate = 0.001):
        super(CIFIAR_CNN, self).__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # 首先, 由于是 RGB 数据, 输入通道数为 3, 尺寸为 (batch_size, 3, 32, 32)
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 形成 64 *16 * 16 的输出
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),  # Downsample to 64 * 16 * 1
            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # n * 128 * 16 * 16
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # n * 128 * 16 * 16
            nn.ReLU(),
            nn.AdaptiveMaxPool2d((8, 8)) ,   # n * 128 * 16 * 16 target size, 该池化操作更加温和
            nn.Flatten(1, -1),      # 将全连接层的 64 * 8 * 8 的张量展平为一维 (10000 , (64 * 8 * 8))
            nn.Linear(256 * 8 * 8, 512),
            nn.ReLU(),         # 放在 Linear 之后, 激活函数, 用于实现非线性学习, 得到更复杂的模型
            nn.Dropout(p=0.2),  # Dropout 是一种正则化方法, 用于防止过拟合, 随机将一部分神经元输出置为 0 (0.2 概率)
            nn.Linear(512, 10),  # CIFAR-10 has 10 classes, so output is 10 dimensions
        )
    def forward(self,X):
        return self.net(X)

# 加载数据集, 进行训练
class CIFAR10Dataset_Practice():
    def __init__(self, learning_rate = 0.01, max_epoch = 1000, train_batch_size = 500, test_batch_size=500):
        self.train_batch_size = train_batch_size
        self.test_batch_size = test_batch_size
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = CIFIAR_CNN().to(self.device)    # 将 model 取为刚定义的 model 部分
        self.criterion = nn.CrossEntropyLoss()
        self.optim = optim.Adam(self.model.parameters(), lr=learning_rate)  # 在 optim 中定义学习率

        self.load_data(self.train_batch_size, self.test_batch_size)

        self.train_model(max_epoch)
        self.test_model()
        torch.save(self.model.state_dict(), './cnn_models.pth')

    def load_data(self, train_batch_size, test_batch_size):
        # 定义数据预处理  transforms
        t = v2.Compose([
            v2.ToImage(),
            v2.Resize((32, 32)),  # 在 transform 中, 将图像缩放到 32x32 大小, 减少特征数量
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
        self.train_dataset = CIFAR10(root='./cifar10', train=True, download=True, transform=t)
        self.test_dataset = CIFAR10(root='./cifar10', train=False, download=True, transform=t)
        self.train_loader = DataLoader(self.train_dataset, batch_size=train_batch_size, shuffle=True)
        self.test_loader = DataLoader(self.test_dataset, batch_size=test_batch_size, shuffle=True)
	
    def train_model(self, max_epoch):
        self.model.train()    # 只需要调用 train() 函数即可
        # 初始时模型是未训练的模型
        for epoch in range(max_epoch):
            train_loss = 0.0
            for data, labels in self.train_loader:
                self.optim.zero_grad()   # 在每次小批量训练时, 都需要清零梯度
                loss = self.criterion(self.model(data), labels)
                loss.backward()         # 反向传播
                self.optim.step()       # 利用反向传播得到的梯度, 进行参数更新
                train_loss += loss.item() * data.size(0)    # 计算总和的误差
            avg_train_loss = train_loss / len(self.train_loader.dataset)
            print(f"Epoch {epoch + 1}/{max_epoch}, Loss: {avg_train_loss}", end=",")

            # predict and evaluate
            with torch.no_grad():
                idx = random.randint(0, self.test_batch_size)
                # Only predict the first batch
                data, labels = next(iter(self.test_loader))
                data, labels = data.to(self.device), labels.to(self.device)
                # Get predictions
                label_pred = self.model(data)
                y_true = labels.cpu().numpy()
                y_pred = torch.argmax(label_pred, dim=1).cpu().numpy()
                print("Accuracy test: ", accuracy_score(y_true, y_pred))

    def test_model(self):
        # reload the train dataset and test dataset
        self.train_loader = DataLoader(self.train_dataset, batch_size=self.test_batch_size, shuffle=True)
        self.test_loader = DataLoader(self.test_dataset, batch_size=self.test_batch_size, shuffle=True)

        def get_dataloader_acc(dataloader):
            corr_num = 0
            tot_num = 0
            for data, labels in dataloader:
                data, labels = data.to(self.device), labels.to(self.device)
                pred = (torch.argmax(self.model(data), dim=1).cpu().numpy() == labels)  # 必须保证 dim = 1, 否则维数不匹配
                corr_num += torch.sum(pred).item()
                tot_num += len(labels)
            return corr_num , tot_num

        self.model.eval()
        corr, tot = get_dataloader_acc(self.train_loader)
        print(f"predicted correct number in train set : {corr}/{tot}")
        print("prediction accuracy in train set : ", corr / tot)

        corr, tot = get_dataloader_acc(self.test_loader)
        print(f"predicted correct number in train set : {corr}/{tot}")
        print("prediction accuracy in train set : ", corr / tot)

if __name__ == "__main__":
    CIFAR10Dataset_Practice(max_epoch=250)
```

使用上述代码, 在带有 Cuda 的电脑上, 训练 1000 次,  得到在训练集中的预测结果为 0.9958 而测试集的分类正确率仅为 0.5328, 出现了显著的过拟合现象。
![[attachments/IMG_20241201_094118.jpg|300]]
同时还有一个问题,  训练时可能会出现预测准确度为 0.1 之后始终保持的情况,  这说明 CNN 有可能具有一定的初值敏感性,  而模型收敛和发散可能和初值有一定的关系。因此如何后续进一步调整  CNN 模型成为下一步工作的重点。


### (4) CNN  网络优化和过拟合问题解决方案





## 四、其他 CNN
## 1. 卷积计算模块
卷积在 PyTorch 中有两种方式，一种是 `torch.nn.Conv2d()`，一种是 `torch.nn.functional.conv2d()`，这两种形式本质都是使用一个卷积操作。

这两种形式的卷积对于输入的要求都是一样的，首先需要输入是一个 `torch.autograd.Variable()` 的类型，大小是 `(batch, channel, H, W)`，其中 `batch` 表示输入的一批数据的数目，第二个是输入的通道数，一般一张彩色的图片是 3，灰度图是 1，而卷积网络过程中的通道数比较大，会出现几十到几百的通道数，`H` 和 `W` 表示输入图片的高度和宽度，比如一个 `batch` 是 32 张图片，每张图片是 3 通道，高和宽分别是 50 和 100，那么输入的大小就是 `(32, 3, 50, 100)`

例如导入torch的autograd模块：
```python
import numpy as np
import torch
from torch import nn
from torch.autograd import Variable
import torch.nn.functional as F
from PIL import Image
import matplotlib.pyplot as plt

im = Image.open('./cat.png').convert('L') # 读入一张灰度图的图片
im = np.array(im, dtype='float32') # 将其转换为一个矩阵
# 将图片矩阵转化为 pytorch tensor，并适配卷积输入的要求
im_t = torch.from_numpy(im.reshape((1, 1, im.shape[0], im.shape[1]))) # 将图片转换为toech表示的张量

# 可视化图片
fig = plt.imshow(im.astype('uint8'), cmap='gray')
fig.figure.savefig('fig-res-1-basic_conv_4_1.pdf')

# 使用 nn.Conv2d （in_channels, out_channels, kernel_size）
conv1 = nn.Conv2d(1, 1, 3, bias=False) # 定义卷积

# ---------------------定义卷积部分------
sobel_kernel = np.array(
    [[-1, -1, -1], 
     [-1, 8, -1], 
     [-1, -1, -1]], dtype='float32') # 定义轮廓检测算子(卷积核)
sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # 适配卷积的输入输出
conv1.weight.data = torch.from_numpy(sobel_kernel) # 给卷积的 kernel 赋值

edge1 = conv1(Variable(im_t)) # 将卷积作用在图片的pytorch张量上,获取新的卷积图
# ----------------使用conv1函数进行卷积-----
edge1 = edge1.data.squeeze().numpy() # 将输出转换为numpy数组格式(图片的格式)
# 这样就可以使用 plt.imshow 进行显示了
fig = plt.imshow(edge1, cmap='gray') # 显示边缘检测后的轮廓图
```

上述过程中，如果使用 `F.conv2d` 方法，可以将

```python
# 使用 F.conv2d
sobel_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') # 定义轮廓检测算子
sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # 适配卷积的输入输出
weight = Variable(torch.from_numpy(sobel_kernel))

edge2 = F.conv2d(Variable(im), weight) # 作用在图片上
edge2 = edge2.data.squeeze().numpy() # 将输出转换为图片的格式
```

不同的地方在于使用 `nn.Conv2d()` 相当于直接定义了一层卷积网络结构，而使用 `torch.nn.functional.conv2d()` 相当于定义了一个卷积的操作。

使用 `nn.Conv2d()` 会默认定义一个随机初始化的 weight，如果需要修改，那么取出其中的值对其修改，如果不想修改，那么可以直接使用这个默认初始化的值，非常方便。

**实际使用中我们基本都使用 `nn.Conv2d()` 这种形式**

## 2. 池化层
池化即为使图片大小减小的操作，

卷积网络中另外一个非常重要的结构就是 `池化`，这是利用了图片的下采样不变性，即一张图片变小了还是能够看出了这张图片的内容，而使用池化层能够将图片大小降低，非常好地提高了计算效率，同时池化层也没有参数。池化的方式有很多种，比如<mark style="background: transparent; color: red">最大值池化，均值池化等等，在卷积网络中一般使用最大值池化。</mark>

在 PyTorch 中最大值池化的方式也有两种，一种是 `nn.MaxPool2d()`，一种是 `torch.nn.functional.max_pool2d()`，他们对于图片的输入要求跟卷积对于图片的输入要求是一样的，就不再赘述，下面举例说明。

```python
# 使用 nn.MaxPool2d
pool1 = nn.MaxPool2d(2, 2)
print('before max pool, image shape: {} x {}'.format(im.shape[2], im.shape[3]))
small_im1 = pool1(Variable(im))
small_im1 = small_im1.data.squeeze().numpy()
print('after max pool, image shape: {} x {} '.format(small_im1.shape[0], small_im1.shape[1]))

fig = plt.imshow(small_im1, cmap='gray')
```

**跟卷积层一样，实际使用中，我们一般使用 `nn.MaxPool2d()`**

## 3. LeNet5 卷积模块

LeNet5 的架构的提出是基于如下的观点：==图像的特征分布在整张图像上，通过带有可学习参数的卷积，从而有效的减少了参数数量，能够在多个位置上提取相似特征==。

在LeNet5提出的时候，没有 GPU 帮助训练，甚至 CPU 的速度也很慢，因此，LeNet5的规模并不大。其包含<mark style="background: transparent; color: yellow">七个处理层，每一层都包含可训练参数（权重）</mark>，当时使用的输入数据是 32×3232×32 像素的图像。LeNet-5 这个网络虽然很小，但是它包含了深度学习的基本模块：==卷积层，池化层，全连接层==。它是其他深度学习模型的基础，这里对LeNet5进行深入分析和讲解，通过实例分析，加深对与卷积层和池化层的理解。

```python
import torch
from torch import nn
import torch.nn.functional as F

# 自定义一个LeNet5类
class LeNet5(nn.Module):
    def __init__(self):  # 定义方法是首先继承LeNet5中的类，然后在init中定义全连接层
        super(LeNet5, self).__init__()
        # 1-input channel, 6-output channels, 5x5-conv
        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 x 6 x 5 中间层5个,输入为1,输出为6
        # 6-input channel, 16-output channels, 5x5-conv
        self.conv2 = nn.Conv2d(6, 16, 5) # 6-> 16 
        # 16x5x5-input, 120-output
        
        # 注意这个是Linear的层, 
        self.fc1 = nn.Linear(16 * 5 * 5, 120) 
        # 120-input, 84-output
        self.fc2 = nn.Linear(120, 84)
        # 84-input, 10-output
        self.fc3 = nn.Linear(84, 10)
		
	    # LeNet5中有 
    def forward(self, x):  # 定义前向池化层
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))
        x = torch.flatten(x, 1) # 将结果拉升成1维向量，除了批次的维度
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = LeNet5()
print(net)
```

