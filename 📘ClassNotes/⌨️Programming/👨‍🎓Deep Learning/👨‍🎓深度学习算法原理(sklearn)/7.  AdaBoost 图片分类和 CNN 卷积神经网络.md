## ä¸€ã€åŸºäº AdaBoost çš„äººè„¸æ£€æµ‹
### (1) Opencv è‡ªå¸¦åº“äººè„¸æ£€æµ‹åŸç†
opencv çš„äººè„¸è¯†åˆ«ä¸­, **æœ€å¸¸ç”¨çš„æ˜¯ Haar ç‰¹å¾å’Œ  LBP ç‰¹å¾**,  å¯¹äº  Haar ç‰¹å¾æ£€æµ‹éƒ¨åˆ†,  å®é™…ä¸Š**éœ€è¦å®šä¹‰ç›¸åº”çš„çŸ©é˜µç‰¹å¾, åŒ…å«è¾¹ç•Œçº¿æ€§ç‰¹å¾ç­‰ç­‰**,  AdaBoost æ˜¯äººè„¸æ£€æµ‹çš„ä¸»æµç®—æ³•, æ¯ä¸€æ¬¡å¿«é€Ÿæ’é™¤ä¸å±äºå¯¹åº”ç‰©ä½“çš„åŒºåŸŸ, ä¿ç•™å…¶ä¸­ä¸ç¡®å®šåŒºåŸŸ,  

Haair ç‰¹å¾æ˜¯å°†äººè„¸fenwei   Edge Features , Line Features, Center-Surround Features å‡ ç§,   **å¦‚æœé‡‡ç”¨ Haar ç‰¹å¾æ£€æµ‹äººè„¸, åˆ™éœ€è¦é‡‡ç”¨ Haar ç‰¹å¾çº§è”è¡¨ä½œä¸ºè®­ç»ƒé›†**, å®é™…ä¸Šæ˜¯ xml æ–‡ä»¶;  è€Œ LBP ç‰¹å¾ä¹Ÿæ˜¯åŸºäºç‰¹å¾çº§è”è¡¨è¿›è¡Œè®­ç»ƒçš„; 

Theano å·²ç»ä¸å†ç»´æŠ¤ï¼Œä¹Ÿä¸æ”¯æŒ Python çš„æœ€æ–°ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ Python 3.12 ç­‰ç­‰, ä»…æ”¯æŒ Python 3.7 æˆ–æ›´ä½çš„ç‰ˆæœ¬

### (2) AdaBoost ç®—æ³•è¯´æ˜
AdaBoost (Adaptive Boosting)ç®—æ³•æ˜¯ä»[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æ·±åº¦å­¦ä¹ ç®—æ³•åŸç†(sklearn)/2. å†³ç­–æ ‘å’Œå›å½’æ ‘|å†³ç­–æ ‘]]å‘å±•å‡ºçš„ä¸€ç§ç®—æ³•,  å®é™…ä¸Šæ˜¯é’ˆå¯¹åŒä¸€è®­ç»ƒé›†æ ·æœ¬çš„ä¸åŒç‰¹å¾è®­ç»ƒå¤„ä¸åŒçš„å¼±åˆ†ç±»å™¨,  å¹¶çº§è”å¾—åˆ°å®é™…åˆ†ç±»å™¨,  å³é›†æˆå­¦ä¹ ç®—æ³•ã€‚

ä¸»è¦æµç¨‹åŒ…å«: 
1. åˆå§‹åŒ–æƒé‡  
2. è®­ç»ƒå¼±å­¦ä¹ å™¨ 
3. æ›´æ–°æ ·æœ¬æƒé‡å’Œæ„å»ºå¼ºå­¦ä¹ å™¨ 
![[Excalidraw/7. æ¨¡å¼è¯†åˆ«å’Œäººè„¸æ£€æµ‹ 2024-11-29 14.48.48|300]]
AdaBoost **å…³æ³¨ä¹‹å‰è¢«é”™è¯¯åˆ†ç±»çš„ç‚¹, ä»¥ä¿®æ­£æ¨¡å‹çš„ç¼ºé™·**.  è€Œæ¯ä¸€è½®éƒ½ä¼šè®­ç»ƒä¸€ä¸ªæ–°çš„å¼±å­¦ä¹ å™¨ã€‚

ç®—æ³•æµç¨‹:  
é¦–å…ˆç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›† \{ (x_1 , y_1), \dots (x_n, y_n) \} å…¶ä¸­$y_i$ å±äº  (-1,1) 

æ ·æœ¬æƒé‡æ›´æ–°: 
$$w^{t + 1} = w_{i}^{(t)} \times \exp  (- \alpha  \times  y_{i} \times   h_{t}(x_{i}))$$
å…¶ä¸­ $w_i^{(t)}$ ä¸º t è½®ä¸­æ ·æœ¬æƒé‡, $\alpha$  ä¸ºå¼±å­¦ä¹ å™¨æƒé‡,  $y_i$ ä¸ºçœŸå®æ ‡ç­¾ã€‚
<mark style="background: transparent; color: red">æœ€ç»ˆçš„æ¨¡å‹æ˜¯æ‰€æœ‰å¼±å­¦ä¹ å™¨çš„åŠ æƒæ±‚å’Œç»“æœ</mark>, å•å±‚å†³ç­–æ ‘ä»¥æœ€å°è¯¯å·®ä¸ºè¡¡é‡æ‰¾åˆ°æœ€ä¼˜åˆ—, 
$$\left[H(x) = \text{sign}\left(\sum  _{t = 1}^{T}  \alpha_{t} \cdot  h_{t}(x)\right)\right]$$
å…¶ä¸­ $h_t(x)$ æ˜¯ç¬¬ t ä¸ªå¼±å­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœã€‚


é¦–å…ˆ, AdaBoost ç®—æ³•ä¼šä½¿ç”¨æƒé‡å‘é‡ $D$ é¢„æµ‹è¾“å…¥å€¼, è€Œ<mark style="background: transparent; color: red">æ¯æ¬¡è®­ç»ƒä¹‹å, åˆ†å¯¹çš„æ ·æœ¬æƒå€¼ä¸‹é™, åˆ†é”™çš„æ ·æœ¬æƒå€¼æé«˜, </mark>
æˆ‘ä»¬å–  $\varepsilon$ ä¸ºæ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ¯”ä¾‹, åˆ™åœ¨å•å±‚å†³ç­–æ ‘ä¸­, æœ‰:
$$\alpha = \frac{1}{2} \log_{e} \left(\frac{1-  \varepsilon}{\varepsilon}\right)= \frac{1}{2} \ln \left(\frac{1- \varepsilon}{\varepsilon}\right)$$
å¯¹äºæƒå€¼, æ›´æ–°å‘é‡ $D$, å…¶ä¸­, **æ­£ç¡®æ ·æœ¬æƒå€¼å’Œé”™è¯¯æ ·æœ¬æƒå€¼åˆ†åˆ«æ›´æ–°**ä¸º:
$$D_{i + 1}^{corr} = \frac{D_{i}^{(i)} \varepsilon^{- \alpha}}{ \text{sum}(D)} \qquad D_{i + 1}^{err} = \frac{D_{i}^{(i)} \varepsilon\alpha}{\text{sum} (D)}$$
æˆ‘ä»¬ä»¥å›¾åƒåˆ†ç±»è®­ç»ƒä¸ºä¾‹,  å…¶ä¸­å¤§å‹æ•°æ®é›†åŒ…å« [ImageNet](https://www.image-net.org/), CIFAR-10 ç­‰ç­‰, å…¶ä¸­  CIFAR-10 æ˜¯ torchvision ä¸­çš„ä¸€ä¸ªè‡ªå¸¦æ•°æ®é›†, å‚è€ƒ[CIFAR-10 Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) éƒ¨åˆ†; 
```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
``` 

åº”ç”¨åœºæ™¯: `AdaBoost` é€šå¸¸ç”¨äºä»¥ä¸‹ç±»å‹çš„é—®é¢˜: 
1. **äºŒåˆ†ç±»é—®é¢˜**ï¼šä¾‹å¦‚åƒåœ¾é‚®ä»¶åˆ†ç±»ã€å®¢æˆ·æµå¤±é¢„æµ‹ç­‰ã€‚
2. **å¤šåˆ†ç±»é—®é¢˜**ï¼šé€šè¿‡å°†å¤šä¸ªäºŒåˆ†ç±»é—®é¢˜è½¬åŒ–ä¸ºå¤šåˆ†ç±»é—®é¢˜ï¼Œå¯ä»¥è§£å†³æ›´å¤šå¤æ‚çš„ä»»åŠ¡ã€‚
3. **æå‡å¼±å­¦ä¹ å™¨çš„æ€§èƒ½**ï¼šé€‚åˆç”¨äºé‚£äº›å•ä¸ªåˆ†ç±»å™¨è¡¨ç°ä¸å¥½çš„ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯å½“åŸºç¡€åˆ†ç±»å™¨è¡¨ç°è¾ƒå·®æ—¶ï¼Œé€šè¿‡é›†æˆæ–¹æ³•æ¥æé«˜æ¨¡å‹çš„å‡†ç¡®åº¦ã€‚

`AdaBoost` ç®—æ³•ç‰¹ç‚¹:
1. **å¯ä»¥ç”¨äºä»»ä½•å¼±åˆ†ç±»å™¨**ï¼Œæœ€å¸¸è§çš„æ˜¯å†³ç­–æ ‘ã€‚
2. **å¯¹äºåˆ†ç±»é”™è¯¯çš„æ ·æœ¬èµ‹äºˆæ›´å¤§çš„æƒé‡**ï¼Œä»è€Œä¿ƒä½¿ä¸‹ä¸€è½®å­¦ä¹ å™¨å¯¹è¿™äº›éš¾ä»¥åˆ†ç±»çš„æ ·æœ¬è¿›è¡Œæ›´å‡†ç¡®çš„åˆ†ç±»ã€‚
3. **å¯¹å™ªå£°æ•æ„Ÿ**ï¼Œå› ä¸ºå®ƒä¼šè¶Šæ¥è¶Šä¸“æ³¨äºé”™è¯¯åˆ†ç±»çš„æ ·æœ¬ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼ˆå°¤å…¶æ˜¯åœ¨å™ªå£°æ•°æ®ä¸Šï¼‰ã€‚

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œéœ€è¦é¢„æµ‹ä¸€ä¸ªç”µå­é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶ã€‚æˆ‘ä»¬ä½¿ç”¨ `AdaBoost` æ¥æé«˜åˆ†ç±»æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å•ä¸ªåŸºç¡€åˆ†ç±»å™¨ï¼ˆå¦‚å†³ç­–æ ‘ï¼‰æ•ˆæœä¸ç†æƒ³æ—¶ã€‚

### (3) å®æˆ˜:  AdaBoost äºŒåˆ†ç±»åƒåœ¾é‚®ä»¶ç®—æ³• 
æˆ‘ä»¬é‡‡ç”¨ sklearn ä¸­çš„æ•°æ®é›†  `fetch_20newsgroups_vectorized` æ¥è®­ç»ƒäºŒåˆ†ç±»åƒåœ¾é‚®ä»¶é—®é¢˜
å¯¹äºç‰¹å¾çŸ©é˜µ, æˆ‘ä»¬ä¸€èˆ¬é‡‡ç”¨è¯é¢‘ç‰¹å¾çŸ©é˜µè¿›è¡Œå»ºç«‹, å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/ğŸ‘¨â€ğŸ“æ·±åº¦å­¦ä¹ ç®—æ³•åŸç†(sklearn)/1.æœºå™¨å­¦ä¹ ç®—æ³•å’Œæ–‡æœ¬åˆ†ç±»æŒ–æ˜(Naive Bayes)|1.æœºå™¨å­¦ä¹ ç®—æ³•å’Œæ–‡æœ¬åˆ†ç±»æŒ–æ˜(Naive Bayes)]]  éƒ¨åˆ†,ç„¶åç›´æ¥ä»å…¶ä¸­æå–çŸ©é˜µ (éœ€è¦å…¨å±€æ¨¡å¼)
```python
import numpy as np  
from sklearn.model_selection import train_test_split  
from sklearn.ensemble import AdaBoostClassifier  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.datasets import fetch_20newsgroups_vectorized  
from sklearn.metrics import accuracy_score  
  
"""  
åŠ è½½ç¤ºä¾‹æ•°æ®é›†ï¼Œè¿™é‡Œä½¿ç”¨ `fetch_20newsgroups_vectorized` ä½œä¸ºä¸€ä¸ªæ–‡æœ¬åˆ†ç±»é—®é¢˜çš„ç¤ºä¾‹  
å®ƒæä¾›äº† 20 ç±»æ–°é—»ç»„æ•°æ®é›†ï¼Œå…¶ä¸­ä¸€ç±»ä»£è¡¨åƒåœ¾é‚®ä»¶ï¼Œå…¶ä»–ç±»ä»£è¡¨éåƒåœ¾é‚®ä»¶ã€‚  
æˆ‘ä»¬å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œ1 ä»£è¡¨åƒåœ¾é‚®ä»¶ï¼Œ0 ä»£è¡¨éåƒåœ¾é‚®ä»¶  
"""  
  
# è½½å…¥æ•°æ®  
newsgroups_data = fetch_20newsgroups_vectorized(subset='train', download_if_missing=True)  
X = newsgroups_data.data  # ç‰¹å¾çŸ©é˜µ
y = np.array([1 if target == 1 else 0 for target in newsgroups_data.target])  # ç›®æ ‡å€¼ï¼ˆ0 æˆ– 1)
# åˆ‡åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  
  
# åˆå§‹åŒ– AdaBoost æ¨¡å‹ï¼Œä½¿ç”¨å†³ç­–æ ‘ä½œä¸ºåŸºåˆ†ç±»å™¨  
# æˆ‘ä»¬ä½¿ç”¨å†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦ä¸º1æ¥**ç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªå¼±å­¦ä¹ å™¨**
base_estimator = DecisionTreeClassifier(max_depth=1)  
adaboost = AdaBoostClassifier(  
    estimator=base_estimator,  
    n_estimators=50,  
    learning_rate=1,  
    algorithm="SAMME"  
)  
# è®­ç»ƒæ¨¡å‹  
adaboost.fit(X_train, y_train)  
# é¢„æµ‹  
y_pred = adaboost.predict(X_test)  
# è¾“å‡ºå‡†ç¡®ç‡  
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
# Accuracy: 0.9540500736377026 
```

åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­, å¦‚æœä¼ ç»Ÿçš„å†³ç­–æ ‘æ¨¡å‹ä¸å¥½, åˆ™å¯ä»¥é‡‡ç”¨ AdaBoost å¯ä»¥è·å–åˆ°å¾ˆé«˜çš„å‡†ç¡®ç‡ã€‚

## äºŒã€ç‰¹å¾æå–ç›¸å…³ç®—æ³•è®²è§£
### (1) å¸¸ç”¨ç‰¹å¾æå–ç®—æ³• 
æœ€å¤šç”¨çš„äººè„¸è¯†åˆ«æŠ€æœ¯æ˜¯å±€éƒ¨ç‰¹å¾æå–æ–¹æ³•, è€Œé€šè¿‡ä¸»æˆåˆ†åˆ†æï¼Œ å®¹æ˜“æ‰¾åˆ°æœ€å¤§æ–¹å·®çš„è½´; 
å±€éƒ¨ç‰¹å¾æå–åŸºæœ¬æ€æƒ³æ˜¯<mark style="background: transparent; color: red">æœ€å°åŒ–åŒç±»å†…çš„æ–¹å·®, è€Œæœ€å¤§åŒ–ä¸åŒç±»çš„æ–¹å·®</mark>
å¸¸ç”¨çš„å±€éƒ¨ç‰¹å¾æå–æ–¹æ³•åŒ…æ‹¬ **Gabor æœ€å°æ³¢**, **ç¦»æ•£ Cosin å˜æ¢**å’Œ**å±€éƒ¨ LBP(Local Binary Pattern, å±€éƒ¨äºŒå€¼æ¨¡å¼)ç®—æ³•**, å¾€å¾€ç”¨äºç‰¹å¾æå–éƒ¨åˆ†ã€‚

## ä¸‰ã€ CIFAR-10 æ¨¡å‹å¤šåˆ†ç±»é—®é¢˜
### (1) æ•°æ®é›†å›¾ç‰‡é¢„è§ˆ
ä¾‹å¦‚æ˜¾ç¤ºæ•°æ®é›†çš„ç¬¬ä¸€å¼ å›¾ç‰‡, å¯ä»¥é‡‡ç”¨ (åŒæ—¶æ¼”ç¤ºäº† v2.Compose å¦‚ä½•ä½¿ç”¨, ç”¨äºæ˜¾ç¤ºå¤šä¸ªå˜æ¢):
```python 
from torch.utils.data import DataLoader  
from torchvision.datasets import CIFAR10  
from sklearn.ensemble import AdaBoostClassifier  
import torchvision.transforms as transforms  
import torchvision.transforms.v2 as v2
from PIL import Image

class CIFAR_practice():  
    def __init__(self):  
        self.train_batch_size = 1000  
        self.test_batch_size = 200  
        self.class_labels = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']  
        self.load_data()  
        self.show_test_images() 
	def load_data(self):
    # å®šä¹‰æ•°æ®é¢„å¤„ç†  transforms    t = v2.Compose([  
        v2.ToDtype(torch.float32, scale=True),  
        v2.ToTensor(),  
        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_dataset = CIFAR10( root='./cifar10', train=True, download=True, transform=t)  
    self.train_loader = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=True)  
    test_dataset = CIFAR10(root='./cifar10', train=False, download=True, transform=t)  
    self.test_loader  = DataLoader(test_dataset, batch_size=self.test_batch_size, shuffle=True)
    
	def show_test_images(self):  
	    for images, labels in self.train_loader:  
	        img = transforms.ToPILImage()(images[0])  
	        label =  self.class_labels[labels[0].item()]  
	        img.show()  
	        print(label)  
	        break
```
### (2) åŸºäº sklearn çš„ AdaBoostClassifier å†³ç­–æ ‘åˆ†ç±»æ•ˆæœ
å¯¹äºsklearn æ¨¡å‹,  ç”±äºåº“éƒ½å·²ç»å°è£…å¥½äº†, æ‰€ä»¥å¯ä»¥ç›´æ¥é‡‡ç”¨ PCA è¿›è¡Œé™ç»´, ç„¶åç”¨ AdaBoost ç®—æ³•ç›´æ¥è¿›è¡Œå›¾åƒåˆ†ç±»é¢„æµ‹å³å¯ã€‚ä»£ç å¾ˆç®€å•, å¦‚ä¸‹ : 

```python fold title:sklearnçš„å†³ç­–æ ‘å›¾åƒåˆ†ç±»
import torch  
from torchvision.datasets import CIFAR10  
from PIL import Image  
from  torch import nn  
import cv2  
import numpy as np  
  
# Used in adaboost Algorithm  
from sklearn.ensemble import AdaBoostClassifier  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.decomposition import PCA  
from sklearn.metrics import accuracy_score  
  
"""  
é‡‡ç”¨åŸºç¡€çš„ PCA å’Œ Adaboost ç®—æ³•ï¼Œå¯¹ cifar10 æ•°æ®é›†è¿›è¡Œåˆ†ç±» ä½†æ˜¯å®é™…æˆåŠŸé¢„æµ‹ç‡æä½, éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–  
cnn åˆ†ç±»ç‰ˆæœ¬å‚è€ƒ image_classify_cnn.py"""  
class CIFAR_practice():  
    def __init__(self):  
        self.train_batch_size = 1000  
        self.test_batch_size = 200  
        self.load_data()  
        self.train_adaboost_model()  
  
    def load_data(self):  
        self.train_dataset = CIFAR10( root='./cifar10', train=True, download=True)  
        self.test_dataset = CIFAR10(root='./cifar10', train=False, download=True)  
  
    def get_labels(self, indices):  
        labels = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']  
        return [labels[i] for i in indices]  
  
    def getdata_adaboostModel(self):  
        def resize_images(img_data, size):  
            resized_images = np.zeros((img_data.shape[0], size[0], size[1], 3), dtype=np.float32)  
            for i in range(img_data.shape[0]):  
                resized_images[i] = cv2.resize(img_data[i], size)  
            return resized_images  
  
        new_sz = (28, 28)  
        data_train = resize_images(torch.tensor(self.train_dataset.data).numpy(), new_sz)  
        labels_train = torch.tensor(self.train_dataset.targets).numpy()  
        data_test = resize_images(torch.tensor(self.test_dataset.data).numpy(), new_sz)  
        labels_test = torch.tensor(self.test_dataset.targets).numpy()  
        return data_train, labels_train, data_test, labels_test  
    def train_adaboost_model(self):  
        """  
        ç›´æ¥é‡‡ç”¨ sklearn è®­ç»ƒå‡º adaboost æ¨¡å‹, ç„¶åè¿›è¡Œé¢„æµ‹  
        :return:  
        """        data_train, labels_train, data_test, labels_test = self.getdata_adaboostModel()  
  
        data_train  = np.reshape(data_train, (-1, 28 * 28 * 3))  
        data_test   = np.reshape(data_test, (-1, 28 * 28 * 3))  
        model = PCA(n_components=50)  
        pca_data_train = model.fit_transform(data_train)  
        pca_data_test = model.transform(data_test)  
        adb = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=4), n_estimators=50, learning_rate=1,  
                                 algorithm='SAMME')  
        adb.fit(pca_data_train, labels_train)  
        label_pred_old = adb.predict(pca_data_train)  
        label_pred_new = adb.predict(pca_data_test)  
        print("accuracy on train data", accuracy_score(labels_train, label_pred_old))  
        print("accuracy on test data", accuracy_score(labels_test, label_pred_new))  
  
    @staticmethod    def criterion(cls, y_true, y_pred):  
        loss = nn.CrossEntropyLoss()  
        return loss(y_true, y_pred)  
  
    def train_cnn_model(self):  
        pass  
if __name__ == "__main__":  
    CIFAR_practice()  
    # if reaches really low
```

> [!NOTE] max_depth å‚æ•°éƒ¨åˆ†
> - è¾ƒæµ…çš„æ ‘ï¼ˆä¾‹å¦‚ `max_depth=1`ï¼‰é€šå¸¸è¢«ç§°ä¸º **å†³ç­–æ¡©**ï¼ˆdecision stumpsï¼‰ï¼Œè¿™ç§æ ‘åªèƒ½æ ¹æ®ä¸€ä¸ªç‰¹å¾åšå‡ºå†³ç­–, å‡ ä¹æ²¡æœ‰åˆ†ç±»èƒ½åŠ›ã€‚
> - è¾ƒå¤§çš„ `max_depth` å€¼å…è®¸æ ‘è¿›è¡Œæ›´å¤šçš„åˆ†è£‚ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå› ä¸ºæ ‘ä¼šå˜å¾—æ›´åŠ å¤æ‚ï¼Œèƒ½å¤Ÿæ‹Ÿåˆè®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°ã€‚
> - ä¸€èˆ¬å¯¹äº AdaBoost ç®—æ³•é‡‡ç”¨ 1 è¿›è¡Œï¼Œæ¥ç¡®ä¿æ˜¯ä¸€ä¸ªå¼±å­¦ä¹ å™¨ã€‚

ç»“æœå¦‚ä¸‹:
```python
accuracy on train data 0.36794
accuracy on test data 0.3548
```

 åˆ†æé¢„æµ‹ç²¾åº¦æä½åŸå› : 
 **é¿å…ä½¿ç”¨ PCA é™ç»´** 
- ä¸Šè¿°ä»£ç ä½¿ç”¨äº† PCA å°†å›¾åƒçš„ç»´åº¦ä» `28 * 28 * 3 = 2352` é™åˆ° `50`ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸æ¿€è¿›çš„é™ç»´æ­¥éª¤ã€‚PCA åœ¨é™ç»´è¿‡ç¨‹ä¸­ä¸¢å¤±äº†å¤§é‡çš„åŸå§‹å›¾åƒä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å›¾åƒæ—¶ï¼ŒPCA ä¸ä¸€å®šèƒ½æœ‰æ•ˆåœ°ä¿ç•™ç‰¹å¾ã€‚ 
- å›¾åƒæ•°æ®é€šå¸¸å…·æœ‰é«˜åº¦çš„éçº¿æ€§ç‰¹å¾ï¼ŒPCA æ˜¯åŸºäºçº¿æ€§å‡è®¾çš„é™ç»´æ–¹æ³•ï¼Œä¸ä¸€å®šé€‚åˆå›¾åƒæ•°æ®ã€‚å»ºè®®ä½¿ç”¨æ›´ä¸ºå¼ºå¤§çš„æ¨¡å‹ï¼ˆå¦‚ CNNï¼‰ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨æœªé™ç»´çš„å›¾åƒæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚
- å¦‚æœä»…ç”¨ä½œæ˜¯å¦ä¸ºäººè„¸çš„äºŒåˆ†ç±», å¯èƒ½å®é™…åˆ†ç±»ç²¾åº¦ä¼šæé«˜ã€‚
å› æ­¤, AdaBoost ä¸»è¦ç”¨äºå¤„ç†ç®€å•çš„å¼±åˆ†ç±»å™¨(å¦‚å†³ç­–æ ‘),  è€Œå¯¹äºå›¾åƒå­¦ä¹ å’Œæ·±åº¦å­¦ä¹ , æˆ‘ä»¬é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ¥å®Œæˆã€‚
### (3) åŸºäº CNN çš„å›¾ç‰‡åˆ†ç±»æ–¹æ³•
ä¸‹é¢æˆ‘ä»¬ä½¿ç”¨ CNN å’Œ `torch` å®ç°å›¾åƒåˆ†ç±»
CNN  åŸºæœ¬åŸç†å¯ä»¥å‚è€ƒ [CNN è¯¦ç»†ä»‹ç»å’ŒåŸç†è¯¦è§£](https://blog.csdn.net/IronmanJay/article/details/128689946)ï¼Œ å…¶ä¸­æ¶µç›–äº† padding çš„æ¦‚å¿µéƒ¨åˆ†;

åœ¨ CNN ä¸­ï¼Œè¾“å‡ºé€šé“ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾æˆ–æ»¤æ³¢å™¨ï¼‰æ˜¯é€šè¿‡å·ç§¯æ ¸ï¼ˆfilteræˆ–kernelï¼‰è¿›è¡Œå·ç§¯è¿ç®—åç”Ÿæˆçš„ç‰¹å¾ã€‚è¾“å‡ºé€šé“çš„æ•°é‡é€šå¸¸ç”±å·ç§¯å±‚çš„å‚æ•°å†³å®šã€‚æ¯”å¦‚ä¸€ä¸ªå·ç§¯å±‚è‹¥è®¾å®šæœ‰Â `64`Â ä¸ªæ»¤æ³¢å™¨ï¼Œé‚£ä¹ˆè¾“å‡ºé€šé“æ•°å°±æ˜¯Â `64`ã€‚è¾“å‡ºçš„æ•°æ®ç»´åº¦åˆ™ä¼šå˜æˆÂ `(5000, 64, H', W')`ï¼Œå…¶ä¸­Â `H'`Â å’ŒÂ `W'`Â æ˜¯ç»è¿‡å·ç§¯ã€æ± åŒ–ç­‰æ“ä½œåå›¾åƒçš„æ–°é«˜åº¦å’Œå®½åº¦ã€‚

ä»£ç å¦‚ä¸‹: 
```python fold title:è®­ç»ƒCNNå·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå›¾åƒ10åˆ†ç±»é—®é¢˜
import random
import torch
from torch.nn.modules.module import T
from torchvision.datasets import CIFAR10
from PIL import Image
from  torch import nn
import numpy as np
from torch.utils.data import DataLoader
import torch.optim as optim
import torchvision.transforms as transforms
from torch.nn import AdaptiveMaxPool2d
from torchvision.transforms import v2
from sklearn.metrics import accuracy_score

class CIFIAR_CNN(nn.Module):
    def __init__(self, learning_rate = 0.001):
        super(CIFIAR_CNN, self).__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # é¦–å…ˆ, ç”±äºæ˜¯ RGB æ•°æ®, è¾“å…¥é€šé“æ•°ä¸º 3, å°ºå¯¸ä¸º (batch_size, 3, 32, 32)
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # å½¢æˆ 64 *16 * 16 çš„è¾“å‡º
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),  # Downsample to 64 * 16 * 1
            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # n * 128 * 16 * 16
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # n * 128 * 16 * 16
            nn.ReLU(),
            nn.AdaptiveMaxPool2d((8, 8)) ,   # n * 128 * 16 * 16 target size, è¯¥æ± åŒ–æ“ä½œæ›´åŠ æ¸©å’Œ
            nn.Flatten(1, -1),      # å°†å…¨è¿æ¥å±‚çš„ 64 * 8 * 8 çš„å¼ é‡å±•å¹³ä¸ºä¸€ç»´ (10000 , (64 * 8 * 8))
            nn.Linear(256 * 8 * 8, 512),
            nn.ReLU(),         # æ”¾åœ¨ Linear ä¹‹å, æ¿€æ´»å‡½æ•°, ç”¨äºå®ç°éçº¿æ€§å­¦ä¹ , å¾—åˆ°æ›´å¤æ‚çš„æ¨¡å‹
            nn.Dropout(p=0.2),  # Dropout æ˜¯ä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•, ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ, éšæœºå°†ä¸€éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºç½®ä¸º 0 (0.2 æ¦‚ç‡)
            nn.Linear(512, 10),  # CIFAR-10 has 10 classes, so output is 10 dimensions
        )
    def forward(self,X):
        return self.net(X)

# åŠ è½½æ•°æ®é›†, è¿›è¡Œè®­ç»ƒ
class CIFAR10Dataset_Practice():
    def __init__(self, learning_rate = 0.01, max_epoch = 1000, train_batch_size = 500, test_batch_size=500):
        self.train_batch_size = train_batch_size
        self.test_batch_size = test_batch_size
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = CIFIAR_CNN().to(self.device)    # å°† model å–ä¸ºåˆšå®šä¹‰çš„ model éƒ¨åˆ†
        self.criterion = nn.CrossEntropyLoss()
        self.optim = optim.Adam(self.model.parameters(), lr=learning_rate)  # åœ¨ optim ä¸­å®šä¹‰å­¦ä¹ ç‡

        self.load_data(self.train_batch_size, self.test_batch_size)

        self.train_model(max_epoch)
        self.test_model()
        torch.save(self.model.state_dict(), './cnn_models.pth')

    def load_data(self, train_batch_size, test_batch_size):
        # å®šä¹‰æ•°æ®é¢„å¤„ç†  transforms
        t = v2.Compose([
            v2.ToImage(),
            v2.Resize((32, 32)),  # åœ¨ transform ä¸­, å°†å›¾åƒç¼©æ”¾åˆ° 32x32 å¤§å°, å‡å°‘ç‰¹å¾æ•°é‡
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
        self.train_dataset = CIFAR10(root='./cifar10', train=True, download=True, transform=t)
        self.test_dataset = CIFAR10(root='./cifar10', train=False, download=True, transform=t)
        self.train_loader = DataLoader(self.train_dataset, batch_size=train_batch_size, shuffle=True)
        self.test_loader = DataLoader(self.test_dataset, batch_size=test_batch_size, shuffle=True)
	
    def train_model(self, max_epoch):
        self.model.train()    # åªéœ€è¦è°ƒç”¨ train() å‡½æ•°å³å¯
        # åˆå§‹æ—¶æ¨¡å‹æ˜¯æœªè®­ç»ƒçš„æ¨¡å‹
        for epoch in range(max_epoch):
            train_loss = 0.0
            for data, labels in self.train_loader:
                self.optim.zero_grad()   # åœ¨æ¯æ¬¡å°æ‰¹é‡è®­ç»ƒæ—¶, éƒ½éœ€è¦æ¸…é›¶æ¢¯åº¦
                loss = self.criterion(self.model(data), labels)
                loss.backward()         # åå‘ä¼ æ’­
                self.optim.step()       # åˆ©ç”¨åå‘ä¼ æ’­å¾—åˆ°çš„æ¢¯åº¦, è¿›è¡Œå‚æ•°æ›´æ–°
                train_loss += loss.item() * data.size(0)    # è®¡ç®—æ€»å’Œçš„è¯¯å·®
            avg_train_loss = train_loss / len(self.train_loader.dataset)
            print(f"Epoch {epoch + 1}/{max_epoch}, Loss: {avg_train_loss}", end=",")

            # predict and evaluate
            with torch.no_grad():
                idx = random.randint(0, self.test_batch_size)
                # Only predict the first batch
                data, labels = next(iter(self.test_loader))
                data, labels = data.to(self.device), labels.to(self.device)
                # Get predictions
                label_pred = self.model(data)
                y_true = labels.cpu().numpy()
                y_pred = torch.argmax(label_pred, dim=1).cpu().numpy()
                print("Accuracy test: ", accuracy_score(y_true, y_pred))

    def test_model(self):
        # reload the train dataset and test dataset
        self.train_loader = DataLoader(self.train_dataset, batch_size=self.test_batch_size, shuffle=True)
        self.test_loader = DataLoader(self.test_dataset, batch_size=self.test_batch_size, shuffle=True)

        def get_dataloader_acc(dataloader):
            corr_num = 0
            tot_num = 0
            for data, labels in dataloader:
                data, labels = data.to(self.device), labels.to(self.device)
                pred = (torch.argmax(self.model(data), dim=1).cpu().numpy() == labels)  # å¿…é¡»ä¿è¯ dim = 1, å¦åˆ™ç»´æ•°ä¸åŒ¹é…
                corr_num += torch.sum(pred).item()
                tot_num += len(labels)
            return corr_num , tot_num

        self.model.eval()
        corr, tot = get_dataloader_acc(self.train_loader)
        print(f"predicted correct number in train set : {corr}/{tot}")
        print("prediction accuracy in train set : ", corr / tot)

        corr, tot = get_dataloader_acc(self.test_loader)
        print(f"predicted correct number in train set : {corr}/{tot}")
        print("prediction accuracy in train set : ", corr / tot)

if __name__ == "__main__":
    CIFAR10Dataset_Practice(max_epoch=250)
```

ä½¿ç”¨ä¸Šè¿°ä»£ç , åœ¨å¸¦æœ‰ Cuda çš„ç”µè„‘ä¸Š, è®­ç»ƒ 1000 æ¬¡,  å¾—åˆ°åœ¨è®­ç»ƒé›†ä¸­çš„é¢„æµ‹ç»“æœä¸º 0.9958 è€Œæµ‹è¯•é›†çš„åˆ†ç±»æ­£ç¡®ç‡ä»…ä¸º 0.5328, å‡ºç°äº†æ˜¾è‘—çš„è¿‡æ‹Ÿåˆç°è±¡ã€‚
![[attachments/IMG_20241201_094118.jpg|300]]
åŒæ—¶è¿˜æœ‰ä¸€ä¸ªé—®é¢˜,  è®­ç»ƒæ—¶å¯èƒ½ä¼šå‡ºç°é¢„æµ‹å‡†ç¡®åº¦ä¸º 0.1 ä¹‹åå§‹ç»ˆä¿æŒçš„æƒ…å†µ,  è¿™è¯´æ˜ CNN æœ‰å¯èƒ½å…·æœ‰ä¸€å®šçš„åˆå€¼æ•æ„Ÿæ€§,  è€Œæ¨¡å‹æ”¶æ•›å’Œå‘æ•£å¯èƒ½å’Œåˆå€¼æœ‰ä¸€å®šçš„å…³ç³»ã€‚å› æ­¤å¦‚ä½•åç»­è¿›ä¸€æ­¥è°ƒæ•´  CNN æ¨¡å‹æˆä¸ºä¸‹ä¸€æ­¥å·¥ä½œçš„é‡ç‚¹ã€‚


### (4) CNN  ç½‘ç»œä¼˜åŒ–å’Œè¿‡æ‹Ÿåˆé—®é¢˜è§£å†³æ–¹æ¡ˆ





## å››ã€å…¶ä»– CNN
## 1. å·ç§¯è®¡ç®—æ¨¡å—
å·ç§¯åœ¨ PyTorch ä¸­æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯Â `torch.nn.Conv2d()`ï¼Œä¸€ç§æ˜¯Â `torch.nn.functional.conv2d()`ï¼Œè¿™ä¸¤ç§å½¢å¼æœ¬è´¨éƒ½æ˜¯ä½¿ç”¨ä¸€ä¸ªå·ç§¯æ“ä½œã€‚

è¿™ä¸¤ç§å½¢å¼çš„å·ç§¯å¯¹äºè¾“å…¥çš„è¦æ±‚éƒ½æ˜¯ä¸€æ ·çš„ï¼Œé¦–å…ˆéœ€è¦è¾“å…¥æ˜¯ä¸€ä¸ªÂ `torch.autograd.Variable()`Â çš„ç±»å‹ï¼Œå¤§å°æ˜¯Â `(batch, channel, H, W)`ï¼Œå…¶ä¸­Â `batch`Â è¡¨ç¤ºè¾“å…¥çš„ä¸€æ‰¹æ•°æ®çš„æ•°ç›®ï¼Œç¬¬äºŒä¸ªæ˜¯è¾“å…¥çš„é€šé“æ•°ï¼Œä¸€èˆ¬ä¸€å¼ å½©è‰²çš„å›¾ç‰‡æ˜¯ 3ï¼Œç°åº¦å›¾æ˜¯ 1ï¼Œè€Œå·ç§¯ç½‘ç»œè¿‡ç¨‹ä¸­çš„é€šé“æ•°æ¯”è¾ƒå¤§ï¼Œä¼šå‡ºç°å‡ ååˆ°å‡ ç™¾çš„é€šé“æ•°ï¼Œ`H`Â å’ŒÂ `W`Â è¡¨ç¤ºè¾“å…¥å›¾ç‰‡çš„é«˜åº¦å’Œå®½åº¦ï¼Œæ¯”å¦‚ä¸€ä¸ªÂ `batch`Â æ˜¯ 32 å¼ å›¾ç‰‡ï¼Œæ¯å¼ å›¾ç‰‡æ˜¯ 3 é€šé“ï¼Œé«˜å’Œå®½åˆ†åˆ«æ˜¯ 50 å’Œ 100ï¼Œé‚£ä¹ˆè¾“å…¥çš„å¤§å°å°±æ˜¯Â `(32, 3, 50, 100)`

ä¾‹å¦‚å¯¼å…¥torchçš„autogradæ¨¡å—ï¼š
```python
import numpy as np
import torch
from torch import nn
from torch.autograd import Variable
import torch.nn.functional as F
from PIL import Image
import matplotlib.pyplot as plt

im = Image.open('./cat.png').convert('L') # è¯»å…¥ä¸€å¼ ç°åº¦å›¾çš„å›¾ç‰‡
im = np.array(im, dtype='float32') # å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªçŸ©é˜µ
# å°†å›¾ç‰‡çŸ©é˜µè½¬åŒ–ä¸º pytorch tensorï¼Œå¹¶é€‚é…å·ç§¯è¾“å…¥çš„è¦æ±‚
im_t = torch.from_numpy(im.reshape((1, 1, im.shape[0], im.shape[1]))) # å°†å›¾ç‰‡è½¬æ¢ä¸ºtoechè¡¨ç¤ºçš„å¼ é‡

# å¯è§†åŒ–å›¾ç‰‡
fig = plt.imshow(im.astype('uint8'), cmap='gray')
fig.figure.savefig('fig-res-1-basic_conv_4_1.pdf')

# ä½¿ç”¨ nn.Conv2d ï¼ˆin_channels, out_channels, kernel_sizeï¼‰
conv1 = nn.Conv2d(1, 1, 3, bias=False) # å®šä¹‰å·ç§¯

# ---------------------å®šä¹‰å·ç§¯éƒ¨åˆ†------
sobel_kernel = np.array(
    [[-1, -1, -1], 
     [-1, 8, -1], 
     [-1, -1, -1]], dtype='float32') # å®šä¹‰è½®å»“æ£€æµ‹ç®—å­(å·ç§¯æ ¸)
sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # é€‚é…å·ç§¯çš„è¾“å…¥è¾“å‡º
conv1.weight.data = torch.from_numpy(sobel_kernel) # ç»™å·ç§¯çš„ kernel èµ‹å€¼

edge1 = conv1(Variable(im_t)) # å°†å·ç§¯ä½œç”¨åœ¨å›¾ç‰‡çš„pytorchå¼ é‡ä¸Š,è·å–æ–°çš„å·ç§¯å›¾
# ----------------ä½¿ç”¨conv1å‡½æ•°è¿›è¡Œå·ç§¯-----
edge1 = edge1.data.squeeze().numpy() # å°†è¾“å‡ºè½¬æ¢ä¸ºnumpyæ•°ç»„æ ¼å¼(å›¾ç‰‡çš„æ ¼å¼)
# è¿™æ ·å°±å¯ä»¥ä½¿ç”¨ plt.imshow è¿›è¡Œæ˜¾ç¤ºäº†
fig = plt.imshow(edge1, cmap='gray') # æ˜¾ç¤ºè¾¹ç¼˜æ£€æµ‹åçš„è½®å»“å›¾
```

ä¸Šè¿°è¿‡ç¨‹ä¸­ï¼Œå¦‚æœä½¿ç”¨ `F.conv2d` æ–¹æ³•ï¼Œå¯ä»¥å°†

```python
# ä½¿ç”¨ F.conv2d
sobel_kernel = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype='float32') # å®šä¹‰è½®å»“æ£€æµ‹ç®—å­
sobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # é€‚é…å·ç§¯çš„è¾“å…¥è¾“å‡º
weight = Variable(torch.from_numpy(sobel_kernel))

edge2 = F.conv2d(Variable(im), weight) # ä½œç”¨åœ¨å›¾ç‰‡ä¸Š
edge2 = edge2.data.squeeze().numpy() # å°†è¾“å‡ºè½¬æ¢ä¸ºå›¾ç‰‡çš„æ ¼å¼
```

ä¸åŒçš„åœ°æ–¹åœ¨äºä½¿ç”¨Â `nn.Conv2d()`Â ç›¸å½“äºç›´æ¥å®šä¹‰äº†ä¸€å±‚å·ç§¯ç½‘ç»œç»“æ„ï¼Œè€Œä½¿ç”¨Â `torch.nn.functional.conv2d()`Â ç›¸å½“äºå®šä¹‰äº†ä¸€ä¸ªå·ç§¯çš„æ“ä½œã€‚

ä½¿ç”¨Â `nn.Conv2d()`Â ä¼šé»˜è®¤å®šä¹‰ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„ weightï¼Œå¦‚æœéœ€è¦ä¿®æ”¹ï¼Œé‚£ä¹ˆå–å‡ºå…¶ä¸­çš„å€¼å¯¹å…¶ä¿®æ”¹ï¼Œå¦‚æœä¸æƒ³ä¿®æ”¹ï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªé»˜è®¤åˆå§‹åŒ–çš„å€¼ï¼Œéå¸¸æ–¹ä¾¿ã€‚

**å®é™…ä½¿ç”¨ä¸­æˆ‘ä»¬åŸºæœ¬éƒ½ä½¿ç”¨Â `nn.Conv2d()`Â è¿™ç§å½¢å¼**

## 2. æ± åŒ–å±‚
æ± åŒ–å³ä¸ºä½¿å›¾ç‰‡å¤§å°å‡å°çš„æ“ä½œï¼Œ

å·ç§¯ç½‘ç»œä¸­å¦å¤–ä¸€ä¸ªéå¸¸é‡è¦çš„ç»“æ„å°±æ˜¯Â `æ± åŒ–`ï¼Œè¿™æ˜¯åˆ©ç”¨äº†å›¾ç‰‡çš„ä¸‹é‡‡æ ·ä¸å˜æ€§ï¼Œå³ä¸€å¼ å›¾ç‰‡å˜å°äº†è¿˜æ˜¯èƒ½å¤Ÿçœ‹å‡ºäº†è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œè€Œä½¿ç”¨æ± åŒ–å±‚èƒ½å¤Ÿå°†å›¾ç‰‡å¤§å°é™ä½ï¼Œéå¸¸å¥½åœ°æé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶æ± åŒ–å±‚ä¹Ÿæ²¡æœ‰å‚æ•°ã€‚æ± åŒ–çš„æ–¹å¼æœ‰å¾ˆå¤šç§ï¼Œæ¯”å¦‚<mark style="background: transparent; color: red">æœ€å¤§å€¼æ± åŒ–ï¼Œå‡å€¼æ± åŒ–ç­‰ç­‰ï¼Œåœ¨å·ç§¯ç½‘ç»œä¸­ä¸€èˆ¬ä½¿ç”¨æœ€å¤§å€¼æ± åŒ–ã€‚</mark>

åœ¨ PyTorch ä¸­æœ€å¤§å€¼æ± åŒ–çš„æ–¹å¼ä¹Ÿæœ‰ä¸¤ç§ï¼Œä¸€ç§æ˜¯Â `nn.MaxPool2d()`ï¼Œä¸€ç§æ˜¯Â `torch.nn.functional.max_pool2d()`ï¼Œä»–ä»¬å¯¹äºå›¾ç‰‡çš„è¾“å…¥è¦æ±‚è·Ÿå·ç§¯å¯¹äºå›¾ç‰‡çš„è¾“å…¥è¦æ±‚æ˜¯ä¸€æ ·çš„ï¼Œå°±ä¸å†èµ˜è¿°ï¼Œä¸‹é¢ä¸¾ä¾‹è¯´æ˜ã€‚

```python
# ä½¿ç”¨ nn.MaxPool2d
pool1 = nn.MaxPool2d(2, 2)
print('before max pool, image shape: {} x {}'.format(im.shape[2], im.shape[3]))
small_im1 = pool1(Variable(im))
small_im1 = small_im1.data.squeeze().numpy()
print('after max pool, image shape: {} x {} '.format(small_im1.shape[0], small_im1.shape[1]))

fig = plt.imshow(small_im1, cmap='gray')
```

**è·Ÿå·ç§¯å±‚ä¸€æ ·ï¼Œå®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨Â `nn.MaxPool2d()`**

## 3. LeNet5 å·ç§¯æ¨¡å—

LeNet5 çš„æ¶æ„çš„æå‡ºæ˜¯åŸºäºå¦‚ä¸‹çš„è§‚ç‚¹ï¼š==å›¾åƒçš„ç‰¹å¾åˆ†å¸ƒåœ¨æ•´å¼ å›¾åƒä¸Šï¼Œé€šè¿‡å¸¦æœ‰å¯å­¦ä¹ å‚æ•°çš„å·ç§¯ï¼Œä»è€Œæœ‰æ•ˆçš„å‡å°‘äº†å‚æ•°æ•°é‡ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªä½ç½®ä¸Šæå–ç›¸ä¼¼ç‰¹å¾==ã€‚

åœ¨LeNet5æå‡ºçš„æ—¶å€™ï¼Œæ²¡æœ‰ GPU å¸®åŠ©è®­ç»ƒï¼Œç”šè‡³ CPU çš„é€Ÿåº¦ä¹Ÿå¾ˆæ…¢ï¼Œå› æ­¤ï¼ŒLeNet5çš„è§„æ¨¡å¹¶ä¸å¤§ã€‚å…¶åŒ…å«<mark style="background: transparent; color: yellow">ä¸ƒä¸ªå¤„ç†å±‚ï¼Œæ¯ä¸€å±‚éƒ½åŒ…å«å¯è®­ç»ƒå‚æ•°ï¼ˆæƒé‡ï¼‰</mark>ï¼Œå½“æ—¶ä½¿ç”¨çš„è¾“å…¥æ•°æ®æ˜¯Â 32Ã—3232Ã—32Â åƒç´ çš„å›¾åƒã€‚LeNet-5 è¿™ä¸ªç½‘ç»œè™½ç„¶å¾ˆå°ï¼Œä½†æ˜¯å®ƒåŒ…å«äº†æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¨¡å—ï¼š==å·ç§¯å±‚ï¼Œæ± åŒ–å±‚ï¼Œå…¨è¿æ¥å±‚==ã€‚å®ƒæ˜¯å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºç¡€ï¼Œè¿™é‡Œå¯¹LeNet5è¿›è¡Œæ·±å…¥åˆ†æå’Œè®²è§£ï¼Œé€šè¿‡å®ä¾‹åˆ†æï¼ŒåŠ æ·±å¯¹ä¸å·ç§¯å±‚å’Œæ± åŒ–å±‚çš„ç†è§£ã€‚

```python
import torch
from torch import nn
import torch.nn.functional as F

# è‡ªå®šä¹‰ä¸€ä¸ªLeNet5ç±»
class LeNet5(nn.Module):
    def __init__(self):  # å®šä¹‰æ–¹æ³•æ˜¯é¦–å…ˆç»§æ‰¿LeNet5ä¸­çš„ç±»ï¼Œç„¶ååœ¨initä¸­å®šä¹‰å…¨è¿æ¥å±‚
        super(LeNet5, self).__init__()
        # 1-input channel, 6-output channels, 5x5-conv
        self.conv1 = nn.Conv2d(1, 6, 5)  # 1 x 6 x 5 ä¸­é—´å±‚5ä¸ª,è¾“å…¥ä¸º1,è¾“å‡ºä¸º6
        # 6-input channel, 16-output channels, 5x5-conv
        self.conv2 = nn.Conv2d(6, 16, 5) # 6-> 16 
        # 16x5x5-input, 120-output
        
        # æ³¨æ„è¿™ä¸ªæ˜¯Linearçš„å±‚, 
        self.fc1 = nn.Linear(16 * 5 * 5, 120) 
        # 120-input, 84-output
        self.fc2 = nn.Linear(120, 84)
        # 84-input, 10-output
        self.fc3 = nn.Linear(84, 10)
		
	    # LeNet5ä¸­æœ‰ 
    def forward(self, x):  # å®šä¹‰å‰å‘æ± åŒ–å±‚
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))
        x = torch.flatten(x, 1) # å°†ç»“æœæ‹‰å‡æˆ1ç»´å‘é‡ï¼Œé™¤äº†æ‰¹æ¬¡çš„ç»´åº¦
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = LeNet5()
print(net)
```

