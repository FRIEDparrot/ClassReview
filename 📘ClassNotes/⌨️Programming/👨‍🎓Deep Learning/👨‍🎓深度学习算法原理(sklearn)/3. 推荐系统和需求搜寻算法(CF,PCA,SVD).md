## 一、需求搜寻算法简介 
推荐系统的部分需要对于需求的深入理解，如果仅基于传统的搜索引擎算法，关键赓续哟啊的字查询信息难以定位需求。而实际上用户一般更需要的是符合个人偏好的结果。

一般的推荐系统的主要功能包括 : 
1. **打包销售(Cross Selling)**:即在推荐某些产品时, 也会同时推荐一同购买的产品
2. **协同过滤**: 主要是定位用户的购买需求。基于模糊关键字从列表中明确需求。此外也包括看过此商品之后购买的其它商品。反映了隐式需求(曾经浏览过某页面的用户所购买的产品)。
3. 用户商品评论列表的抽和汇总分析, 用于评估产品质量的分布水平。

推荐系统的主要算法包含 : 
1. 基于人口统计学的推荐机制 : 根据用户的基本信息获取相关程度, 并根据相似用户喜好的的其他物品推荐给当前用户。
2. 基于协同过滤的推荐机制 : 主要分为<b><mark style="background: transparent; color: blue">基于用户的推荐和基于项目的推荐</mark></b>。
3. 基于隐语义的推荐模型 : 其中, **目前精度最高的算法是 SVD 隐语义模型**。

一般在训练阶段, 属于 CPU 密集型任务, 而在分类或者预测阶段属于 IO 密集型任务。 一般不使用脚本语言进行设计。

协同过滤的模以及算法包含: 
1. 数据处理与 UI 矩阵 
2. UserCF 和 ItemCF 推荐模型
3. KMeans 相似性计算 
4. SVD 相似性计算  

### (1) 物品数据的 KMeans 聚类
KMeans 聚类参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓深度学习算法原理(sklearn)/补充知识/4.KNN算法和KMeans算法#二、K-Means算法(K均值算法)|KMeans算法]] , 并给出一个简单的 iris 数据集 KMeans 聚类代码:
```python
import sklearn  
from sklearn.datasets import load_iris  
from sklearn.cluster import KMeans  
from sklearn.preprocessing import StandardScaler  
from sklearn.model_selection import train_test_split  
import matplotlib.pyplot as plt  
  
iris = load_iris()  
  
iris_data = iris.data  
iris_target = iris.target  
  
X_train, X_test, y_train, y_test =  train_test_split(iris_data, iris_target, test_size=0.2, random_state=None)  
  
kms = KMeans(n_clusters=3)  
kms.fit(X_train, y_train)  
  
y_pred = kms.predict(iris_data)  
  
fig, axes = plt.subplots(2,1, figsize=(5,10))  
axes[0].scatter(iris_data[:,0], iris_data[:,1], c=iris_target)  
axes[0].set_title("real results")  
axes[1].scatter(iris_data[:,0], iris_data[:,1], c=y_pred)  
axes[1].set_title("KMeans results")  
plt.show()
```

KMeans 聚类结果如下:
![[attachments/Pasted image 20240914173557.png]]

```python
# 获取聚类中心点
centers = kms.cluster_centers_
print("Cluster centers:\n", centers)
```

二分 KMeans 仍然参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓深度学习算法原理(sklearn)/补充知识/4.KNN算法和KMeans算法|4.KNN算法和KMeans算法]]


### (2) 协同过滤 User CF 和 Item CF 原理
协同过滤(Collabrate Filtering, CF) 分为 User CF 和 Item CF 部分;
首先需要建立用户偏好矩阵, 根据 KNN 算法，以距离或者夹角余弦为距离, 分别计算其到每一类用户的距离;
![[Excalidraw/3. 推荐系统和需求搜寻算法 2024-09-14 18.03.57|550]]
User CF 即采用KNN等将用户(USER C)通过usr_vector 归类到某一类用户(A或B)中。并将该类用户没有买过的物品进行推荐
Item CF 即选取新用户(USER C)比较偏好的物品 (item E), 并通过分类获取与 item E 相似的物品 (通过item_vec可以找出如item A相似,则将item A 也推荐给 item E的用户)

### (3) 一般矩阵的 SVD 分解及其证明 
对于基于模型的 CF 方法中, 最常用的是隐语义模型, 采用的是 SVD 分解, 即矩阵的奇异值分解。参考[[📘ClassNotes/📐Mathmatics/📏linear algebra/第五章 矩阵的相似变换#(3) 矩阵的相似对角化|第五章 矩阵的相似变换]]. 

对于 n 阶方阵 $A^{n\times n}$, 有相似矩阵:
$$A = Q \Sigma Q ^{-1}$$
其中 $\Sigma$ 为特征值构成的对角阵, $Q$ 为特征向量排成的列阵。但是,此方法仅仅是对方阵而言的。实际**多数情况下样本不是方阵**，此时可以<mark style="background: transparent; color: red">以奇异值分解描述普通矩阵的重要特征</mark>

<b><mark style="background: transparent; color: blue">定义(矩阵的SVD分解)</mark></b>:设 A 是任意的 $M \times N$ 矩阵, 则有矩阵的 SVD 分解为:
$$\Large\boxed{A =  U \Sigma V^{T}}$$
其中 $U$ 是 $M \times M$ 方阵, $\Sigma$ 是一个 $M \times N$ 矩阵, 仅有对角线上有元素, 且<mark style="background: transparent; color: red">对角线上的元素称为奇异值</mark>, 其它元素均为0. 而 $V^T$ 是一个 $N \times N$ 方阵。即:
$$\Large\boxed{A_{m \times  n} = U _{m\times m} \Sigma_{m \times  n} V^{T}_{n \times n}}$$
其中 U 的元素称为<mark style="background: transparent; color: red">左奇异向量</mark>, V 中的元素称为<mark style="background: transparent; color: red">右奇异向量</mark>, 并有关系 $U^{T} U = V^{T}V= I$

参考 [Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) 补充 [polar_decomposition](https://en.wikipedia.org/wiki/Polar_decomposition#Matrix_polar_decomposition)  推导可以参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓深度学习算法原理(sklearn)/补充知识/SVD decomposition.pdf|SVD decomposition.pdf]]  和 [SVD分解的证明](https://zhuanlan.zhihu.com/p/399547902)

**给出推导**: 首先, 设A的秩为r, 即有 $r = \min \left\{ m,n\right\}$, 此时 $A^T$ 秩显然也为 $r$, 经过[一文关于矩阵秩公式r(AA^T)=r(A^T A)=r(A)的证明](https://zhuanlan.zhihu.com/p/447703725) 得到:
$$rank (A^{T}A ) =  rank (A A^{T} )$$
则 $\text{rank}(A^T A) = r$, 此时根据$A^TA$为半正定矩阵(显然$x^TA^TAx \geq 0$), 因此其特征值为非负实数。其中**秩的个数即为非零特征值的个数**，取其特征值得到:
$$\boxed{\Large  (A^{T}A) v_{i}= \lambda_{i} v_{i}}$$
其中 $A$ 为 $m \times n$ 矩阵, **同时,由于$AA^T$为实对称阵, 相似对角化后 $v_i, v_j$ 是正交向量, 则**:
$$<Av_{i} ,Av_{j}> = v_{j}^{T} A^{T} A v_{i}  = \lambda v_{j}^{T}  v_{i} = \begin{cases}
\lambda_{i} \qquad  1 \leq  i = j \leq  r  \\
0 \qquad  i \neq j  \text{ or } i = j > r
\end{cases}$$
显然 $<Av_i, Av_j>$ 为正交向量组, 此时取 i = j 即得如下式子:
$$\boxed{\Large ||Av_{i}|| = \sqrt{\lambda_{i}}\qquad i = 1, 2, \dots  r}$$
$$\Large\boxed{||Av_{i}|| = 0 \qquad  i = r + 1, \dots n}$$
其中$v_{i}$就是上面的<mark style="background: transparent; color: red">右奇异向量</mark>, 我们取 $\sigma_i  = \sqrt{\lambda_i}$ 即为<b><mark style="background: transparent; color: blue">奇异值</mark></b>，而令 
$$\Large \boxed{u_{i}  = \frac{Av_{i}}{||Av_{i}||} =  \frac{1}{\sqrt{\lambda_{i}}} Av_{i} \qquad  i= 1, \dots  n} $$
**其中由于 A 为 $m*n$ 故 $\boldsymbol{u}$ 为 $m \times 1$ 的向量, 实际上就是向量u**<b><mark style="background: transparent; color: blue">(左奇异向量)</mark></b>。然后经过如下证明过程:
![[attachments/Pasted image 20240919170741.png]]
得到 SVD 分解:
$$\Large\boxed {A_{m \times  n} = U_{m \times  m }\Sigma_{m \times  n} V_{n \times n}^{T}}$$
### (4) 矩阵的部分奇异值(SVD)分解
奇异值$\sigma$和特征值类似, 显然**在 $\Sigma$ 中, $\sigma$ 值也是从大到小排列的**。此时**可能矩阵前 10% 甚至 1% 的特征值的和就占据了全部奇异值之和的99%以上**。我们取前 $r$ 个奇异值, 其余的 r+1阶以后的奇异值非常接近0,则定义<b><mark style="background: transparent; color: blue">矩阵的部分SVD分解</mark></b>为:
$$\boxed{\Large A_{m \times  r} \approx U_{m \times r} \Sigma_{r \times r} V^{T}_{r\times  n}}$$
在numpy.linalg 中提供了SVD分解函数:
```python 
import numpy.linalg import svd
U, S, VT  = svd(A)
print(U, S, VT)
```


## 二、PCA 主成分分析和 SVD 分解的应用
需要说明, PCA 和 SVD 都是 sklearn.decomposition 中的内容。 
### (1) PCA 主成分分析原理
[sklearn实战之降维算法PCA与SVD_使用svd进行主成分分析鸢尾花分类-CSDN博客](https://blog.csdn.net/qq_48314528/article/details/119845670)
PCA 的基本原理是：如果一个==高维数据集如果能够被相关变量表示, 那么仅有一些维有意义。根据此原理, 可以提取出高维变量中某些特征或者相关变量, 即可采用低维数据表示该变量==, 而<mark style="background: transparent; color: red">不重要的维可以在计算中忽略</mark>。此时<b><mark style="background: transparent; color: blue">如果寻找到数据中方差最大的方向, 则被称为向量的主成分</mark></b> 

在高维数据中, 往往噪声不带有有效信息, 且部分特征是相关的, 因此需要在减少特征数量同时保持保留有效信息。<mark style="background: transparent; color: red">根据样本方差, 方差越大, 特征所含有信息越多。 一般地, 方差小的数据, 其体现信息比较少, 因此可以舍去</mark>: 

一般地, 样本方差计算为(采用 n-1 是样本的无偏估计): 
$$\text{Var} = \frac{1}{n -1} \sum^{n}_{i=1} (x_{i} - \overline{x})^{2}$$
PCA 的一般步骤如下: 首先, 计算样本的均值向量和协方差矩阵$S$:
$$\mu =\frac{1}{n} \sum^{n}_{i=1} x_{i} \qquad  S = \frac{1}{n}\sum^{n}_{i=1}(x_{i}- \mu) (x_{i} - \mu)^{T}$$
然后计算 S 的特征值 $\lambda_{i}$ 和对应的特征向量 $v_i$ 
$$Sv_{i}  = \lambda_{i} v_{i}  \qquad  i = 1,2,\dots n$$
然后<mark style="background: transparent; color: red">对特征值和特征向量进行递减排序, 如果定义一个向量有 k 个主成分, 则对应的 k 个主成分即为 k 个最大的特征值及所对应的特征向量</mark>。注意: 这个通过 PCA`(n_components = x)` 进行指定(如果仅仅可视化,可以降维到二维)
$$y = W^{T}  (x -u)   \qquad  W = (v_{1}, v_{2}, \dots  v_{n})$$
而 <b><mark style="background: transparent; color: blue">PCA 基的变换</mark></b>为
$$\boxed{\Large x = W y + u \qquad  W =  (v_{1}, v_{2}, \dots  v_{k})}$$
下面采用 PCA 降维之后预测 Iris 数据集并绘制图像
```python
from sklearn.datasets import load_iris  
from sklearn.decomposition import PCA  
from sklearn.preprocessing import StandardScaler  
from sklearn.model_selection import train_test_split, cross_val_score  
from sklearn.naive_bayes import GaussianNB  
from sklearn.metrics import accuracy_score  
import matplotlib.pyplot as plt  
  
X,y = load_iris(return_X_y=True)  
x_train, x_test,y_train, y_test = train_test_split(X,y,test_size=0.35,random_state=42)  
  
pca = PCA(n_components=2).fit(x_train) # 2 components  
print(pca.components_)     # 主成分的特征向量(每一行对应一个主成分， 即对应的 V 矩阵)
print("variance:" ,pca.explained_variance_)         # 查看降维后每个新特征向量上所带的信息量大小（可解释性方差的大小）  
print("variance_ratio:", pca.explained_variance_ratio_)   # 查看降维后每个新特征向量上所带的信息量占总信息量的比例（可解释性方差的百分比）  
  
x_train_new = pca.transform(x_train)
x_test_new = pca.transform(x_test)  
  
gnb = GaussianNB()  
gnb.fit(x_train_new,y_train)  
y_pred = gnb.predict(x_test_new)  
  
fig, axes = plt.subplots(1,2, figsize=(12,5), subplot_kw = {"xticks":[],"yticks":[]})  # subplot_kw 指定绘制顺序
axes[0].scatter(x_test_new[:,0],x_test_new[:,1],c=y_test)  
axes[1].scatter(x_test_new[:,0],x_test_new[:,1],c=y_pred)  
plt.show()  
  
print("predict accuracy:", accuracy_score(y_test,y_pred, normalize=True))
```

> [!caution] PCA 的 n_components 参数
> 1. 为整数时, 则保留相应的维数。
> 2. 如果不写任何值, 则返回 min(X.shape) (由于样本量一般大于特征个数, 相当于没有转换特征, 因此除了绘制累计可解释方差贡献率曲线 `np.cumsum(pca_line.explained_variance_ratio_)` 以外, 一般不采用该方式)
> 3. 可以采用 PCA 用最大似然估计进行自选超参数 `n_components` , 只需指定
> `pca = PCA(n_components = "mle")` 即可 
> 4. 输入 [0,1] 间的浮点数并取 `svd_solver = full` 时, 有 `pca_f = PCA(n_components=0.97,svd_solver="full")` 则降维到保证降维后**总解释性方差贡献率大于n_components指定百分比的信息量**的维数。

另外绘制三维图像也非常简单(不进行预测):
```python
from sklearn.decomposition import PCA  
# unused but required import for doing 3d projections with matplotlib < 3.2  
import mpl_toolkits.mplot3d

fig = plt.figure(1, figsize=(8, 6))  
ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)  
  
X_reduced = PCA(n_components=3).fit_transform(iris.data)  
ax.scatter(  
    X_reduced[:, 0],  
    X_reduced[:, 1],  
    X_reduced[:, 2],  
    c=iris.target,  
    s=40, # marker size )
plt.show()
```
### (2) PCA 中的 SVD 
SVD 实际上是<b><mark style="background: transparent; color: blue">不计算协方差矩阵, 直接找出一个新特征向量组成的 n 维空间</mark></b>, 即直接获取右分解向量矩阵 $V^T$, 而 $U$ 和 $\Sigma$ 在 fit 完成之后, 即被舍弃。
而 `print(pca.components_)` 可以直接获取 PCA 中 SVD 分解器的$V$参数。

实际上PCA 的 SVD 求解器是通过 PCA (svd_solver) 控制的, 参数包含:
**“auto”，“full”，“arapck”，“randomized”，默认"auto"**，具体解释如下:
![[attachments/Pasted image 20240922105941.png]]

需要说明 PCA和特征选择的区别: **特征选择后的特征矩阵是可解读的，而PCA降维后的特征矩阵是不可解读的, 即将已经存在的特征进行压缩, 且降维完毕后特征不是原矩阵的任何特征, 而是通过某些方式进行组合起来的，新的特征**.
**重要的是： 如果原特征矩阵是图像，V(k,n)这 个空间矩阵也可以被可视化的话，我们就可以通过两张图来比较，就可以看出新特征空间究竟从原始数据里提取了 什么重要的信息**。例如人脸识别中有较为好的应用。

但是在矩阵分解时, PCA一般在原有特征基础上, 找出能够让信息尽量聚集的新的特征向量。

> [!NOTE] 补充: PCA 的 inverse_transform 部分
> PCA 可以通过 X **右乘所提取的特征矩阵 V** 生成新矩阵 $X_{dr}$ 则让 X_dr 右乘 $V(k,n)$ 逆矩阵 $V^{-1}_{(k,n)}$ 即可将 X_dr 还原为 $X$, 但舍弃了降维后的部分的信息。

### (3) 

从 Kaggle 上获取推荐系统数据集 [retailrocket](https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset) 进行分析;


#### Task 1
**When a customer comes to an e-commerce site, he looks for a product with particular properties: price range, vendor, product type and etc. These properties are implicit**, so it's hard to determine them through clicks log.

Try to **create an algorithm which predicts properties of items in "addtocart" event by using data from "view" events for any visitor in the published log**.

#### Task 2
Description:
Process of analyzing ecommerce data include very important part of data cleaning. Researchers noticed that in some cases browsing data include up to 40% of abnormal traffic.

Firstly, abnormal users add a lot of noise into data and make recommendation system less effective. In order to increase efficiency of recommendation system, abnormal users should be removed from the raw data.

Secondly, abnormal users add bias to results of split tests, so this type of users should be removed also from split test data.

Goals:
- The main goal is to find abnormal users of e-shop.

Subgoals:
- Generate features
- Build a model
- Create a metric that helps to evaluate quality of the model