#### 1. softmaxå‡½æ•°
softmaxå‡½æ•°å¯ä»¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ ¹æ®ä¸‰ä¸ªæ•°å€¼çš„ç›¸å¯¹å¤§å°,å°†å…¶å˜ä¸ºå’Œä¸º1çš„æ¦‚ç‡
$$S_{ij} = \frac{e^{z_i}}{\sum_k e^{z_k}}$$
-  ğ‘†ğ‘–æ˜¯ç»è¿‡softmaxçš„ç±»åˆ«æ¦‚ç‡è¾“å‡º
-  ğ‘§_ğ‘˜æ˜¯ç¥ç»å…ƒçš„è¾“å‡º

ä¸åŒçš„å€¼åæ˜ å‡ºå–ä¸åŒçš„éƒ¨åˆ†çš„æ¦‚ç‡

#### 2. äº¤å‰ç†µæŸå¤±å‡½æ•°
äº¤å‰ç†µçš„å®šä¹‰æ˜¯æ ¹æ®æ­£ç¡®åˆ†ç±»æ•°é‡å®šä¹‰çš„æŸå¤±å‡½æ•°, å‚è€ƒ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#(3) Loss Function|Loss Function]] éƒ¨åˆ†, å®šä¹‰ä¸º:
$$\boxed{\Large C = -\underset{i}{\sum} y_{i} \log_{}  \hat{y}_{i}}$$
ä¸€èˆ¬<mark style="background: transparent; color: red">å›å½’é—®é¢˜é‡‡ç”¨ MSE, MEAE ç­‰ç­‰ä½œä¸ºæŸå¤±å‡½æ•°,  åˆ†ç±»é‡‡ç”¨ crossEntropy ä½œä¸º Loss </mark>

äº¤å‰ç†µæŸå¤±å‡½æ•°å’ŒSoftmax**ç»å¸¸ä¸€èµ·ä½¿ç”¨**ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¿™ç§ç»„åˆè¢«ç§°ä¸ºâ€œSoftmax-äº¤å‰ç†µâ€ã€‚è¿™ç§æ­é…éå¸¸å¸¸è§ï¼Œå¹¶ä¸”ç†è®ºä¸Šå’Œå®è·µä¸Šéƒ½å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚

æœ‰æ—¶, softmax å’Œäº¤å‰ç†µä¼šè¿›è¡Œåˆå¹¶è®¡ç®—, å³æœ‰:
$$\text{Softmax: } \hat{y}_j = \frac{\exp(z_j)}{\sum_{k=1}^q \exp(z_k)}$$
$$\text{Cross-entropy: } l(y, \hat{y}) = -\sum_{j=1}^q y_j \log(\hat{y}_{j})$$
éœ€è¦æ³¨æ„çš„æ˜¯, python ä¸­, `torch.nn.CrossEntropyLoss` å·²ç»å°† **Softmax** å’Œ **äº¤å‰ç†µ** æŸå¤±å‡½æ•°æ•´åˆåœ¨äº†ä¸€èµ·ï¼Œå› æ­¤ **ä¸éœ€è¦åœ¨ç½‘ç»œæœ€åä¸€å±‚æ˜¾å¼æ·»åŠ  Softmax**ã€‚ç›´æ¥å°† **logits**ï¼ˆæœªç»è¿‡Softmaxçš„ç½‘ç»œè¾“å‡ºï¼‰ä¼ å…¥å³å¯ã€‚
å› æ­¤é‡‡ç”¨äº¤å‰ç†µ `nn.CrossEntropyLoss` æ—¶, ä¸éœ€è¦ softmax ä½œä¸ºæœ€åä¸€å±‚ä½¿ç”¨, è€Œé‡‡ç”¨  nn.Linear ç­‰å‡½æ•°ã€‚

å®é™…ä¸Š, å¦‚æœè¿›è¡Œ[[ğŸ“˜ClassNotes/âŒ¨ï¸Programming/ğŸ‘¨â€ğŸ“Deep Learning/âš“Deep Learning Basic Concepts/Chapter4 Linear Neural Networks for Classification#(4) Softmax and Cross-Entropy loss Relation|è”åˆæ¨å¯¼]], åˆ™å®¹æ˜“å¾—åˆ° :
$$\Large\boxed{l(y, \hat{y}) =  \log\sum^{q}_{k=1} \exp (o_{k}) - \sum^{q}_{j=1} y_{j} o_{j}}$$
å¹¶æœ‰:
$$\frac{\partial l(y, \hat{y})}{\partial o_{j}} = \text{softmax}(o_{j})- y_{j}$$
