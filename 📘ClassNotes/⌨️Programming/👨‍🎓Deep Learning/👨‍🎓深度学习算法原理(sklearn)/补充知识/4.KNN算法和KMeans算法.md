### 一、KNN算法
KNN(K-Nearest Neighbor Classification)算法, 也叫K最邻近分类算法, 是一种常用的识别，分类算法。
KNN算法要求分成的类别是已知的, 且训练样本点的类别均已知。

基本过程如下:
1. **对于每个点, 求解数据点到其余各个训练样本点的距离d**。
2. **求解对应的最邻近的K个训练样本点**。
3. 判断样本点中最多的点的类别， 并将该点与其他归为一类。 
优点是简单方便, 但是对噪声数据比较敏感(k不同，可以起到一定抗噪声作用)

需要说明的是, KNN算法<mark style="background: transparent; color: red">可以采用不同的距离度量公式</mark>, 就文本分类, **一般采用夹角余弦**(即和原点的夹角余弦值)作为距离公式。
$$\cos \theta = \frac{A\cdot B}{|A| |B|}$$
```python 
from numpy import linalg as la
def dist_cos(vector1, vector2):
	return dot(vector1, vector2)/(la.norm(vector1) * la.norm(vector2))
```

### 二、K-Means算法(K均值算法)
聚类要求： <mark style="background: transparent; color: red">类内距离小，类间距离大, 目标是未知</mark> 
1. 随机取K个点作为初始的聚类中心，代表各个聚类(一般选取数据点(注意是不同的数据点)，有时随机)
2. 计算求解各个样本相对于各个数据的距离。**并将点归类到与其距离最近的点**
3. 重新求解新类别的均值，再次调整中心位置(调整中心位置的过程:<mark style="background: transparent; color: red">采用子集的均值进行更新</mark>)
4. **重新求解，迭代至中心点近似不动为止**

简单示例参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓深度学习算法原理(sklearn)/3. 推荐系统和需求搜寻算法(CF,PCA,SVD)#(1) 物品数据的 KMeans 聚类|KMeans 聚类]] 

> [!hint] 补充
> KMeans 适合处理球状分布的数据, 对于聚类结果是密集的,  且类间的区别比较明显时, KMeans 效果较好, 同时处理大数据集的效果较好。

缺点: 
1. 随机选取中心点, 容易导致迭代次数大或者陷入局部最优
2. 必须事先给出 k 的个数
3. 对于异常数据是敏感的 

- K-Means算法没有训练样本数据，因此为无监督学习
- 影响: 初始中心，输入数据K的选择，距离的度量
- 改进：二分K-Means算法--> <mark style="background: transparent; color: red">避免收敛于局部的问题</mark>   划分的标准是最大限度降低SSE值(最小化误差平方和)

对于二分 KMeans 算法, 主要是将所有点作为一个簇, 并将簇一分为二， <mark style="background: transparent; color: red">选择能够最大限度降低聚类代价的函数(误差平方和)的簇划分为两个簇</mark>,不断进行直到簇的数量等于给定数目 $k$ 为止。每一次选择**误差平方和最大的簇进行划分**。

以下是一个**二分KMeans的示例算法**: 
每一次采用 KMeans(n_cluster=2), 每一次采用 sse_error 作为划分最大的簇的标准。
```python 
import numpy as np  
from sklearn.cluster import KMeans  
from sklearn.datasets import load_iris  
import matplotlib.pyplot as plt  
  
def sse_error(cluster):  
    center = np.mean(cluster, axis=0)  
    sse = np.sum((cluster - center) ** 2)  
    return sse  
  
def bisecting_kmeans(X, k):  
    clusters = [X]  
    while len(clusters) < k:  
        largest_cluster = max(clusters, key=sse_error)  # sse error  
        clusters.remove(largest_cluster)  
        kmeans = KMeans(n_clusters=2).fit(largest_cluster)  
        clusters.append(largest_cluster[kmeans.labels_ == 0])  
        clusters.append(largest_cluster[kmeans.labels_ == 1])  
    return clusters  
  
# 加载数据集  
iris = load_iris()  
X = iris.data  
  
# 设定簇的数量  
k = 3  
clusters = bisecting_kmeans(X, k)  
  
# 可视化结果  
colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']  
for i, cluster in enumerate(clusters):  
    plt.scatter(cluster[:, 0], cluster[:, 1], c=colors[i % len(colors)])  
plt.title("Bisecting KMeans Clustering")  
plt.show()
```
示例结果如下:
![[attachments/Pasted image 20240919110305.png]]
每个部分仍然采用 kmeans 随机生成聚类中心点, 最终得到相应的聚类结果。二分KMeans算法能够避免收敛于局部的问题。
