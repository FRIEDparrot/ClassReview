## 一、BP 神经网络理论
### (1) BP 神经网络基本简介
初始的神经网络模型是MP模型. 将人工神经元视为一个二值开关元件。并根据组合方式处理逻辑运算。
最早的有使用价值的神经网络模型是感知器模型（梯度下降也是感知器网络）。而后期发展的 RBF 神经网络模型可以容易地模拟出任何一种非线性数据的变化趋势。

BP 算法即多层前馈神经网络的学习算法。一般是利用外界的输入样本的次级， 并通过不断迭代修正权重向量，使网络输出与期望相符合。<mark style="background: transparent; color: red">一个基本的 BP 神经网络分为如下几个部分</mark>:
1. 输入层
2. 激活函数层 
3. 误差计算 
4. 输出层 
5. 迭代公式

下图说明了单个神经元和 BP 网络的基本结构，
![[Excalidraw/5. 神经网络理论初步 2024-09-29 15.49.56|600]]
输出层即输入上一层的权重和上一层部分的乘积，然后输出分类标签向量部分。

### (2) BP 网络的反向传播机制
<b><mark style="background: transparent; color: blue">和多层感知机不同, BP神经网络的每一都计算与期望结果的偏差, 并按照反向传播的方法将误差传递到上一层， 用于修正上一层的权重</mark></b>。
#### 1. 正向传播
正向传播过程中, 设输入为 $i$ 输出为 $o$, 隐藏层 $h$, 
$$\text{net} =  w^{T} o +b$$
其中, b 为阈值, 实际上也是在 $x$ 首列加入均为 1 的向量; 

实际上按照上图右侧的部分: i~h, h~o 中各有一系权 $w_{ih}, w_{ho}$, 是一个带有两个权重层的输入输出结构。并依次传递计算为:
$$\Large  \boxed{h_{i} =  w_{ih} x_{i} + b_{h} \quad h_{o} =f_{1}(hi), \quad yi = w_{ho} \times  h_{o} + b_{o}  \quad  y_{o} = f_{2}(y_{i})}$$
其中, 每一层的传递函数(激活函数)为:
$$f(x) = \frac{1}{1 + e^{-x}} = y$$
#### 2. 误差计算
在误差计算中, 首先会**计算网络实际输出 $y_{o}$ 和期望输出 $y_d$ 的差**。<mark style="background: transparent; color: red">并判断差是否低于容限</mark>, 如果高于容限，则进行误差反向传播。即取: 
1. 误差向量
$$\text{err} =d_{o}- y_{o}$$
2. 全局误差函数
我们在后面计算误差反向传播时, 实际上是最小化全局误差函数: 
$$e  = f_{\text{err}} = \frac{1}{2} \sum (d_{o} - y_{o})^{2}$$
#### 3. 反向传播
我们取 dlogit 函数为 sigmoid 函数的导函数:
$$f'(\text{net}) = \frac{1}{1 + e^{-\text{net}}} - \frac{1}{(1 + e^{-net})^{2}}  = y(1-y)$$
输出层的误差对于$w_{ho}$的微分可以写为:
$$\frac{\partial \text{e}}{\partial w_{ho}} = \frac{\partial \text{e}}{\partial y_{i}} \frac{\partial y_{i}}{\partial w_{ho}}$$
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/👨‍🎓深度学习算法原理(sklearn)/推导部分/BP 神经网络的反向传播计算.pdf|BP 神经网络的反向传播计算.pdf]]，可以获取到每一项的导数:
$$\Large\boxed{\frac{\partial e}{\partial w_{ho}} = - \delta_{0} h_{o}\qquad  \frac{\partial e}{\partial w_{ih}} =  - \delta_{h} x_{i}}$$
其中, 设 $\delta$ 为每一项的梯度, 则根据下式得到<b><mark style="background: transparent; color: blue"></mark></b>为:
$$\boxed{\frac{\partial e}{\partial h_{o}}  = - \delta_{o}\qquad  \delta_{o} = (d-y_{o}) f_{2}'(y_{i})}\tag{1}$$
$$\boxed{\frac{\partial e}{\partial h_{i}} =  - \delta_{h}\qquad  \delta_{h} = - \delta_{0} w_{ho} f_{1}'(h_{i})}\tag{2}$$
并得到每一层的权重迭代更新公式:
$$\boxed{\Large\begin{cases}
w_{ho} ^{N+1} = w_{ho}^{N} + \eta_{1} \cdot  \delta_{o} h_{o} \\
w_{ih}^{N+1} = w_{ih}^{N} +\eta_{2}  \cdot   \delta_{h} x_{i}
\end{cases}}$$
### (3) BP 网络的动量因子简介
除了学习率 $\eta$, 迭代次数 (max_iter) 和误差容限 (error_boundary), 往往我们还使用<mark style="background: transparent; color: red">动量因子</mark>来进行 BP 网络的参数调优。一般可以取为0.3左右; 

引入动量因子的好处: 最早的BP算法在修正权值时, 需要**按照迭代次数 t 的梯度进行调整**。而不考虑在 t-1 部分的梯度方向的影响, <mark style="background: transparent; color: red">导致网络发生震荡, 收敛缓慢等等</mark>, 而<mark style="background: transparent; color: red">动量因子考虑分配 t 时刻和 t-1 时刻的梯度</mark>; 此外，<u>没有附加动量因子时, 网络有可能陷入局部极小值。</u>

具体的分配方法是设动量因子为 $MC$ 并**记录上一次的梯度大小**:
$$w_{ih}^{N+1} = w_{ih} + (1 - MC)\times  \eta \left. \frac{\partial e}{\partial w_{ih}}\right|_{t} + MC \times \eta \left. \frac{\partial e}{\partial w_{ih}} \right|_{t =  t-1}$$

另外, 由于网络的初始权值是随机选取的, BP网络对于权重的初始值是非常敏感的,  导致迭代过程中误差函数的计算结果也不相同, 得到曲线也不相同。

误差信号反向传播计算过程中, 参考(1),(2); 首先计算 output 和 y_true 的差 err, 并根据 output = y, 可以直接采用 $f_{2}'(y) = y(1-y)$ 计算出输出层梯度, 然后直接采用 $\text{err}* f_{2}'(x)$ 直接得到 $\delta_{o}$，然后即可直接采用 $\delta_o * h$ 得到第一个梯度 $\frac{\partial e}{\partial w_{ho}}$ , 同时记录这个梯度以备动量因子使用。

对于学习速率的动态调节, 策略是<mark style="background: transparent; color: red">在迭代中检验权值是否导致误差函数值的降低; 如果降低, 则说明当前的学习速率可以增加, 则适当增加一个量; 如果不是, 则说明调整过度, 此时可以降低学习速率</mark>.

## 二、自组织特征映射神经网络(SOM)
