## 一、灰狼算法
### (1) 简介及优点
灰狼优化算法(Grey Wolf Optimization, GWO)是基于灰狼的捕食行为和协同机制提出的优化算法。

其基本优点包括结构简单, 调参少和容易实现等优点。同时可以平衡全局搜索和局部搜索。对于灰狼算法, 将狼群分为 $\alpha$ ,$\beta$ , $\delta$ , $\omega$ 四层, 而每一层的

![[Excalidraw/4. 灰狼优化算法(GWO) 2024-04-25 20.51.49|550]]
首先, 对于上述社会等级进行建模，可以将三匹代表最优解的狼定义为$\alpha,\beta,\gamma$, 而其余的狼定义为$\omega$，进行围绕搜索和位置更新。
而狼群的狩猎过程可以建模为: <mark style="background: transparent; color: red">包围, 跟踪 -> 追捕猎物和 -> 攻击猎物</mark> 三个过程

### (2) 优化算法过程
#### 1. 包围和跟踪模拟
首先在包围期间, 假设猎物的位置为 $X_p$, 灰狼的位置向量为 X, 由于事先实际上是不知道猎物的具体位置的, 则可以设猎物的真实位置为$C\vec{X}_{p}$, 即在对应位置所在的直线上。此时**灰狼和猎物的间距向量**可以计算为:
$$\vec{D} = C \cdot  \vec{X}_{p}(t)  - \vec{X}(t)$$
其中$C = 2 r_{2}$, 而$r_2$是范围在(0,1)以内的随机数(==附注:实际上是分量在(0,1)内的随机向量==); 
![[Excalidraw/4. 灰狼优化算法(GWO) 2024-04-25 21.31.06|350]]
而<mark style="background: transparent; color: red">灰狼每次按照如下公式进行包围和位置更新</mark>(A为系数, 一般是小于1的值)，即从不同方向逼近猎物, 显然下一次的位置可以计算为:
$$\vec{X} (t + 1) = \vec{X}_{p}(t) - A\cdot  \vec{D}$$
其中$\vec{A}$为随机矢量(实际上可以用随机数代替计算), 并且有:
$$A=  a (2 r_{1} -1) = a * \text{randrange}(-1,1)$$
其中$r_{1}$为范围为(0,1)的随机数, 
> [!abstract] 说明
> 上式中, $a$为<b><mark style="background: transparent; color: blue">收敛因子</mark></b>, 是用于模拟灰狼对猎物的包围和攻击行为的。显然, a越小, 则包围圈缩地越紧。
> $a$是决定全局搜索和局部搜索的关键性因素, 当$a$越大时, 包围圈较大, 利于全局搜索, 而$a$较小时包围圈小, 利于局部搜索。
> 因此我们总是一开始将a取一个较大的值(往往取2), 而之后随着迭代逐步减小。

#### 2. 迭代过程
在迭代过程中, ==每一次迭代分别取$\alpha$ , $\beta$, $\gamma$ 为三个潜在最优解==， 其余狼均为$\omega$层, 因此迭代过程中，先确定 $\alpha$, $\beta$ , $\delta$, 由于对应的"猎物"位置显然是不同的, 其余的狼分别$\alpha, \beta, \delta$移动, 并使用下式计算:

先算出任意一头狼与$\alpha$, $\beta$, $\gamma$ 距离
$$D_{\alpha}= C_{1} \vec{X}_{\alpha} - \vec{X}\qquad  D_{\beta}= C_{2} \vec{X}_{\beta} - \vec{X} \qquad  D_{\delta}= C_{3} \vec{X}_{\delta} - \vec{X}$$
类似上方可取 $C_{i} \in (0, 2)$, 然后分别计算<b><mark style="background: transparent; color: blue">受各个狼群影响时需要下一次调整到的位置</mark></b>:
$$\vec{X}_{1} = \vec{X}_{\alpha} - A_{1} \vec{D}_{\alpha} \qquad \vec{X}_{2}  = \vec{X}_{\beta} - A_{2}\vec{D}_{\beta}\qquad  \vec{X}_{3}= \vec{X}_{\delta} - A_{3}\vec{D}_{\delta}$$
由于我们是考虑三个狼群都影响$\omega$层狼的移动, 则最终的位置向量可以表示为平均:
$$\vec{X} (t + 1) =  \frac{\vec{X}_{1}+ \vec{X}_{2} + \vec{X}_{3}}{3}$$
依照此公式进行迭代即可获取到相应的最优解。

#### 3. 收敛因子的计算
一般取最大迭代次数为T, 则收敛因子由2递减到0, 对应的部分为(t迭代次数): 
$$\alpha = 2(1 -  \frac{t}{T})$$
## 二、编程示例
下面的代码演示了灰狼算法的过程: 

```python title:灰狼算法示例代码
# Grey Wolf Optimization Algorithm  
import matplotlib.pyplot as plt  
import  numpy as np  
  
num_GWO = 50   # 50 wolves  
max_epoch = 50  
x_range = [-3, 3]  
y_range = [-3, 3]  
  
def F(X1, X2):  
    return 3 * (1 - X1) ** 2 * np.exp(-(X1 ** 2) - (X2 + 1) ** 2) - 10 * (X1 / 5 - X1 ** 3 - X2 ** 5) * np.exp( \  
        - X1 ** 2 - X2 ** 2) - 1 / 3 ** np.exp(-(X1 + 1) ** 2 - X2 ** 2)  
  
# the lower the function value, the higher fitness  
def fitness(X):  
    return 8 - F(X[:,0], X[:,1])  
  
def plot_figure(fig, x_range, y_range, F, x, y):  
    ax = fig.add_subplot(1,1,1,projection='3d')  
    xx = np.arange(x_range[0], x_range[1], 0.1);  
    yy = np.arange(y_range[0], y_range[1], 0.1);  
    [X, Y] = np.meshgrid(xx, yy);  
    Z = F(X,Y)  
    ax.plot_surface(X,Y,Z, alpha = 0.8, cmap = 'rainbow')  
    ax.scatter(x, y, F(x, y), color='black', marker='^')  
    ax.view_init(45, 135 + 180)  
    plt.show(block = False)  
    plt.pause(0.3)  
    fig.clear()  
  
if __name__ == "__main__":  
    fig1 = plt.figure()  
    plt.ion()
    # initailize the population  
    xp = np.random.rand(num_GWO, 2) * [(x_range[1] - x_range[0]), (y_range[1] - y_range[0])] + [x_range[0], y_range[0]]    
    data_plot = np.zeros([max_epoch, 1])  

    for epoch in range (max_epoch):  
        data_plot[epoch, :] = np.min(F(xp[:, 0], xp[:, 1]), axis=0)  
        plot_figure(fig1, x_range, y_range, F, xp[:,0], xp[:,1])  
  
        yp = fitness(xp);  
        # use index for get the sort according to function "fitness"  
        fitness_index = np.argsort(yp)[::-1];             # reverse after sorting  
        xp_new = (xp[fitness_index, :])  # sort xp into sequence of fitness  
        XP_best = xp_new[0:3,:]          # obtain alpha, beta and delta  
  
        a = 2 * (1 - epoch / max_epoch)  
        # generate the coefficients  
        r1 = np.random.rand(3,1)  
        r2 = np.random.rand(3,1)  
        A = a * (2 * r1 - 1);   C = 2 * r2;  
  
        for i in range (3, num_GWO):  
            D = C * XP_best - np.repeat([xp_new[i,:]], 3, axis=0)          # calculate the D1, D2, D3  
            X = XP_best - A * D  
            xp_new[i,:] = np.mean(X,axis=0)    # move the wolf  
        xp = xp_new  
  
    plt.ioff()  
    fig2 = plt.figure()  
    ax = fig2.add_subplot(1,1,1)  
    ax.plot(range(max_epoch), data_plot)  
    plt.show()
```

从结果可以看到, 灰狼算法有极高的收敛效率, 其收敛图像如下:
![[attachments/Pasted image 20240426010732.png|400]]
