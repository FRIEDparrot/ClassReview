## 一、 大数定律
**概率收敛**的定义: 
设$f_A$是$n$次独立重复试验中事件$A$发生的次数， 而$p$是事件$A$在每次实验中发生的概率，则对于任意的正数$\varepsilon > 0$均有:
$$\lim_{n \rightarrow \infty} \left\{|Y_{n} - a| < \varepsilon \right\}  = 1$$
则称$Y_{n}$依概率收敛于$a$, 并记为$Y_{n}\overset{P}{\rightarrow} a$, 并有以下的性质: 
我们设$X_{n}\overset{P}{\rightarrow} a, Y_{n}\overset{P}{\rightarrow } b$, 则有$g(X_{n},Y_{n}) \overset{P}{\rightarrow} g(a,b)$

### (1) 切比雪夫大数定理
切比雪夫大数定律: 设$X_{1},  X_{2},\dots X_{n}$相互独立，服从统一分布的随机变量序列， 且**具有数学期望**$E(X_{k}) = \mu$(其中$k = 1,2 \dots$) 是前 $n$个变量的算数平均$\frac{1}{n}\sum^{n}_{k=1}X_{k}$,则对于任意的$\varepsilon> 0$ 均有： 
$$\lim_{n\rightarrow  + \infty} P\left\{\left|\frac{1}{n}\sum^{n}_{k=1} X_{k} - \mu \right| < \varepsilon \right\} = 1$$
### (2) 辛钦大数定律
辛钦大数定律: 我们设$X_1, X_2, \dots$相互独立, 服从同一分布且有<mark style="background: transparent; color: red">数学期望</mark>$E(X_{k}) = \mu$, 则序列
$$\overline{X} = \frac{1}{n} \sum^{n}_{k=1} X_{k} \overset{P}{\rightarrow }\mu$$
其中伯努利大数定律是辛钦大数定律的一个重要推论。

### (3) 伯努利大数定律
伯努利大数定律: 设$f_{A}$是$n$次独立重复试验中$A$发生的次数，而$p$是事件$A$在每次实验中发生的概率， 则对于任意的正数$\varepsilon > 0$, 均有: 
$$\lim_{n\rightarrow \infty} \left\{ \left|\frac{f_{A}}{n}  -p \right| < \varepsilon \right\} = 1\quad \text{or} \quad  \lim_{n \rightarrow \infty} \left\{\left| \frac{f_{A}}{n} - p \right|\geq \varepsilon \right\} = 0$$
伯努利大数定律表明，当实验次数很大时， 可以使用事件的频率代替事件的概率

## 二、中心极限定理
### (1) 独立同分布的中心极限定理
定理一(独立同分布的中心极限定理) 设随机变量 $X_{1}, X_{2},\dots X_{n}$相互独立且服从同一分布，且具有数学期望和方差$E(X_{k}) = \mu, D(X_{k}) = \sigma^{2}>0$, 则有随机变量之和$\sum^{n}_{k=1} X_{k}$的标准化变量: 
$$Y_{n} = \frac{\sum^{n}_{k=1}X_{k}  - E(\sum^{n}_{k=1} X_{k})}{\sqrt{D \left(\sum^{n}_{k=1} X_{k}\right)}} = \frac{\sum^{n}_{k=1} X_{k}-  n \mu}{\sqrt{n}\sigma}$$
的<mark style="background: transparent; color: red">分布函数</mark>$F_{n}(x)$对于任意$X$满足: 
$$\lim_{n \rightarrow \infty} F_{n} (x) = \lim_{n \rightarrow \infty} P\left\{Y_{n} \leq  x\right\} = \int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi}} e^{-\frac{t^{2}}{2}} dt = \Phi(x)$$
即对于均值为$\mu$, 方差为$\sigma^{2} >0$的独立同分布的随机变量$X_{1}, X_{2},  \dots X_{n}$之和$\sum^{n}_{k=1} X_{k}$的标准化变量，当$n$充分大时，有:
$$\frac{\sum^{n}_{k=1} X_{k} - n \mu}{\sqrt{n}\sigma} \sim N(0,1)$$
这一结果是数理统计中，大样本统计推断的基础。

### (2) 李亚普诺夫定理
定理二(李雅普诺夫Lyapunov定理) 设随机变量 $X_{1}, X_{2},\dots X_{n}$相互独立，且具有数学期望和方差$E(X_{k}) = \mu_{k}, D(X_{k}) = \sigma^{2}_{k}>0, k = 1,2 \dots$, 此时，我们记$B_{n}^{2} = \sum^{n}_{k=1} \mu_{k}$, 此时如果存在正数$\delta$, 使得当$n\rightarrow \infty$时, 有:
$$\frac{1}{B_{n}^{2 + \delta}} \sum^{n}_{k=1} E\left\{\left|X_{k} - \mu_{k} \right|^{2 + \delta}\right\} = 0$$
此时随机变量之和$\sum^{n}_{k=1} x_{k}$的标准化变量
$$Z_{n} = \frac{\sum^{n}_{k=1}  X_{k} - E \left(\sum^{n}_{k=1}  X_{k}\right) }{\sqrt{D\left(\sum^{n}_{k=1} X_{k}\right)}}= \frac{\sum^{n}_{k=1}  X_{k} - \sum^{n}_{k=1} \mu_{k}}{B_{n}}$$
的分布函数满足:
$$\lim_{n \rightarrow \infty} F_{n}(x) = \lim_{n \rightarrow \infty} P\left\{Z_{n} \leq  x\right\} = \int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi}} e^{-\frac{t^{2}}{2}} dt = \Phi(x)$$

### (3) 棣莫弗-拉普拉斯定理
设随机变量$\eta_{n}$服从参数为$n, p$的二项分布，则对于任意的$x$，均有: 
$$\lim_{n \rightarrow \infty} P\left\{ \frac{\eta_{n} - np}{np (1- p})\leq x\right\} = \int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi} } e^{- \frac{t^{2}}{2}} dt = \Phi(x)$$
即正态分布是二项分布的极限分布
