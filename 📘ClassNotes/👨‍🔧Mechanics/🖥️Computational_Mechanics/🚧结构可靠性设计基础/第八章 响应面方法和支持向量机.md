## 一、响应面法及其发展
为了发展一种通过少量运算 ， 得到在概率上能够代替真实隐式功能函数的显示函数,  基本思是通过一系列实验产生若干样本， 并使用拟合的显示函数代替隐式函数。从而保证显示函数得到的失效概率收敛于真实的隐式功能函数的失效概率。

**响应面法**是可靠性分析最早使用的代理模型方法。基本思想是**选定近似隐式功能函数的多项式**， 再通**过迭代实现响应面函数的失效概率对隐式功能函数的失效概率的高精度近似**。支持向量机原理参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/深度学习算法原理(sklearn)/补充知识/5.SVM支持向量机|5.SVM支持向量机]] 
> [!cite] 说明
> 响应面法着重于解决如下几方面的问题:
> 1. 响应面函数的形式选取
> 2. 实验样本点的抽取 
> 3. 响应面函数的拟合

### (1) 加权回归分析的基本原理
首先假设研究的隐式极限状态方程为$g(X) = 0$, 其中输入变量为$n$维, 即$X = (X_{1}, X_{2}, \dots X_{n})$， 此时， 为了求解该隐式极限状态方程的失效概率， 使用如下的==线性响应面函数==对应的g(x)来近似最终的极限状态函数:
$$\boxed{\overline{g}(x) = b_{0} + \sum^{n}_{i=1} b_{i}X_{i} = 0\tag{8.1.1.1}}$$
对于线性响应面函数，其中有n+1个待定系数,即$b_0, b_1, \dots b_n$, 此时，通过抽取m个样本点$x_i$来求解对应系数,<b><mark style="background: transparent; color: blue">求解系数使用最小二乘法</mark></b>, 有**下列公式成立**: 
$$\Large \boxed{b = (a^{T} a)^{-1}a^{T} Y}\tag{8.1.1.2}$$
其中有: $Y$为响应列阵, **而a为由m个实验点构成的$(m \times n+1)$响应量列阵**。
$$a = \left[\begin{matrix}
1 & x_{11} & \dots &  x_{1n} \\ 
1 & x_{21} & \dots & x_{2n}  \\ 
\vdots   \\ 
1 & x_{m1} & \dots &   x_{mn}
\end{matrix}\right]$$
由于 响应面可靠性分析方法中， 最关键目标是使用$\overline{g}(x) = 0$近似$g(x) = 0$, 我们在响应面中, <b><mark style="background: transparent; color: blue">使用加权回归的思想求解待定系数向量b</mark></b>, 此时, 我们取 $\overline{g}(x)$ 较小的点并赋给较大的权值$w$。此时，可以使$|g(x_{i})|$较小的点在确定$\overline{g}(X)$中获得更大的权重。
而待定系数向量 $b$ 可以使用下式求得, 其中$W$是大小为 $m \times  m$ 的 权重矩阵<b><mark style="background: transparent; color: blue">(原理是最小二乘方法)</mark></b>， 具体推导参考[最小二乘法详细推导](https://blog.csdn.net/MoreAction_/article/details/106443383): 
$$\Large\boxed{b = \frac{a^{T} W Y}{a^{T} W a}}\tag{8.1.1.3}$$
上面是 n + 1行 1 列的向量, 下面是 n + 1行, n + 1列的矩阵(实际运算时， 下面求逆乘上面)

### (2) 权重矩阵的构造方式
**拟合响应面**过程中，我们可以按照下式取每个每个试验点权重:
$$\Large\boxed{w_{i} = \frac{\min (|g(x_{i})| + \delta) }{|g(x_{i})| + \delta}  \qquad (i = 1, 2,\dots m)}\tag{8.1.2.1}$$
设有m个数据点， 则只需将权重按对角排列即可形成 $m\times m$大小的权重矩阵:
$$W = \text{diag}(w_{i})$$
其中 $\delta$ 可以设置为一个小的数($10^{-3}$可取),目的是避免实验点权重系数出现零值。通过上式可以看出, <mark style="background: transparent; color: red">对于所有的点</mark>, 实际上是以$g(x)$绝对值加权的， 且离面最近的点权值为1. 

如果按照上述方法构造权重矩阵,   则称为**线性加权响应面法**(WRSM); 
另外也可以直接赋值相同的权， 即取全部对角元素恒定为1， 非对角元素恒定为0。此时称为传统线性响应面方法(TLRSM)

### (3) 加权线性响应面选取的基本步骤
1. n个变量, 我们首先初始化中心点$x_{i} = \mu$,  选用线性的响应面函数 $\overline{g}(x) = b_0 + \sum^{n}_{i=1} b_i X_i$,  在第一次迭代时<b>使用Buncher<mark style="background: transparent; color: red">选取另外2n个实验点</mark></b>组合成试验点矩阵如下:
> [!NOTE] Buncher  试验点选取方法
> 以抽样中心点为均值点, 每个点按照f更改一个对应的坐标值。设输入变量维数为n,  新选2n个试验点分别选取为
> $$(x_{i1} + \sigma ,x_{i2} ,  \dots x_{in})\qquad  (x_{i1} - \sigma ,x_{i2} ,  \dots x_{in})$$
> $$(x_{i1} ,x_{i2} + \sigma,  \dots x_{in})\qquad  (x_{i1}, x_{i2} - \sigma ,  \dots x_{in})$$
> 以此类推。其中加的sigma可以乘一个系数值f， 称为插值系数。
> 最终获得 $(x_{i1}, x_{i2}, \dots x_{i2n+1})$ 共2n +1个设计点。
2.  分别调用$g(x)$求解在这些设计点处的响应值。 参考[[#(2) 权重矩阵的构造方式]]构造实验点的权重矩阵$W$, 并**使用加权最小二乘回归的方法, 即参考式(8.1.1.3), 确定待定系数向量b**; 并由试验点使用，获取k次迭代的待定系数向量b,  即可构造出初始的响应面函数$\overline{g}^{(k)}(x)$;  
3. 参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/🚧结构可靠性设计基础/第三章 可靠性灵敏度分析的矩方法#(3) 改进 FOSM 可靠性分析方法|AFOSM方法]],  使用<mark style="background: transparent; color: red">改进的一次二阶矩法</mark>求得$\overline{g}^{(k)}$的==设计点 $x_{D}^{(k)} = (x_{D1}^{(k)}, x_{D2}^{(k)}, \dots x_{Dn}^{(k)})$ 以及对应的可靠度指标== $\beta^{(k)}$, 需要注意的是, **使用改进一次二阶矩方法时， 只需将预测的响应面函数$g'(x)$代入AFOSM中代替原响应函数即可， 不使用原先的g(x)求解偏导数**: 
4. 设**使用AFOSM求解得到的设计点**为$(x_D^{(k)}, g(x_D^{(k)}))$,  而初始样本点取均值 $(\mu_{X}, g(\mu_{X}))$; 则插值公式如下: ($g(x_D^{(k)}$也可以使用$\overline{g}(x_D^{(k)}$预测函数值来替代， 调用次数更少)。 此时， 新的中心点计算为插值:
$$\Large\boxed{x_{i}^{(k+1)} = \mu_{X_{i}} + (x_{Di}^{(k)}- \mu_{X_{i}}) \frac{g(\mu_{X})}{g(\mu_{X}) - g_{RS}(x_{D}^{(k)})}}\tag{8.1.3.1}$$
通过下式可以看出，插值结果近似在响应面上， 即近似有 $g(x_i^{(k+1)}) \approx 0$ 
![[Excalidraw/第八章 响应面方法和支持向量机 2024-03-28 10.49.35|450]]
获取下一次计算的 $x_{1i}$ 坐标,  然后重复第1步,  直到**两次可靠度指标的计算差值小于预先给的精度指标**, 认为结果收敛。

> [!caution] 注意
> 需要说明的是，直接使用AFOSM迭代只能获到正确的模型， 但是获取不到正确的Pf, 正确的Pf仍然是利用模型进行MCS模拟获取的。

代码示例如下:
```matlab title:线性响应面法代码示例
%% %%%%%%%%%%%% 算例 8.1 响应面法求解可靠性分析的可靠度 %%%%%%%%%%%%%%%%%%% 
% 对于指数型功能函数为g(X) = e^(0.2 x1 + 1.4)- x2, 其中x1, x2为标准正态分布
clear, clc;
g = @(x) exp(0.2 .* x(:,1) + 1.4) - x(:,2);
mu_ = [0,0];
sigma_d = [1,1];
sigma_ = diag(sigma_d.^2);

% [Pf, Pf_mu, Pf_sigma] = MCS_solu(mu_, sigma_,g, 1e7);
% [x_i, beta_res, Pf2] = AFOSM_solu(mu_,sigma_, g);

%% %%%%%%%%%%%% 线性加权响应面方法进行的可靠性分析 %%%%%%%%%%%%%%%%%%%%%%
n = size(mu_, 2);       
f = 1;                  % 设置插值系数, 即选取点距离的sigma差倍数, 默认为1

%% %%%%%%%%%%%%%%%%%%% 利用计算权重 %%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
weight_func = @(gx) min(abs(gx) + 1e-3)./(abs(gx) + 1e-3);   % WRSM权重函数, g越接近0越大
% weight_func = @(gx) ones(length(x), 1);  % TWRSM权重函数

beta_pre = 0;              % 初始迭代beta值
x0 = mu_;  y0 = g(mu_);    % 用于插值的初始样本点
xp = zeros(2 * n + 1, n);  % 设计点组
x_i = mu_;                 % 初始时, 设计点组以mu_ 为中心点;

for epoch = 1:1000
    % 第一次迭代 (buncher设计点选择方法), 选用新的2n个设计点 
    xp(1,:) = x_i;
    for i = 1:n 
        f_delta = zeros(1,n); f_delta(i) = sigma_d(i) * f;
        xp(2 * i,:) = x_i + f_delta;
        xp(2*i+1,:) = x_i - f_delta;
    end
    m = length(xp);  % 构造以下矩阵
    Y = g(xp);  % 此处调用原始函数g求解所有点的Y (m  * 1);
    W = diag(weight_func(Y));               % (m *  m);
    A = [ones(m,1), xp];                    % (m *n+1);

    b = ((A' * W * A) \ (A' * W * Y))';  % 获取待定系数向量 (n+1 * 1)
    % 对应的响应面函数为b0 + b1 x1 + b2 x2 + ... + bn xn,
    % 只需将预测响应面函数代入AFOSM中代替原响应函数即可
    
    g_new = @(x) b(1) + b(2) * x(:,1) + b(3) * x(:,2);
    % 用 AFOSM 求解设计点的可靠度指标 beta^k 以及设计点
    [x_i, beta_res, ~] = AFOSM_solu(mu_, sigma_, g_new); 
    
    if abs(beta_res - beta_pre) < 0.001
        break;
    else  % 进行插值,  更新设计点
        x_i = mu_ +  (x_i - x0) * y0 / (y0 - g_new(x_i));
        sprintf("epoch: %d, Pf: %f, beta : %f", epoch, Pf, beta_res)
        beta_pre = beta_res;
    end
end
%% %%%%%%%%%%%%%%%% 使用MCS方法, 计算响应面失效概率  %%%%%%%%%%%%%%%%%%
num_MCS = 5e6;
xpp = lhsnorm(mu_, sigma_, num_MCS,"on");
fail_points = find(g_new(xpp) < 0);
Pf = size(fail_points,1)/num_MCS;
```

## 二、加权非线性响应面法
### (1) 基本原理
由于加权线性响应面法不能反映非线性对于可靠性的影响，可以利用二次函数最小二乘方法，二次非线性响应面模型如下:
$$\boxed{\Large \overline{g}(X) = b_{0} + \sum^{n}_{i=1} b_{i}X_{i} + \sum^{n}_{i=1}  c_{i}X_{i-n}^{2}}\tag{8.2.1.1}$$
其中$b_{i}, c_{i}$为待定系数。上式是一种不含交叉项的二次响应面模型，并可以合写为:
$$\left\{ b_{i}, c_{i}\right\} = (b_{0}, b_{1}, \dots b_{2n})$$

> [!cite] 说明
> 参考[[#(3) 加权线性响应面选取的基本步骤]],  由于响应面法每一次会抛弃原先的试验点， 而建立新的试验点。往往导致计算效率降低。 我们考虑在最后几次迭代重复利用试验点,  减少新增的试验点个数, 来减少计算量， 从而提升计算效率。
### (2) 选取响应面策略与权函数的构造
方法1: 与[[#(2) 权重矩阵的构造方式|传统权重矩阵]]相同, 功能函数分式形权设置
$$\Large w_{i} = \frac{\min (|g(x_{i})| + \delta) }{|g(x_{i})| + \delta}  \qquad (i = 1, 2,\dots m)\tag{8.2.2.1}$$
方法2 : 取<mark style="background: transparent; color: red">功能函数与密度函数的比值型权数</mark>, 即密度越大, $|g(x)|$ 越小,  $w_{i}$也越大
$$\Large\begin{cases}
h_{i} = \left|\frac{g(x_{i}) + \delta}{f_{X}(x_{i})} \right|  \\
w_{i} = \frac{\min_{i = 1}^{m} h_{i} }{h_{i}}
\end{cases}\tag{8.2.2.2}$$

### (3) 加权非线性响应面方法的基本步骤
参考[[#(3) 加权线性响应面选取的基本步骤]] 
1. 初始先以<mark style="background: transparent; color: red">传统响应面法</mark>确定$\overline{g}^{(1)}(X)$设计点和初始的(2n+1)个设计点组$x_D^{(1)}$ 
2. 第$k$次迭代时,  按照相同方法，从$k-1$次的设计点$(x_D^{(k-1)}, g(x_D^{(k-1)}))$与均值点$(\mu_X, g(\mu_X))$ 插值得到中心点(插值得到的点是$g(x_{1}^{*(k)}) \approx 0$的点，因此仍然使用:
$$\Large x_{i}^{(k+1)} = \mu_{X_{i}} + (x_{Di}^{(k)}- \mu_{X_{i}}) \frac{g(\mu_{X})}{g(\mu_{X}) - g(x_{D}^{(k)})}$$
需要说明: 对于一次插值， 共有m个实验点, 而对应的列为n+1列， 而在非线性响应面中应当有 2n +1 列, 对应地， 仍然有
$$b = \frac{a^{T} W Y}{a^{T} W a}$$
其中
$$a = \left[\begin{matrix}
1 & x_{11} & x_{12} & \dots &  x_{11}^{2} & \dots & x_{1n}^{2}  \\ 
1 & x_{21} & x_{22} & \dots &  x_{21}^{2} & \dots & x_{2n}^{2}  \\ 
\dots  \\ 
1 & x_{m1} & x_{m2} &  \dots  & x_{m1}^{2} & \dots  & x_{mn}^{2}
\end{matrix}\right]$$
W 为 $m * m$ 矩阵; 
```matlab fold title:加权二次响应面法示例代码
%% %%%%%%%%%%%%%%%%%%% 加权二次响应面法 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
clear, clc;
mu_     = [0, 0, 0];
sigma_d = [1, 1, 1];
sigma_ = diag(sigma_d.^2);
g = @(x) 1./40 .* x(:,1).^4 + 2 .* x(:,2).^2 + x(:,3)+ 3;

% [x_i, beta_res, Pf] = AFOSM_solu(mu_, sigma_, g);  % 直接使用 AFOSM 会导致非线性产生较大误差
% [Pf, Pf_mu, Pf_sigma] = MCS_solu(mu_, sigma_,g, 1e7);   % 3.09e-4  & 0.0000	-0.0000	-0.0010 
%% %%%%%%%%%%%%%%%%%%% 加权非线性响应面方法求解部分 %%%%%%%%%%%%%%%%%%% 
n = size(mu_, 2);
w = @(gx) min(abs(gx) + 1e-3)./(abs(gx) + 1e-3);   % 这个是w1
%fx = @(xp) prod(normpdf(xp, mu_, sigma_d), 2);  % 计算联合概率密度函数, 查一下源码
%w = @(xp, gx) min((abs(gx) + 1e-3)./fx) ./ ((abs(gx) + 1e-3)./fx);

f = 1;                   % 插值系数
beta_pre = 1e-3;         % 初始值,设置为 0
x_i = mu_; x0 = mu_; y0 = g(mu_); 

for epoch = 1:1000
    xp = buncher_sample(x_i, sigma_d, "f", f);  % 建立样本点(插值系数选用1)
    y = g(xp);  m = length(y);            % 获取真实功能函数值 
    W = diag(w(y));                            % 权重矩阵
    A = [ones(m,1), xp, xp.^2];           % 构造矩阵A
    b = ((A' * W * A) \ (A' * W  * y))';  % 按照原始公式最小二乘求解b

    % 使用b获取新的预测函数值
    g_new = @(x) b(1) + sum(b(2:n + 1) .* x, 2) + sum(b(n+2: 2*n+1).* x.^2, 2);
    
    [x_i, beta_res, ~] = AFOSM_solu(mu_, sigma_, g_new);
    
    if abs((beta_res - beta_pre)/beta_pre) < 0.001
        break
    else
        x_i = mu_ + (x_i - mu_) .* (y0)./(y0 - g_new(x_i));
        sprintf("epoch: %d, beta: %f", epoch, beta_res)
        beta_pre = beta_res;
    end
end

%% %%%%%%%%%%%%%%%% 使用MCS方法, 计算响应面失效概率  %%%%%%%%%%%%%%%%%%
num_MCS = 5e6;
xpp = lhsnorm(mu_, sigma_, num_MCS,"on");
fail_points = find(g_new(xpp) < 0);
Pf = size(fail_points,1)/num_MCS;
```

```matlab title:buncher_sample.m fold
% @brief buncher 抽样函数
function xp = buncher_sample(x_i, sigma_d, varargin)
%buncher_sample buncher 抽样, 以点x_i为中心, 获取 2n+1 个buncher设计点
%f 插值系数, 默认为1
    p = inputParser;
    validScalarPosNum = @(x) isnumeric(x) && isscalar(x) && (x > 0);   % 检查输入的部分是不是正整数
    addRequired (p,'x_i');      % 占据第一个位置
    addRequired (p,'sigma_d');  % 占据第二个位置 
    addOptional(p, 'f', 1, validScalarPosNum);  % 添加可选参数(插值系数f)
    parse(p, x_i, sigma_d ,varargin{:});          % 匹配输入参数, 同时进行检查

    f = p.Results.f;            % 获取对应的参数

    n = size(x_i,2);
    xp = zeros(2 * n + 1, n);   % 设计点组
    xp(1,:) = x_i;
    for i = 1:length(x_i)
        f_delta = zeros(1,n); f_delta(i) = sigma_d(i) * f;
        xp(2 * i,:) = x_i + f_delta;
        xp(2*i+1,:) = x_i - f_delta;
    end
end
```
## 三、支持向量机相关概念
### (1) VC维的概念
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/深度学习算法原理(sklearn)/补充知识/5.SVM支持向量机|5.SVM支持向量机]], 响应面方法虽然是基于最小二乘的一种多项式系数求解方法， 但是<mark style="background: transparent; color: red">对于高度非线性问题，响应面法的计算量将急剧增加</mark>。因而往往仅适用于非线性程度不高的问题。

首先， 对于一个二分类问题， 如果存在$m$个样本, 可以将函数即中的函数按照所有可能的$2^m$种形式分开， 则称这个分类器所能够最大样本数目$m$ 为函数集的 VC 维。

例如, 显然对于3个点, 线性分类器必然可将其以$2^{3}$种形式全部分开，因此线性分类器的VC维至少是3。

### (2) 经验风险
首先假设变量X与变量y之间具有某种依赖关系， 且关系是未知的。此时需要依据训练样本$(x_1, y_1), (x_2, y_2), .... (x_{l}, y_{l})$选择最佳权值向量w的函数对依赖关系进行估计。<mark style="background: transparent; color: red">最佳逼近</mark>$f(x, w_0)$的选择原则一般是使得经验风险(empirical risk)R(w)最小:

**期望风险**的表达式如下:
$$R(w) = \int \mathcal{L} (y, f(x,w)) dP(\boldsymbol{x},y)\tag{8.3.2.1}$$
其中，损失函数为$\mathcal{L}(y, f(x,w))$, 而P(x,y)是常用的平方型损失函数如下:
$$\mathcal{L}(y, f(x,w)) = (y -  f(x,w))^{2}$$
由于P(x,y)无法求解，因此我们使用经验风险代替期望风险，<mark style="background: transparent; color: red">经验风险表达式为</mark>:
$$R_{emp} (w) =  \frac{1}{l}\sum^{l}_{i=1} \mathcal{L} (y_{i}, f(x_{i},w))\tag{8.3.2.2}$$
可以看出, 平方型损失函数的经验风险即预测值差值的方差。
> [!NOTE] 说明
> 目前的许多算法是在经验风险最小化的基础上提出的。但是往往模型过于复杂时会导致过学习的情况发生。

### (3) 结构风险(实际风险)
实际风险$R(w)$和经验风险$R_{emp}(w)$之间，至少以 $1-\eta$ ($\eta \in [0,1]$)的概率存在如下关系:
$$R(w) \leq  R_{emp}(w) + \sqrt{\left| \frac{h \left(\ln \frac{2l}{h} + 1\right) - \ln (\eta / 4)}{l}\right|}$$
其中, $h$为对应**逼近函数的VC维数, $l$为样本数**; 另外需要说明的是, <mark style="background: transparent; color: red">根号这一项表示置信范围</mark>, 因而<mark style="background: transparent; color: red">结构风险实际上是经验风险和置信范围之和</mark>。
上述式子可以简化为:
$$R(w) =  R_{emp} (w) + \Omega\left(\frac{l}{h}\right)$$
一般地, 当 $l/h > 20$ 时, 置信范围 $\Omega$ 的值很经验小, 可以忽略。此时<mark style="background: transparent; color: red">经验风险接近实际风险</mark>。**当所选函数集的VC维过大(分类器非线性程度高)时**, 置信范围必须相应增大，即$h$越大，往往会导致过拟合现象发生。 而**所选函数集VC维过低时, 往往会导致欠学习现象**, 导致经验风险$R_{emp}(w)$增大。
![[Excalidraw/第八章 响应面方法和支持向量机 2024-03-29 15.16.13|700]]

为了让整个结构风险达到最小化， 我们往往需要确定$R_{emp}$和$\Omega$和最小的点。而**支持向量机则是以结构风险最小化为优化目标的一种方法**。

统计学习理论提供了一种一般性的<b><mark style="background: transparent; color: blue">结构风险最小化原则</mark></b>, 方法是将函数集构造成子集$S_{1} \subset S_{2} \subset  \dots \subset  S_{n}$, 并且**要求对应的h分别按照从小到大进行排序**。此时, **只需要在子集之间折中考虑VC维和经验风险**，使得实际风险的最小化即可。即选择$S_{k}$**使得对应的置信风险和经验风险之和最小**即可。

### (4) 对偶优化

其中第一个为<mark style="background: transparent; color: red">拉格朗日函数极值条件， 第二个为互补松弛条件</mark>，第三个为原始约束条件, 第四个为拉格朗日系数约束。
## 四、支持向量机分类和回归算法(SVM)
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/深度学习算法原理(sklearn)/补充知识/5.SVM支持向量机#四、非线性支持向量机与带有松弛变量的SVM方法|非线性支持向量机与带有松弛变量的SVM方法]]以及[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/深度学习算法原理(sklearn)/补充知识/5.SVM支持向量机#五、支持向量机回归算法|支持向量机回归算法]]， 有非常详细的讲解, 此处不进行叙述。

SVM的留一法交叉验证误差参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/深度学习算法原理(sklearn)/补充知识/5.SVM支持向量机#六、交叉验证误差#(1) 留一法交叉验证误差|留一法交叉验证误差]] 

需要说明的是, 一般**回归算法的可靠性分析效率大于分类算法, 主要是由于分类信息仅使用正负号的结果, 而回归方法也利用了不同样本的距离数据, 因而分类效率更高**。

我们首先使用UQLab工具箱进行计算, 文档见[uqlab.com/user-manuals](https://www.uqlab.com/user-manuals)， 另外在OneDrive > Software Tutorial 中也有现成的文档供参考, 此处仅给出算例和代码部分:

```matlab title:常用的核函数
'Linear-NS'
'Polynomial'
'Sigmoid'
'Linear'
'Exponential'
'Gaussian'
'Matern-3_2'
'Matern-5_2'
```

我们可以先进行直接抽样， 计算每一个点的 g 值，再根据事先抽样得到的X,Y建立 SVM 回归模型

需要说明的是, 如果核函数选择不同，甚至可能会导致错误， 在此处我们选择polynomial 为 SVM 分类模型的核函数;
如果出错可能考虑更换核函数和清除 MetaOpts 的问题

下面的代码使用分类和回归进行了建模, 但是效果并非很好(回归建议减少点)

```matlab fold title:SVM方法示例代码
%% %%%%%%%%%%%%% Support Vector Machine %%%%%%%%%%%%% 
clearvars, clc;
%% %%%%%%%%%%%% 非线性单自由度无阻尼振动系统的SVM建模 %%%%%%% 
%mu_ = [1, 1, 0.1 , 0.5, 1, 1];
%sigma_d = [0.05, 0.1, 0.01, 0.05, 0.2, 0.2];
%omega_0 = @(x) sqrt((x(:,2) + x(:,3)) ./  x(:,1));
%g = @(x) 3 .* x(:,4) - abs( 2.* X(:,5) ./ (x(:,1).* omega_0(x).^2) .* sin(omega_0(x).^2 .* x(:,6)./2));

%% %%%%%%%%%%%% 两杆支撑系统的可靠性分析与SVM建模 %%%%%%%%%%% 
mu_ =    [200e6, 47.75e3, 100e-3, 100e-3,   0.03,  0.018, pi/3];
sigma_d = [20e6, 3.90e3,   5e-3,   5e-3, 1.5e-3, 0.9e-3, 1.15 * pi ./ 180];
g = @(x) x(:,1) - 2 .* x (:,2) .* sqrt(x(:,3).^2 + (x(:,4)./2).^2) ./(pi .* (x(:,5).^2 - x(:,6).^2)) .* (sin(x(:,7 ))./x(:,3) + 2 .* cos(x(:,7))./x(:,4));
sigma_ = diag(sigma_d.^2);

num_SVM = 1000;

Xtrain= lhsnorm(mu_, sigma_, num_SVM);
Ytrain = sign(g(Xtrain) + eps);
Xval = lhsnorm(mu_, sigma_, 1500);
Yval = g(Xval);

% %%%%%%%%%%%%%%% create the SVC metamodel %%%%%%%%%%%%%%%
rng(1,'twister')
uqlab

%% %%%%%%%%%%%%% 支持向量分类模型预测方法 %%%%%%%%%%%%%%%%%%
MetaOpts.Type = 'Metamodel';
MetaOpts.MetaType = 'SVC';               % use this for build the SVC metamodel
MetaOpts.ExpDesign.X = Xtrain;          
MetaOpts.ExpDesign.Y = Ytrain;

MetaOpts.ValidationSet.X = Xval;
MetaOpts.ValidationSet.Y = Yval;

MetaOpts.Kernel.Family = 'Gaussian';   % Select the Gaussian kernel family:
MetaOpts.EstimMethod   = 'CV';           % Cross-Validation
MetaOpts.Optim.Method  = 'GS';           % Use the cross Enthalpy for the optimization;
MetaOpts.QPSolver      = 'SMO';          % SMO 序列最小优化算法
MetaOpts.Penalization  = 'linear';       % Use the span leave-one-out (LOO)
%%%%%%%%%%%%%% 建立分类代理模型 %%%%%%%%%%%%%%%%%%%% 
% --------- 生成新的样本, 使用模型进行预测----------
model = uq_createModel(MetaOpts);
X_new = lhsnorm(mu_, sigma_, 1e6);
[Y_class, Y_new] = uq_evalModel(model, X_new);
Pf = size(find(Y_new < 0), 1) ./ 1e6;
uq_print(model);

fprintf("Pf for SVC: %f\n", Pf);
clear MetaOpts Pf Y_class Y_new
%% %%%%%%%%%%%%% 支持向量回归模型预测方法 %%%%%%%%%%%%%%%%%%% 
MetaOpts.Type = 'Metamodel';              
MetaOpts.MetaType = 'SVR';                % use this for build the SVC metamodel
MetaOpts.ExpDesign.X = Xtrain;
MetaOpts.ExpDesign.Y = g(Xtrain);

% ----- following are optional --------------------
% Select a linear penalization: 
MetaOpts.Kernel.Family = 'Gaussian';     % Select the Polynomial kernel family-> note: ** select Gaussian may error!
MetaOpts.EstimMethod   = 'CV';           % Also can use 'CV' as cross validation 
MetaOpts.Optim.Method  = 'GA';           % Use the cross Enthalpy for the optimization;
MetaOpts.QPSolver      = 'SMO';          % SMO 序列最小优化算法
MetaOpts.Loss          = 'l1-eps';       % Loss-Function Type;

model = uq_createModel(MetaOpts);
uq_print(model)    % show the basic infomation of the model builded
%%%%%%%%%%%%%% 建立回归代理模型 %%%%%%%%%%%%%%%%%%%% 
% --------- 生成新的样本, 使用模型进行---------- 
X_new = lhsnorm(mu_, sigma_, 1e6);
Y_new = uq_evalModel(model, X_new);
Pf = size(find(Y_new < 0), 1) ./ 1e6;
fprintf("Pf for SVR: %f\n", Pf);

Pf_mcs = MCS_solu(mu_, sigma_, g, 5e6);
fprintf("Pf for MCS: %f\n", Pf_mcs);
```
