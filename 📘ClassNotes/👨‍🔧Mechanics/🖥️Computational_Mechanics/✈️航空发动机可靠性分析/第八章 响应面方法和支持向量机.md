## 一、响应面法及其发展
为了发展一种通过少量运算 ， 得到在概率上能够代替真实隐式功能函数的显示函数,  基本思是通过一系列实验产生若干样本， 并使用拟合的显示函数代替隐式函数。从而保证显示函数得到的失效概率收敛于真实的隐式功能函数的失效概率。

**响应面法**是可靠性分析最早使用的代理模型方法。基本思想是**选定近似隐式功能函数的多项式**， 再通**过迭代实现响应面函数的失效概率对隐式功能函数的失效概率的高精度近似**。支持向量机原理参考 [[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/Class Contents/基本课程/2.SVM支持向量机|2.SVM支持向量机]] 
> [!cite] 说明
> 响应面法着重于解决如下几方面的问题:
> 1. 响应面函数的形式选取
> 2. 实验样本点的抽取 
> 3. 响应面函数的拟合

### (1) 加权回归分析的基本原理
首先假设研究的隐式极限状态方程为$g(X) = 0$, 其中输入变量为$n$维, 即$X = (X_{1}, X_{2}, \dots X_{n})$， 此时， 为了求解该隐式极限状态方程的失效概率， 使用如下的==线性响应面函数==对应的g(x)来近似最终的极限状态函数 :
$$\boxed{\overline{g}(x) = b_{0} + \sum^{n}_{i=1} b_{i}X_{i} = 0\tag{8.1.1.1}}$$
对于线性响应面函数，其中有n+1个待定系数,即$b_0, b_1, \dots b_n$, 此时，通过抽取m个样本点$x_i$来求解对应系数,<b><mark style="background: transparent; color: blue">求解系数使用最小二乘法</mark></b>, 有**下列公式成立**:
$$\Large \boxed{b = (a^{T} a)^{-1}a^{T} Y}\tag{8.1.1.2}$$
其中有: $Y$为响应列阵, **而a为由m个实验点构成的$(m \times n+1)$响应量列阵**。
$$a = \left[\begin{matrix}
1 & x_{11} & \dots &  x_{1n} \\ 
1 & x_{21} & \dots & x_{2n}  \\ 
\vdots   \\ 
1 & x_{m1} & \dots &   x_{mn}
\end{matrix}\right]$$
由于 响应面可靠性分析方法中， 最关键目标是使用$\overline{g}(x) = 0$近似g(x) = 0, 我们在响应面中, <b><mark style="background: transparent; color: blue">使用加权回归的思想求解待定系数向量b</mark></b>, 此时, 我们取 $\overline{g}(x)$ 较小的点并赋给较大的权值$w$。此时，可以使$|g(x_{i})|$较小的点在确定$\overline{g}(X)$中获得更大的权重。
而待定系数向量 $b$ 可以使用下式求得, 其中$W$是大小为 $m \times  m$ 的 权重矩阵<b><mark style="background: transparent; color: blue">(原理是最小二乘方法)</mark></b>， 具体推导参考[最小二乘法详细推导](https://blog.csdn.net/MoreAction_/article/details/106443383): 
$$\Large\boxed{b = \frac{a^{T} W Y}{a^{T} W a}}\tag{8.1.1.3}$$
上面是 n + 1行 1 列的向量, 下面是 n + 1行, n + 1列的矩阵(实际运算时， 下面求逆乘上面)

### (2) 权重矩阵的构造方式
**拟合响应面**过程中，我们可以按照下式取每个每个试验点权重:
$$\Large\boxed{w_{i} = \frac{\min (|g(x_{i})| + \delta) }{|g(x_{i})| + \delta}  \qquad (i = 1, 2,\dots m)}\tag{8.1.2.1}$$
设有m个数据点， 则只需将权重按对角排列即可形成 $m\times m$大小的权重矩阵: 
$$W = \text{diag}(w_{i})$$
其中 $\delta$ 可以设置为一个小的数($10^{-3}$可取),目的是避免实验点权重系数出现零值。通过上式可以看出, <mark style="background: transparent; color: red">对于所有的点</mark>, 实际上是以$g(x)$绝对值加权的， 且离面最近的点权值为1. 

如果按照上述方法构造权重矩阵,   则称为**线性加权响应面法**(WRSM); 
另外也可以直接赋值相同的权， 即取全部对角元素恒定为1， 非对角元素恒定为0。此时称为传统线性响应面方法(TLRSM)

### (3) 加权线性响应面选取的基本步骤
1. n个变量,   我们首先初始化中心点$x_{i} = \mu$,  选用线性的响应面函数 $\overline{g}(x) = b_0 + \sum^{n}_{i=1} b_i X_i$,  在第一次迭代时<b>使用Buncher<mark style="background: transparent; color: red">选取另外2n个实验点</mark></b>组合成试验点矩阵如下 : 
> [!NOTE] Buncher  试验点选取方法
> 以抽样中心点为均值点, 每个点按照f更改一个对应的坐标值。设输入变量维数为n,  新选2n个试验点分别选取为
> $$(x_{i1} + \sigma ,x_{i2} ,  \dots x_{in})\qquad  (x_{i1} - \sigma ,x_{i2} ,  \dots x_{in})$$
> $$(x_{i1} ,x_{i2} + \sigma,  \dots x_{in})\qquad  (x_{i1}, x_{i2} - \sigma ,  \dots x_{in})$$
> 以此类推。其中加的sigma可以乘一个系数值f， 称为插值系数。
> 最终获得 $(x_{i1}, x_{i2}, \dots x_{i2n+1})$ 共2n +1个设计点。

2.  分别调用g(x)求解在这些设计点处的响应值。 参考[[#(2) 权重矩阵的构造方式]]构造实验点的权重矩阵$W$, 并**使用加权最小二乘回归的方法, 即参考式(8.1.1.3), 确定待定系数向量b**; 并由试验点使用，获取k次迭代的待定系数向量b,  即可构造出初始的响应面函数$\overline{g}^{(k)}(x)$; 

3. 参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/✈️航空发动机可靠性分析/第三章 可靠性灵敏度分析的矩方法#(3) 改进 FOSM 可靠性分析方法|AFOSM方法]],  使用<mark style="background: transparent; color: red">改进的一次二阶矩法</mark>求得$\overline{g}^{(k)}$的==设计点 $x_{D}^{(k)} = (x_{D1}^{(k)}, x_{D2}^{(k)}, \dots x_{Dn}^{(k)})$ 以及对应的可靠度指标== $\beta^{(k)}$, 需要注意的是, **使用改进一次二阶矩方法时， 只需将预测的响应面函数$g'(x)$代入AFOSM中代替原响应函数即可， 不使用原先的g(x)求解偏导数**: 

4. 设**使用AFOSM求解得到的设计点**为$(x_D^{(k)}, g(x_D^{(k)}))$,  而初始样本点取均值 $(\mu_{X}, g(\mu_{X}))$; 则插值公式如下: ($g(x_D^{(k)}$也可以使用$\overline{g}(x_D^{(k)}$预测函数值来替代， 调用次数更少)。 此时， 新的中心点计算为插值:
$$\Large\boxed{x_{i}^{(k+1)} = \mu_{X_{i}} + (x_{Di}^{(k)}- \mu_{X_{i}}) \frac{g(\mu_{X})}{g(\mu_{X}) - g(x_{D}^{(k)})}}\tag{8.1.3.1}$$
通过下式可以看出，插值结果近似在响应面上， 即近似有 $g(x_i^{(k+1)}) \approx 0$  
![[Excalidraw/第八章 响应面方法和支持向量机 2024-03-28 10.49.35|450]]
获取下一次计算的 $x_{1i}$ 坐标,  然后重复第1步,  直到**两次可靠度指标的计算差值小于预先给的精度指标**, 认为结果收敛。


> [!caution] 注意
> 需要说明的是，直接使用AFOSM迭代只能获到正确的模型， 但是获取不到正确的Pf, 正确的Pf仍然是利用模型进行MCS模拟获取的。

代码示例如下:
```matlab title:线性响应面法代码示例
%% %%%%%%%%%%%% 算例 8.1 响应面法求解可靠性分析的可靠度 %%%%%%%%%%%%%%%%%%% 
% 对于指数型功能函数为g(X) = e^(0.2 x1 + 1.4)- x2, 其中x1, x2为标准正态分布
clear, clc;
g = @(x) exp(0.2 .* x(:,1) + 1.4) - x(:,2);
mu_ = [0,0];
sigma_d = [1,1];
sigma_ = diag(sigma_d.^2);

% [Pf, Pf_mu, Pf_sigma] = MCS_solu(mu_, sigma_,g, 1e7);
% [x_i, beta_res, Pf2] = AFOSM_solu(mu_,sigma_, g);

%% %%%%%%%%%%%% 线性加权响应面方法进行的可靠性分析 %%%%%%%%%%%%%%%%%%%%%%
n = size(mu_, 2);       
f = 1;                  % 设置插值系数, 即选取点距离的sigma差倍数, 默认为1

%% %%%%%%%%%%%%%%%%%%% 利用计算权重 %%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
weight_func = @(gx) min(abs(gx) + 1e-3)./(abs(gx) + 1e-3);   % WRSM权重函数, g越接近0越大
% weight_func = @(gx) ones(length(x), 1);  % TWRSM权重函数

beta_pre = 0;              % 初始迭代beta值
x0 = mu_;  y0 = g(mu_);    % 用于插值的初始样本点
xp = zeros(2 * n + 1, n);  % 设计点组
x_i = mu_;                 % 初始时, 设计点组以mu_ 为中心点;

for epoch = 1:1000
    % 第一次迭代 (buncher设计点选择方法), 选用新的2n个设计点 
    xp(1,:) = x_i;
    for i = 1:n 
        f_delta = zeros(1,n); f_delta(i) = sigma_d(i) * f;
        xp(2 * i,:) = x_i + f_delta;
        xp(2*i+1,:) = x_i - f_delta;
    end
    m = length(xp);  % 构造以下矩阵
    Y = g(xp);  % 此处调用原始函数g求解所有点的Y (m  * 1);
    W = diag(weight_func(Y));               % (m *  m);
    A = [ones(m,1), xp];                    % (m *n+1);

    b = ((A' * W * A) \ (A' * W * Y))';  % 获取待定系数向量 (n+1 * 1)
    % 对应的响应面函数为b0 + b1 x1 + b2 x2 + ... + bn xn,
    % 只需将预测响应面函数代入AFOSM中代替原响应函数即可
    
    g_new = @(x) b(1) + b(2) * x(:,1) + b(3) * x(:,2);
    % 用 AFOSM 求解设计点的可靠度指标 beta^k 以及设计点
    [x_i, beta_res, ~] = AFOSM_solu(mu_, sigma_, g_new); 
    
    if abs(beta_res - beta_pre) < 0.001
        break;
    else  % 进行插值,  更新设计点
        x_i = mu_ +  (x_i - x0) * y0 / (y0 - g_new(x_i));
        sprintf("epoch: %d, Pf: %f, beta : %f", epoch, Pf, beta_res)
        beta_pre = beta_res;
    end
end
%% %%%%%%%%%%%%%%%% 使用MCS方法, 计算响应面失效概率  %%%%%%%%%%%%%%%%%%
num_MCS = 5e6;
xpp = lhsnorm(mu_, sigma_, num_MCS,"on");
fail_points = find(g_new(xpp) < 0);
Pf = size(fail_points,1)/num_MCS;
```

## 二、加权非线性响应面法
### (1) 基本原理
由于加权线性响应面法不能反映非线性对于可靠性的影响，可以利用二次函数最小二乘方法，二次非线性响应面模型如下:
$$\boxed{\Large \overline{g}(X) = b_{0} + \sum^{n}_{i=1} b_{i}X_{i} + \sum^{n}_{i=1}  c_{i}X_{i-n}^{2}}\tag{8.2.1.1}$$
其中$b_{i}, c_{i}$为待定系数。上式是一种不含交叉项的二次响应面模型，并可以合写为:
$$\left\{ b_{i}, c_{i}\right\} = (b_{0}, b_{1}, \dots b_{2n})$$

> [!cite] 说明
> 参考[[#(3) 加权线性响应面选取的基本步骤]],  由于响应面法每一次会抛弃原先的试验点， 而建立新的试验点。往往导致计算效率降低。 我们考虑在最后几次迭代重复利用试验点,  减少新增的试验点个数, 来减少计算量， 从而提升计算效率。
### (2) 选取响应面策略与权函数的构造
方法1: 与[[#(2) 权重矩阵的构造方式|传统权重矩阵]]相同, 功能函数分式形权设置
$$\Large w_{i} = \frac{\min (|g(x_{i})| + \delta) }{|g(x_{i})| + \delta}  \qquad (i = 1, 2,\dots m)\tag{8.2.2.1}$$
方法2 : 取<mark style="background: transparent; color: red">功能函数与密度函数的比值型权数</mark>, 即密度越大, $|g(x)|$ 越小,  $w_{i}$也越大
$$\Large\begin{cases}
h_{i} = \left|\frac{g(x_{i}) + \delta}{f_{X}(x_{i})} \right|  \\
w_{i} = \frac{\min_{i = 1}^{m} h_{i} }{h_{i}}
\end{cases}\tag{8.2.2.2}$$

### (3) 加权非线性响应面方法的基本步骤
参考[[#(3) 加权线性响应面选取的基本步骤]] 
1. 初始先以<mark style="background: transparent; color: red">传统响应面法</mark>确定$\overline{g}^{(1)}(X)$设计点和初始的(2n+1)个设计点组$x_D^{(1)}$ 
2. 第$k$次迭代时,  按照相同方法，从$k-1$次的设计点$(x_D^{(k-1)}, g(x_D^{(k-1)}))$与均值点$(\mu_X, g(\mu_X))$ 插值得到中心点(插值得到的点是$g(x_{1}^{*(k)}) \approx 0$的点，因此仍然使用:
$$\Large x_{i}^{(k+1)} = \mu_{X_{i}} + (x_{Di}^{(k)}- \mu_{X_{i}}) \frac{g(\mu_{X})}{g(\mu_{X}) - g(x_{D}^{(k)})}$$
需要说明: 对于一次插值， 共有m个实验点, 而对应的列为n+1列， 而在非线性响应面中应当有 2n +1 列, 对应地， 仍然有
$$b = \frac{a^{T} W Y}{a^{T} W a}$$
其中
$$a = \left[\begin{matrix}
1 & x_{11} & x_{12} & \dots &  x_{11}^{2} & \dots & x_{1n}^{2}  \\ 
1 & x_{21} & x_{22} & \dots &  x_{21}^{2} & \dots & x_{2n}^{2}  \\ 
\dots  \\ 
1 & x_{m1} & x_{m2} &  \dots  & x_{m1}^{2} & \dots  & x_{mn}^{2}
\end{matrix}\right]$$
W 为 $m * m$ 矩阵; 
```matlab title:加权二次响应面法示例代码
%% %%%%%%%%%%%%%%%%%%% 加权二次响应面法 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
clear, clc;
% mu_ = [0,0];
% sigma_d = [1,1];
% sigma_ = diag(sigma_d.^2);
% g = @(x) exp(0.2 .* x(:,1) + 1.4) - x(:,2);
mu_     = [0, 0, 0];
sigma_d = [1, 1, 1];
sigma_ = diag(sigma_d.^2);
g = @(x) 1./40 .* x(:,1).^4 + 2 .* x(:,2).^2 + x(:,3)+ 3;

% [x_i, beta_res, Pf] = AFOSM_solu(mu_, sigma_, g);  % 直接使用 AFOSM 会导致非线性产生较大误差
% [Pf, Pf_mu, Pf_sigma] = MCS_solu(mu_, sigma_,g, 1e7);   % 3.09e-4  & 0.0000	-0.0000	-0.0010 
%% %%%%%%%%%%%%%%%%%%% 加权非线性响应面方法求解部分 %%%%%%%%%%%%%%%%%%% 
n = size(mu_, 2);
w = @(gx) min(abs(gx) + 1e-3)./(abs(gx) + 1e-3);   % 这个是w1
%fx = @(xp) prod(normpdf(xp, mu_, sigma_d), 2);  % 计算联合概率密度函数, 查一下源码
%w = @(xp, gx) min((abs(gx) + 1e-3)./fx) ./ ((abs(gx) + 1e-3)./fx);

f = 1;                   % 插值系数
beta_pre = 1e-3;         % 初始值,设置为 0
x_i = mu_; x0 = mu_; y0 = g(mu_); 

for epoch = 1:1000
    xp = buncher_sample(x_i, sigma_d, "f", f);  % 建立样本点
    y = g(xp);  m = length(y);            % 获取真实功能函数值 
    W = diag(w(y));                       % 权重矩阵
    A = [ones(m,1), xp, xp.^2];           
    b = ((A' * W * A) \ (A' * W  * y))';  % 最小二乘求解b 

    % 使用b获取新的预测函数值
    g_new = @(x) b(1) + sum(b(2:n + 1) .* x, 2) + sum(b(n+2: 2*n+1).* x.^2, 2);
    
    [x_i, beta_res, ~] = AFOSM_solu(mu_, sigma_, g_new);
    
    if abs((beta_res - beta_pre)/beta_pre) < 0.001
        break
    else
        x_i = mu_ + (x_i - mu_) .* (y0)./(y0 - g_new(x_i));
        sprintf("epoch: %d, beta: %f", epoch, beta_res)
        beta_pre = beta_res;
    end
end

%% %%%%%%%%%%%%%%%% 使用MCS方法, 计算响应面失效概率  %%%%%%%%%%%%%%%%%%
num_MCS = 5e6;
xpp = lhsnorm(mu_, sigma_, num_MCS,"on");
fail_points = find(g_new(xpp) < 0);
Pf = size(fail_points,1)/num_MCS;
```

## 三、支持向量机相关概念
### (1) VC维的概念
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/Class Contents/基本课程/2.SVM支持向量机|2.SVM支持向量机]], 响应面方法虽然是基于最小二乘的一种多项式系数求解方法， 但是<mark style="background: transparent; color: red">对于高度非线性问题，响应面法的计算量将急剧增加</mark>。因而往往仅适用于非线性程度不高的问题。

首先， 对于一个二分类问题， 如果存在$m$个样本, 可以将函数即中的函数按照所有可能的$2^m$种形式分开， 则称这个分类器所能够最大样本数目$m$ 为函数集的 VC 维。

例如, 显然对于3个点, 线性分类器必然可将其以$2^{3}$种形式全部分开，因此线性分类器的VC维至少是3。

### (2) 经验风险
首先假设变量X与变量y之间具有某种依赖关系， 且关系是未知的。此时需要依据训练样本$(x_1, y_1), (x_2, y_2), .... (x_{l}, y_{l})$选择最佳权值向量w的函数对依赖关系进行估计。<mark style="background: transparent; color: red">最佳逼近</mark>$f(x, w_0)$的选择原则一般是使得经验风险(empirical risk)R(w)最小:

**期望风险**的表达式如下:
$$R(w) = \int \mathcal{L} (y, f(x,w)) dP(\boldsymbol{x},y)\tag{8.3.2.1}$$
其中，损失函数为$\mathcal{L}(y, f(x,w))$, 而P(x,y)是常用的平方型损失函数如下:
$$\mathcal{L}(y, f(x,w)) = (y -  f(x,w))^{2}$$
由于P(x,y)无法求解，因此我们使用经验风险代替期望风险，<mark style="background: transparent; color: red">经验风险表达式为</mark>:
$$R_{emp} (w) =  \frac{1}{l}\sum^{l}_{i=1} \mathcal{L} (y_{i}, f(x_{i},w))\tag{8.3.2.2}$$
可以看出, 平方型损失函数的经验风险即预测值差值的方差。
> [!NOTE] 说明
> 目前的许多算法是在经验风险最小化的基础上提出的。但是往往模型过于复杂时会导致过学习的情况发生。

### (3) 结构风险(实际风险)
实际风险$R(w)$和经验风险$R_{emp}(w)$之间，至少以 $1-\eta$ ($\eta \in [0,1]$)的概率存在如下关系:
$$R(w) \leq  R_{emp}(w) + \sqrt{\left| \frac{h \left(\ln \frac{2l}{h} + 1\right) - \ln (\eta / 4)}{l}\right|}$$
其中, $h$为对应逼近函数的VC维数, $l$为样本数; 另外需要说明的是, <mark style="background: transparent; color: red">根号这一项表示置信范围</mark>, 因而<mark style="background: transparent; color: red">结构风险实际上是经验风险和置信范围之和</mark>。
上述式子可以简化为:
$$R(w) =  R_{emp} (w) + \Omega\left(\frac{l}{h}\right)$$
需要说明的是, 当$h$较大时， 置信范围必须相应增大， 即$h$越大， 往往会导致过拟合现象发生
![[Excalidraw/第八章 响应面方法和支持向量机 2024-03-29 15.16.13]]
为了让整个结构风险达到最小化， 我们往往需要确定$R_{emp}$和$\Omega$和最小的点。而支持向量机则是以结构风险最小化为优化目标的一种方法。
### (4) 对偶优化
参考[[📘ClassNotes/📐Mathmatics/📈Advanced Mathematics/👆重要定理/拉格朗日乘数法求条件极值|拉格朗日乘数法求条件极值]], 求解$g_{i}(x) \leq 0, h_{j}(x) = 0$条件下的$f(x)$极小值可以使用下式:
$$L(x, \alpha, \beta) =  f(x) + \sum^{m}_{i=1} \alpha_{i}g_{i}(x) + \sum^{k}_{j=1} \beta_{j} h_{j}(x)$$
我们只需要取不同的x值让这个式子最小即可
$$\min f(x) \rightarrow   \min_{x}  L (x, \alpha, \beta)$$
显然，这个式子对于任何的$\alpha, \beta$ 都需要取到最小值, 我们不放设$\alpha， \beta$让右侧的式子取到最大值，则问题转化为:
$$\min f(x) \rightarrow \min_{x} \max_{\alpha \geq 0, \beta} L (x, \alpha, \beta)\overset{转化为对偶问题}{\longrightarrow}  \max_{\alpha \geq 0, \beta}\min_{x} L (x, \alpha, \beta)$$
为了保证==对偶优化问题和初始的问题具有相同的解==， 参考[[📘ClassNotes/👨‍🔧Mechanics/🖥️Computational_Mechanics/✈️航空发动机可靠性分析/推导部分/对偶优化一致解条件推导过程.pdf|对偶优化一致解条件推导过程.pdf]] 得到一致解条件(KKT条件):
$$\Large \begin{cases}
\left.\frac{\partial L(x ,\alpha^{*}, \beta^{*})}{\partial x}  \right|= 0 ,  \\
\alpha_{i}^{*} g_{i}(x^{*}) = 0  \\
h_{j} (x^{*}) = 0,  g_{i} (x^{*}) \leq  0 \\
\alpha_{i} (x^{*})  \geq  0
\end{cases}\tag{8.3.4.1}$$
其中第一个为<mark style="background: transparent; color: red">拉格朗日函数极值条件， 第二个为互补松弛条件</mark>，第三个为原始约束条件, 第四个为拉格朗日系数约束。
## 四、支持向量机分类法(SVM)
### (1) 最值关系
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/Class Contents/基本课程/2.SVM支持向量机|2.SVM支持向量机]], 首先我们对于线性可分的样本
$$D = \left\{(x_{1}, y_{1}), \dots , (x_{l}, y_{l}) \right\},  \quad  y_{i}  \in \{-1 , +1\}$$
假设线性超平面为:
$$\Large\boxed{w \cdot  X + b  = 0}\tag{8.4.1.1}$$
分类目标是最大化超平面距离 $\gamma$, 即最小化$\frac{1}{2} ||w||^{2}$, 同时, 考虑到一些样本不能够被正确分类, 引入松弛变量$\xi_{i}$ , 此时最小化问题成为寻找:
$$\text{argmin}  \frac{1}{2}||w||^{2}  + C \sum^{l}_{i=1} \xi_{i}\qquad  s.t \quad  y(w\cdot  \varphi(x_{i}) + b) \geq  1 - \xi_{i} \quad (\xi_{i} \geq 0, i = 1,2, \dots l)\tag{8.4.1.2}$$
即有:
$$g(x) = -( y(w \cdot  \varphi (x_{i} )+ b ) - 1  + \xi_{i}) \leq  0 \tag{8.4.1.3}$$
> [!NOTE] 说明
> 其中对于每个变量都对应一个$\xi_{i}$, 描述不满足约束$\geq 1$的程度。而$C$表示**正则化常数**。取$C = \infty$时,则约束变为所有的点都必须明确分类 $y_{i}(w \cdot \varphi(x_{i}) + b)\geq 1$, 而C取有限值时, 一些样本点则可以不满足约束。

此时， 引入拉格朗日函数, 得到下面的**拉格朗日函数**(寻求最小值), 其中**最后一项是由于$\xi_{i} \geq 0$构造的**$-\gamma_{i}\xi_{i} \leq  0$: 
$$L(w, b, \xi , \alpha, \gamma) = \frac{1}{2}||w||^{2} + C \sum^{l}_{i=1} \xi_{i} - \sum^{l}_{i=1} \alpha_{i} \left\{y_{i} [w \cdot  \varphi (x_{i}) + b \right\}  - 1 + \xi_{i}) - \sum^{l}_{i=1} \gamma_{i} \xi_{i}$$
上述的优化问题变为: 
$$\min_{w, b, \xi} \max_{\alpha \geq 0, \gamma\geq  0} L (w, b, \xi, \alpha, \gamma\geq  0 ) \overset{对偶问题}{\longrightarrow}  \max_{\alpha \geq   0, \gamma\geq  0}\min_{w, b, \xi} L(w, b, \xi , \alpha , \gamma)$$
根据[[#(4) 对偶优化|KKT条件]],代入并变形容易得到优化问题(其中$\alpha_i$为拉格朗日乘子):
$$\large \min_{w, b, \xi} - \frac{1}{2}\sum^{l}_{i=1} \sum^{l}_{j=1}  \alpha_{i} \alpha_{j} y_{i} y_{j}x_{i}x_{j} + \sum^{l}_{i=1} \alpha_{i}$$
上式得到对偶问题，同时<b><mark style="background: transparent; color: blue">引入核函数说明</mark></b>$x_i, x_j$关系, 即<b><mark style="background: transparent; color: blue">最小化w, b, xi的条件被消去, 只需要改变alpha并寻找下面对偶问题的最大值即可</mark></b>:
$$\Large\boxed{\max_{\alpha \geq  0} \sum^{l}_{i=1} \alpha_{i}  -\frac{1}{2}\sum^{l}_{i=1} \sum^{l}_{j=1} \alpha_{i}\alpha_{j} \mathcal{K}(x_{i}, x_{j})}\tag{8.4.1.4}$$
其中, $\mathcal{K}$表示核函数， 选取参考下一节。 求解上述优化问题， 即可得到所有的$\alpha_i$ 
> [!tip] 说明
> 求解获取到的大部分$\alpha_{i} = 0$,  不为零对应的样本的是支持向量。

我们参考式(8.3.4.1)和(8.4.1.3), 由于$\alpha_{i}^{*} g_{i}(x^{*}) = 0$, 显然通过两个互补松弛条件得到: 
$$\large\begin{cases}
\alpha_{i}  (y(w \cdot \varphi(x_{i}) + b) - 1 + \xi_{i})  = 0  \\
\gamma_{i}  \xi_{i}  = (C -  \alpha_{i}) \xi_{i}= 0
\end{cases}$$
此时, 若有 $\alpha_{i} \in (0, C)$, 显然有: 
$$\Large \begin{cases}
y(w \cdot \varphi(x_{i}) + b) = 1  \\
\xi_{i} = 0
\end{cases}\tag{8.4.1.5}$$
即为**支持向量对应的点**。因此， 可以利用$\alpha \in  (0, C)$过程中的支持向量求解对应的点, 然后求解$b$值(一般做法是取平均)
计算出b之后, 支持向量的==**分类预测函数**==可以计算为:
$$\Large \boxed{g_{SVM }(x) = \text{sgn}\left(\sum^{l}_{i=1} \alpha_{i} y_{i}k(x_{i}, x_{j}) + b\right)}\tag{8.4.1.6}$$

### (2) 核函数的选取
参考[[📘ClassNotes/⌨️Programming/👨‍🎓Deep Learning/Class Contents/基本课程/2.SVM支持向量机#三、非线性支持向量机|非线性支持向量机]]， 核函一般常用如下表达式:
#### 1. 线性核函数
$$\mathcal{K} (x_{i}, x_{j})$$
#### 2. 多项式核函数: 
$$\mathcal{K}(x_{i}, x_{j}) = (x_{i}\cdot x_{j} + 1 )^{d}$$
#### 3. 高斯核
$$K(x_{i},x_{j}) = exp\left(-\frac{||x_{i} - x_{j}||^{2}}{2\sigma^{2}}\right)$$
#### 4. 指数核
$$k(x_{i}, x_{j}) = \exp \left(- \frac{||x_{i} - x_{j}||}{\sigma}\right)$$
#### 5. Sigmoid核(略去)


### (3) 交叉验证误差
推广能力即在算法对于未知的测试集上对数据进行拟合的精度。为了评估推广English， 往往使用测试即并来取验证误差来估计预测的精度。 
`````ad-todo
title: 推导
collapse: open
`````
分类问题的**留一法交叉验证误差**(Leave one out Cross-Validation)为:
$$e_{LOO} = \frac{1}{l} \sum^{l}_{i=1} I(- y_{i} g_{SVM}^{\sim i}(x) )$$
回归问题的留一法交叉验证误差为
$$e_{LOO}  = \frac{1}{l} \sum^{l}_{i=1} I(y_{i}- g_{SVR}^{\sim i}(x) )^{2}$$
其中右上角$\sim i$表示除去第i个样本构建的支持向量分类模型。 
$$I = \begin{cases}
0  \\
1 \qquad  X \leq 0
\end{cases}$$

## 五、支持向量机回归算法
### (1) SVR 回归算法基本介绍
首先， 考虑训练样本集: 
$$D = \left\{ (x_{1}, y_{1}), \dots, (x_{l}, y_{l} )  \right\}$$
假设样本集$D$是$\varepsilon -$线性近似的, 即存在一个超平面表达式$g_{SVR}(x) = w\cdot x + b$使得下式成立:
$$\left| g_{SVR} (x_{i}) - y_{i} \right|  \leq  \varepsilon\quad  \rightarrow  \quad d =   \frac{|w \cdot  x + b - y_{i}|}{\sqrt{||w||^{2} + 1}} \leq  \frac{\gamma}{2}$$
图像如下:
![[Excalidraw/第八章 响应面方法和支持向量机 2024-03-30 11.13.46|300]]
支持向量机回归算法的基本思想是保证训练样本点**在满足$\varepsilon$误差以内的情况下**, **最大化间隔带的宽度**$\gamma$ , 即最小化$||w||$,
取$\varepsilon-$不敏感损失函数为 $\min \left\{0, |y - g_{SVR} - \varepsilon| \right\}$, 引入松弛变量$\xi$和$\xi'$, 分别控制正, 负松弛范围大小，即有:
$$\begin{cases}
g_{SVR} - y_{i} \leq  \varepsilon + \xi_{i}  \\
y_{i}  - g_{SVR} \leq  \varepsilon  +  \xi_{i}'
\end{cases}$$
此时优化问题为:
$$\min \frac{1}{2} ||w||^{2} + C \sum^{l}_{i=1} (\xi_{i} + \xi_{i}')\tag{8.5.1.1}$$
约束条件:
$$\begin{cases}
g_{1i}(x) = (g_{SVR}(x)  - y_{i} - \varepsilon - \xi_{i} ) \leq  0 \\
g_{2i}(x) = (y_{i} - g_{SVR}(x) - \varepsilon - \xi_{i}' ) \leq  0
\end{cases}\tag{8.5.1.2}$$
对应的对偶优化问题转化为:
$$\large \max_{\alpha, \beta \geq  0, \mu,\mu' \geq 0} \min_{w,b,  x} L (w, b,x , \alpha, \beta, \mu , \mu')\tag{8.5.1.3}$$
其中拉格朗日函数形式如下:
$$L (w, b,x , \alpha, \beta, \mu , \mu') =  \frac{1}{2} ||w||^{2} + C\sum^{l}_{i=1} (\xi_{i}+\xi_{i}') - \sum^{l}_{i=1} \mu _{i}\xi_{i} - \sum^{l}_{i=1} \mu_{i}'\xi_{i}' + \sum^{l}_{i=1}  \alpha_{i} g_{1i}(x) + \sum^{l}_{i=1}  \beta_{i} g_{2i}(x)\tag{8.5.1.4}$$
`````ad-todo
title: 推导
collapse: open
`````
与分类问题完全类似，得到非线性回归的优化问题 :
$$\max_{\alpha, \beta\geq 0 } \sum^{l}_{i=1}  y_{i} (\beta_{i}- \alpha_{i}) - \varepsilon(\alpha_{i}  + \beta_{i}) - \frac{1}{2}\sum^{l}_{i=1} \sum^{l}_{j=1} (\beta_{i} - \alpha_{i})(\beta_{j}- \alpha_{j}) k(x_{i}, x_{j})\quad s.t.  \qquad  \sum^{l}_{i=1}  (\beta_{i} - \alpha_{i})= 0 $$
求解对应的$\alpha_i, \beta_i$即可。最终支持向量回归表达式为:
$$\Large\boxed{g_{SVR} (x) =  w \varphi (x) + b =  \sum^{l}_{i=1}(\alpha_{i} - \beta_{i})k (x_{i}, x) + b }$$
$$b = \begin{cases}
y_{j} - \varepsilon - \sum^{l}_{i=1} (\alpha_{i} - \beta _{i})k(x_{i}, x_{j}) \qquad  \alpha_{j}  \in (0, C)\\
y_{j} + \varepsilon - \sum^{l}_{i=1} (\alpha_{i} - \beta _{i})k(x_{i}, x_{j}) \qquad \beta _{j} \in (0, C)
\end{cases}$$
