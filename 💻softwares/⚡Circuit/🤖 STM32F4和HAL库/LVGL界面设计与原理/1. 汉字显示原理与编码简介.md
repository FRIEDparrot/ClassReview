## 1. GBK 编码 
一般  GBK 采用双字节编码, 共有 23940 个码位, 收录 21886 个汉字(21003)和符号(883), GB2312 是专用简体的中文编码, 而 GBK 是其扩充部分;

由于 GBK 文件较大而 GB2312 比较小, 所以对于小 Flash 部分, 可以采用 GB2312 编码, 

| 字符集         | 编码长度       | 说明                         |
| ----------- | ---------- | -------------------------- |
| ASCII       | 1个字节       | 拉丁字母编码，仅128个编码，最简单         |
| GB2312      | 2个字节       | 简体中文字符编码，包含约6000多汉字编码      |
| GBK         | 2个字节       | 对GB2312的扩充，支持繁体中文，约2W多汉字编码 |
| BIG5        | 2个字节       | 繁体中文字符编码，在台湾、香港用的多         |
| **UNICODE** | **一般2个字节** | **国际标准编码，支持各国文字**          |

编码规则简介: GBK 编码中, 由于 ASCII 码的编码范围是 0-127, 而高字节最小是 0x81 (129) 作为区分。

| 字节        | 范围              | 说明                          |
| --------- | --------------- | --------------------------- |
| 第一字节（高）   | 0X81~0XFE       | 共126个区（不包括0X00~0X80，以及0XFF） |
| 第二字节（低）   | 0X40~0X7E       | 63个编码（不包括0X00~0X39，以及0X7F）  |
| 0X80~0XFE | 127个编码（不包括0XFF） |                             |

一般使用 printf %x 可以直接获取汉字的 GBK 编码, 一般只需要搜索在点阵字库中的位置, 即可找到对应的字库点阵进行取模, 描点和还原;(一个点仅有0,1两种)

一般对于 16 x 16 的点阵(256byte)**一个字为 16 x 16/8 = 32byte, 23940个汉字占用的空间为 0.76 MB 左右**, 可以使用 W25Q64 等进行存储。 

首先是字库 (常见的有12/16/24)大小的字库;(**一般采用ATK-XFONT即可制作字库**),进入字库模式, 自行制作即可。
![[attachments/Pasted image 20240722185836.png]]
设置取模方式如图: 
![[attachments/Pasted image 20240722190157.png|600]]

生成 .txt 以及 .bin 文件, 
首先, 重点是**先获取汉字点阵的相对位置, 由于文件读取比较慢, 所以一般可以通过跳转文件指定位置的方式进行读取** 

通过ASCII 查找汉字: 通过低字节判断
1. GBKL < 0x7F时，Hp = ((GBKH – 0x81) * 190 + (GBKL – 0x40)) * csize
2. GBKL > 0x7F时，Hp = ((GBKH – 0x81) * 190 + (GBKL – 0x41)) * csize 

其中，首先根据 GBKH 判断出所在的区, 每个区共有 63 + 127 共 190 个编码; 此时, 判断GBKL, 当 GBKL < 0x7F,  使用 + (GBKL - 0x40); 否则使用 + (GBKL - 0x41, 跳过0x7F)

| 字节      | 范围        | 说明                          |
| ------- | --------- | --------------------------- |
| 第一字节（高） | 0X81~0XFE | 共126个区（不包括0X00~0X80，以及0XFF） |
| 第二字节（低） | 0X40~0X7E | 63个编码（不包括0X00~0X39，以及0X7F）  |
|         | 0X80~0XFE | 127个编码（不包括0XFF）             |

此时，我们定义 CSIZE = 2bytes (16), 则可以**定位到点阵的存储位置**。而发送汉字时, 发送的是GBK编码, 此时即可直接从 bin 中获取相应的位置;

一般而言, 为了取模的大小是8的整倍数, 我们往往使用 12, 16, 24 大小的进行取模和字库制作。

## 2. Unicode 转 GBK 
另外, **unicode 转 gbk 文件**(ugbk), 另外, 对于FATFS使用长文件名时, 会使用 ffunicode.c 中的 uni2oem936 和 oem2uni936, 这导致会占用极大的 Flash(172kb). 因此一般使用:
```c
#define FF_USE_LFN		0
```

如果定义为 1, 则会导致占用约 500kb Flash, 而不为1时, 则不支持长度为 12 以上的命名;

实际上, ffunicode.c 文件的函数仅有最下方调用的3个, 而大数组可以通过文件直接存储。
因此我们可以考虑将 uni2oem936 数组和 oem2uni936 转换为 bin 文件 (C2B 转换); 然后直接通过将文件存储在


## 3. ILI9341 触屏硬件SPI移植部分
在触屏函数中， TP_Read_AD 是通过 SPI 方法读取触屏坐标的函数, 在高速滑动中, 读取的坐标不一定相同。因此一般使用多次读取和舍弃最值的方法: 下面的代码首先读 READ_TIMES = 5次, 排序后, 前后舍弃 1 个数, 其余3个读取的值作平均得到触屏的触摸点坐标。
```c
uint16_t TP_Read_XOY(uint8_t cmd)
{
    uint16_t i, j;
    uint16_t buf[READ_TIMES];
    uint16_t sum=0;
    uint16_t temp;
    for(i=0;i<READ_TIMES;i++){   // read 5 times
        buf[i]=TP_Read_AD(cmd);
    }
    /** sort the data in ascending order */
    for(i=0;i<READ_TIMES-1; i++)
    {
        for(j=i+1;j<READ_TIMES;j++)
        {
            if(buf[i]>buf[j]) 
            {
                /** Switch the i,j  */
                temp=buf[i];
                buf[i]=buf[j];
                buf[j]=temp;
            }
        }
    }
    sum=0;
    for(int i=LOST_VAL;i<READ_TIMES-LOST_VAL;i++){
        sum+=buf[i];
    }
    temp=sum/(READ_TIMES-2*LOST_VAL);
    return temp;
}
```

我们可以采用更快的方式: 
1. 首先读取 6 个数, 将最大值和最小值先赋值`buf[0]`, 使用一个for循环依次比较; 当获取到读数之后, <mark style="background: transparent; color: red">减去最大值和最小值</mark>, 剩余 4 个数; 此时除4的操作可以使用 >> 2 来代替, 以获得更高的运算效率。

修改的函数如下:
```c
uint16_t TP_Read_XOY(uint8_t cmd)
{
    /** read 6 times, throw the maximum and minimum, return the average of others */
    uint16_t i, j;
    uint16_t buf[6];  
    uint16_t sum=0;
    uint16_t maxval, minval;

    /*** Change to Hardware SPI read later */
    for(i=0;i< 6; i++){   // read 5 times
        buf[i]=TP_Read_AD(cmd);
    }

    maxval = buf[0]; minval = buf[0];
    for (i = 0; i < 6; i++)
    {
        if (buf[i] > maxval) maxval = buf[i];
        else if (buf[i] < minval) minval = buf[i];
        sum += buf[i];
    }
    sum -= (maxval + minval); 
    return (sum >> 2);
}
```


对于触屏的状态, 使用了一个独热编码进行定义:
```c
tp_dev.sta|=TP_CATH_PRES;       // 捕获状态
tp_dev.sta |= TP_PRES_DOWN;   // 按下状态
```


对于传输的移植方式: 首先查找 XPT2046 驱数据手册:
![[attachments/Screenshot_20240724_122252_com.microsoft.skydrive.png|900]]
时钟在空闲为低电平(CPOL = 0), 在上升沿获取数据(CPHA = 0)
另外上图可以看出, 在传输 Command 之后, 实际上需要一个时钟进行等待, 然后<mark style="background: transparent; color: red">第二个时钟从位11开始传输, 因此我们首先传输两个0xff</mark>,得到的**第一个左移5位，第二个右移3位相加, 得到的即为所获得的屏幕坐标**。

重点需要注意的是 CS 的操作, 每次读写完一个字节之后都要进行 先拉下后释放片选, 因此必须将片选操作套在循环内, 不能放在循环外面 (这样一周就少了5次)

## 4. 中文字模GBK编码和文件读取
理论上的字模为从高到低排列:
{0x00,0x00,0x40,0x00,0x40,0x00,0x40,0x00,0x41,0x80,0x5F,0x00,0x61,0x04,0x41,0x04,} 而实际上文件存储的 0x00 数据更多，如图所示:
![[attachments/Pasted image 20240724224914.png]]
实际字模每4个之间有一个0x00填充, 这是由于生成问题造成的; 
原始编码方法如图: 一定要仔细核对汉字编码是否正确和紧凑:
![[attachments/Pasted image 20240724230030.png|1200]]

```c
uint8_t LCD_ShowGBK(uint16_t x, uint16_t y, uint8_t *buff, uint8_t font_size){
	uint8_t width;
	switch (font_size){
	case GBK_FONT_SIZE_12:
					width = 12;
					break;
	case GBK_FONT_SIZE_16:
					width = 16;
					break;
	case GBK_FONT_SIZE_24:
					width = 24;
					break;
	default:
		return 1;  // no font input 
	}
	POINT_COLOR = BLACK;
    for (uint8_t i = 0; i < width; i++) {
        uint16_t tmp = ((uint16_t)buff[2* i]  << 8)  + (buff[2* i + 1]);
		/** from up to down to fill the y coor */
		for (uint8_t j = 0; j < width; j++){
			if (tmp & 0x8000){
				ILI9341_DrawPoint(x + i, y + j);
			}
			tmp <<= 1;
		}
    }
	return 0;
}
```

对应的获取编码的代码如下:   
```c
/// @brief get the matrix of pixels from gbk code 
/// @param gbk_code  
/// @param buff 	 buffer to receive the matrix of pixels 
/// @param font_size use  GBK_FONT_SIZE_12, GBK_FONT_SIZE_16, GBK_FONT_SIZE_24
/// @note  Flash  must be initialized before using gbk
/// @return 0: succeed ,1 file open error, 2: not valid gbk code 
uint8_t GBK_FONT_GetCode(uint16_t gbk_code, uint8_t* buff, uint8_t font_size){
	FIL fp; uint8_t font_size_bytes;
	uint8_t hb = (gbk_code >> 8), lb = (gbk_code & 0xff);

	printf("hb: %x, lb: %x\r\n", hb, lb);

	char fname[MAX_ABSOLUTE_PATH_LENGTH];
	if ((gbk_info.Font_exist & font_size) == 0){
		return 1;   // no font file 
	}
	switch (font_size){
	case GBK_FONT_SIZE_12:
					sprintf(fname, "%s/%s/%s", W25Qxx_ROOTDIRECTORY, FONT_FOLDER, GBK12_STD_FONT_FILE_NAME);
					font_size_bytes = gbk_info.gbk12size;
					break;
	case GBK_FONT_SIZE_16:
					sprintf(fname, "%s/%s/%s", W25Qxx_ROOTDIRECTORY, FONT_FOLDER, GBK16_STD_FONT_FILE_NAME);
					font_size_bytes = gbk_info.gbk16size;
					break;
	case GBK_FONT_SIZE_24:
					sprintf(fname, "%s/%s/%s", W25Qxx_ROOTDIRECTORY, FONT_FOLDER, GBK24_STD_FONT_FILE_NAME);
					font_size_bytes = gbk_info.gbk24size;
					break;
	}

	/*** Firstly, judge if the character is in gbk ***/
	if (hb < 0x81 || lb < 0x40 || lb == 0xff || hb == 0xff){
		return 2;
	}
	FRESULT res = f_open(&fp, fname, FA_READ);
	if (res != FR_OK){
		return 1;
	}
	unsigned long font_addr = 0 ;
	if (lb < 0x7f){  // GBK Code  
		font_addr = ((unsigned long)190 * (hb - 0x81) + lb - 0x40) * font_size_bytes;
	}
	else if (lb > 0x7f)
	{
		font_addr = ((unsigned long)190 * (hb - 0x81) + lb - 0x41) * font_size_bytes;
	}else{
		printf("unknwon GBK code: %x%x\r\n", hb, lb);
	}
	if (f_lseek(&fp, font_addr)!=FR_OK || f_read(&fp, buff, font_size_bytes, NULL)!=FR_OK){ // file destrooyed
		f_close(&fp);
		return 1;
	}
	f_close(&fp);
	return 0;
}
```

## 四、中文拼音输入法相关
一般 T9 输入法部分, 可以通过联想获取; 
首先, 对于输入法部分, <mark style="background: transparent; color: red">必须通过输入的拼音去查找对应的汉字部分</mark>, 最终查找对应的汉字;

我们采用**拼音索引表**, 而对应的**码表记录拼音对应的汉字**;

可以从 http://kerwin.cn/dl/detail/u011774239/434431 获取 GBK 拼音码表, 用于实现基于查找的拼音解码算法;


## 五、LVGL 在线字库转换和字库制作
在线字体转换工具 https://lvgl.io/tools/fontconverter 

一般 Windows自带的字体在 C: Windows/Fonts 下,  得到 .ttf 或者 .otf 格式的文件; 
编码范围 Range 参考[[💻softwares/⚡Circuit/🤖 STM32F4和HAL库/LVGL界面设计与原理/1. 汉字显示原理与编码简介|汉字显示原理与编码简介]], 对于GBK编码, 范围是 0x8100 - 0xFE00; 而 Unicode 部分包括:  

| 字符集       | Unicode 范围      |
| --------- | --------------- |
| 汉字        | 0x4e00 - 0x9FA5 |
| 数字, 字母和标点 | 0x20 - 0x7E     |

一般地, 在 LVGL 转换字体完成之后，仅需要采用 LV_FONT_DECLARE(STSong Regular Font) 声明字体, 即可直接进行调用。
另外, 我们也可以将对的字体文件转换为 bin 文件

一般包含所有 ASCII， 汉字和符号的.c 格式文件, 只需要:
`--range 0x20-0xBF --range 0x3000-0x3011 --range 0x4E00-0x9FAF --range 0xFF00-0xFF64`

对于码表, 可以采用 yaml 格式进行存储和解析(基于libcyaml库):
[GitHub - tlsa/libcyaml: C library for reading and writing YAML.](https://github.com/tlsa/libcyaml)

### (2) LVGL 支持汉字显示
首先为了移植三种SimHei字体, 需要修改 lv_conf.h 中的定义如下:
```c
#define LV_FONT_CUSTOM_DECLARE    LV_FONT_DECLARE(SimHei12)     LV_FONT_DECLARE(SimHei16)     LV_FONT_DECLARE(SimHei20)
```

在 LVGL 中, 一律使用 UTF-8 作为 TXT 的解析方法(LV_TXT_ENC)
同时 LVGL 支持 https://fontawesome.com/ 部分符号文字

需要说明的是, 其接口是 lv_global.h 中的 fsdrv_ll 部分, 因此打开的实际上是 lv_fs_fatfs 中的函数部分。会以其为底层函数。
```c
#if LV_USE_FS_FATFS
    lv_fs_drv_t fatfs_fs_drv;
#endif
```

在 lv_fs_fatfs_init() 函数中, 会调用 `lv_fs_drv_t * fs_drv_p = &(LV_GLOBAL_DEFAULT()->fatfs_fs_drv);` 因此在初始化所用的字体 .bin 文件之前, 必须调用 `lv_fs_fatfs_init();` 函数。

但是，如果支持几乎全部的文字, 则可能会直接占用掉将近 500kb 的 ROM 内存
![[attachments/Pasted image 20240820231805.png]]
如果使用 SD 卡, 仍然不行, 因此不是文件读取问题而是 RAM 占用太大导致;
因此我们考虑采用 GB2312 中的常用字符,  其编码范围**0xB0A0到0xF7FE**

一般符号编码范围是:
```c
0x20-0xBF, 0x3000-0x3011
```

为了找到一个均衡的能尽可能多显示汉字同时又同时减少ROM或者 RAM 占用的方式: 
支持采用 GB2312 编码的汉字, 下载汉字表，由于 0x4E00-0x9FAF 为 GBK 部分

- GB2312，又称为GB0，由中国国家标准总局发布，1981年5月1日实施
- GB2312标准共收录6763个汉字，其中一级汉字3755个，二级汉字3008个
- GB2312是一种区位码。分为94个区(01-94)，每区94个字符(01-94)
- 01-09区为特殊符号
- 10-15区没有编码
- 16-55区为一级汉字，按拼音排序，共3755个
- 56-87区为二级汉字，按部首／笔画排序，共3008个
- 88-94区没有编码

实际上的范围是 0xB0A0 - 0xD7F9 为常用的一级汉字, 使用正点原子工具直接生成汉字并补充其余部分得到GB2312编码

此时, 使用c语言数组存储16大小字体的一级常用字, 仅达到 618kb ROM 空间, 比不使用的 468 kb 仅占用 150kb ROM 即实现了中文16大小字体, 达到了使用要求。同样地, 如果不使用原始的 16 号字, 可以继续节省 15 kb ROM 内存;

如果全部支持GB2312编码, 则需要723, 即255kb额外空间，

```python title:生成词典的json文件代码
import re  
import os  
import json  
from pypinyin import lazy_pinyin  
  
dict_word_pinyin = {}  
dict_word_freq   = {}  
  
# 查找汉语拼音, 并构建汉语词典 dict_word_pinyinwith open('webdict_arrange.txt', 'r', encoding='utf-8') as f:  
    lines = f.readlines()  
    for line in lines:  
        word = re.search("[\u4e00-\u9fa5]+", line).group(0)  
        freq = int(re.search("[0-9]+", line).group(0))  
        dict_word_freq[word] = freq  
        pinyin_list = lazy_pinyin(word)  
        pinyin = ' '.join(pinyin_list)  
        if pinyin not in dict_word_pinyin:  
            dict_word_pinyin[pinyin] = [word]  
        else:            dict_word_pinyin[pinyin].append(word)  
  
# 按照词频将词典 dict_word_pinyin 中的部分按照词频进行排序, 方便之后索引  
for key, index in dict_word_pinyin.items():  
    dict_word_pinyin[key] = sorted(index, key=lambda x: dict_word_freq[x], reverse=True) # 按照词频排序  
  
# 将字典里面的部分按照字符串进行排序  
dict_word_pinyin_new = {k: dict_word_pinyin[k] for k in sorted(dict_word_pinyin)}  
  
with open("zh_word_dic.json", 'w', encoding='utf-8') as f:  
    json.dump(dict_word_pinyin_new, f, indent=4, ensure_ascii=True)  
  
json_data = json.dumps(dict_word_pinyin_new)  
  
for key, value in dict_word_pinyin.items():  
    print(key, end = ',')
```

## 六、汉字输入法和语音点歌功能
### 预备部分
首先, 我们以支持全部 GB2312 输入为目标, 利用一个放在.h中的 const 码表存储每个音节的拼音部分，用于方便拼音输入和分词。

在UTF-8编码中，有些汉字会占据三个字节。UTF-8是一种可变长度的字符编码，对于不同的字符使用不同数量的字节来表示：

- ASCII字符（U+0000到U+007F）使用1个字节。
- 拉丁扩展字符（U+0080到U+07FF）使用2个字节。
- 常用汉字（U+0800到U+FFFF）使用3个字节。
- 较少使用的汉字和其他字符（U+10000到U+10FFFF）使用4个字节。

需要说明的是, 在UTF-8编码中，U+4E00到U+9FFF范围内的汉字实际上占用**3个字节**。虽然这些字符在Unicode中是两个字节（16位），但在UTF-8编码中，它们被编码为三个字节。这是因为UTF-8是一种可变长度的编码方式，设计用于兼容ASCII字符（1个字节）和其他字符（2到4个字节）。

例如，汉字 “中” 的Unicode码点是U+4E2D，在UTF-8中编码为三个字节：`E4 B8 AD`。 
所以，即使U+4E00到U+9FFF范围内的汉字在Unicode中是两个字节，在UTF-8中它们仍然是三个字节。
由于我们使用的是 utf-8 作为全局解码, 所以按一个字三个字节计算。

使用python 提取gb2312拼音码表中每个字的数量, 生成数组即可;
```python
# 提取拼音码表中所有拼音打头的数量, 以及对应的拼音序列表
import re
import json

table_set = []
len_set   = []
loop = 1

start_ch = '#'
with open ('GB2312_pinyin.txt', 'r',encoding='utf_8') as f_ref:
    lines = f_ref.readlines()

    for line in lines:
        match = re.search(r"=(.*)", line)
        if (match == None):
            continue
        prefix = re.search(r"(.*)=", line).group(1)
        pinyin = match.group(1)

        if (prefix[0]!=start_ch):
            if (loop != 1):
                print(start_ch, " : ", end='')
                pinyin_tables = json.dumps(table_set, ensure_ascii=False)
                print(pinyin_tables, end='')
                print(len_set)
                len_set = []
                table_set = []
            start_ch = prefix[0]
            loop += 1

        # print(len(pinyin),",",end='', sep='')
        table_set.append(str("{}".format(prefix)))
        len_set.append(len(pinyin),)

    print(start_ch, " : ", end='')
    pinyin_tables = json.dumps(table_set, ensure_ascii=False)
    print(pinyin_tables, end='')
    print(len_set)
```

得到对应脚本中的每个字符以及对应的数量, 

GCC 编译器可以设置如下参数:
```c
#define _CRT_SECURE_NO_WARNINGS
-fexec-charset=UTF-8 -finput-charset=UTF-8
```

对于VS, 可以在 Project > Options 中进行添加
![[attachments/Pasted image 20240821211149.png|800]]


由于一般 printf 输出的是 gbk 编码, 所以参考 https://blog.csdn.net/u012234115/article/details/83186386 将其转换为 utf-8 编码显示即可。

```cpp
#include <string>
#include <string.h>
#include <stdlib.h>
#include "codeconv.h"

#ifdef _WIN32
#include <windows.h>

string GbkToUtf8(const char* src_str)
{
	int len = MultiByteToWideChar(CP_ACP, 0, src_str, -1, NULL, 0);
	wchar_t* wstr = new wchar_t[len + 1];
	memset(wstr, 0, len + 1);
	MultiByteToWideChar(CP_ACP, 0, src_str, -1, wstr, len);
	len = WideCharToMultiByte(CP_UTF8, 0, wstr, -1, NULL, 0, NULL, NULL);
	char* str = new char[len + 1];
	memset(str, 0, len + 1);
	WideCharToMultiByte(CP_UTF8, 0, wstr, -1, str, len, NULL, NULL);
	string strTemp = str;
	if (wstr) delete[] wstr;
	if (str) delete[] str;
	return strTemp;
}

string Utf8ToGbk(const char* src_str)
{
	int len = MultiByteToWideChar(CP_UTF8, 0, src_str, -1, NULL, 0);
	wchar_t* wszGBK = new wchar_t[len + 1];
	memset(wszGBK, 0, len * 2 + 2);
	MultiByteToWideChar(CP_UTF8, 0, src_str, -1, wszGBK, len);
	len = WideCharToMultiByte(CP_ACP, 0, wszGBK, -1, NULL, 0, NULL, NULL);
	char* szGBK = new char[len + 1];
	memset(szGBK, 0, len + 1);
	WideCharToMultiByte(CP_ACP, 0, wszGBK, -1, szGBK, len, NULL, NULL);
	string strTemp(szGBK);
	if (wszGBK) delete[] wszGBK;
	if (szGBK) delete[] szGBK;
	return strTemp;
}
```


下载常用字并且提取字的使用频率:

```c
with open("char_common.json", encoding='utf-8') as f_common_char:  
    common_char = json.load(f_common_char)  
    cnt = [0,0,0,0,0]  
    for item in common_char:  
        freq = item['frequency']  
        cnt[freq] += 1  
    print(cnt)
```

在 https://github.com/ling0322/webdict 上获取词库, 然后提取高频词汇; 以 2000 以上的为高频词汇, 并借用 pypinyin 的 lazypinyin 转换为词汇拼音, 按照词频排序, 代码如下:
```python
dict_word_pinyin[key] = sorted(index, key=lambda x: dict_word_freq[x], reverse=True) # 按照词频排序
```

VS Code 工程预定义方法:
![[attachments/Pasted image 20240822162927.png]]
和这里需要匹配:
![[attachments/Pasted image 20240822163224.png]]

另外, `memmove` 函数设计用于处理源和目标内存区域重叠的情况，这使得它在这种情况下比 `memcpy` 更安全。

### (1) 汉字模糊匹配机制
汉字搜索模糊匹配比较简单, 对于任意一个输入的音节, 可能是全匹配的, 也可能是半匹配的。

会定义一个 buffer 进行接收, 并从后向前优先填充全匹配内容， 填充完常用的(约为后10个)后开始填充模糊匹配内容, 模糊搜索完毕之后，如果buffer还有空位, 则填充其余的全匹配生僻字内容。

### (2) 带词库搜索机制
首先, 对于简单暴力的递归, 并且每个均采用模糊匹配实际上是不可取的, 因此我们考虑更好的词组匹配方式。

在分词过程中, 词汇匹配会**递归检查分词**, <b><mark style="background: transparent; color: red">对于所检查的分词方式, 如果该组合能够有全匹配, 则不使用模糊匹配机制</mark></b>;  

> [!caution] 重点 : 匹配方式逻辑
> 其二, 匹配应当从后向前进行筛选, 即出现 bang 时, 则优先 bang 匹配, 齐次是 ban 匹配, 再次是 ba 匹配, 此外, 如果出现超过一个的全匹配如 ma, 则不考虑 m 匹配; 因此, 编写逻辑是: **先找出最长的拼音, 然后直接从拼音表中进行比较, 找出拼音表 中最长的匹配, 然后逐渐减短**，除非没有精确匹配, 则采用模糊匹配搜寻可能的音节;
> 最长的音节为 chuang, shuang, zhuang, 长度 6, 因此定义 

#### BLOOM 过滤器哈希表匹配加速办法
对于字符串匹配, 可以采用哈希表的方式来优化其性能, 但<mark style="background: transparent; color: red">哈希表是使用空间换取时间的一种方式</mark>;
可以构建一个哈希函数来判断一个字符串是否属于汉语词汇拼音，并返回1或0。为了尽可能小的空间使用，可以考虑以下方法 (<mark style="background: transparent; color: red">模糊匹配也可以通过哈希匹配进行直接优化, 而不使用逐个比较的方式</mark>):

1. **使用布隆过滤器（Bloom Filter）**:
    - 布隆过滤器是一种空间效率高的概率数据结构，可以用于测试一个元素是否属于一个集合。
    - 它使用多个哈希函数来映射元素到一个位数组中，虽然会有一定的误判率，但可以通过调整位数组大小和哈希函数数量来控制误判率。
2. **构建拼音词汇表**:
    - 预先构建一个包含所有汉语拼音词汇的集合。
    - 使用布隆过滤器将这些词汇映射到位数组中。
3. **哈希函数实现**:
    - 使用多个哈希函数来计算字符串的哈希值，并检查对应的位数组位置。

另外, 由于找到对应的字符之后, 必须通过字符串获取到其在数组中的位置索引, 因此还需要建立一个从字符串快速获取数组索引位置的映射表。

由于拼音的总个数为 408, 哈希表大小应当大于 408, 为了解决冲突问题, 我们思考如下的方法: 
首先, 如果随机定制哈希函数, 则实际输出应该是散乱的, 很容易有重叠问题; 因此一开始想到将所有进行哈希的方法是一种比较笨的方法, 同时很难找到合适的哈希函数。
```c
# 使用 pinyin 存储所有拼音,
has_code_len = 1024  
  
off_str_list = []  
offset_list =  []   

i = 0  
l = []  
for i in range(len(pinyin_list)):  
    if hash_func1(pinyin_list[i]) not in l:  
        l.append(hash_func1(pinyin_list[i]))  
    else:        hash_val_origin = hash_func1(pinyin_list[i])  
        hash_val = hash_val_origin  
        offset = 1  
        while hash_val in l:  
            hash_val = (hash_val_origin + offset) % has_code_len    # 线性填充哈希表, 循环寻找可哈希位置  
            offset += 1  
        l.append(hash_val)  
        off_str_list.append(pinyin_list[i])  
        offset_list.append(offset)  
if i == len(pinyin_list) - 1:  
    print("hash func1 is ok, the length is ", has_code_len)  
    print(off_str_list,"\n" , offset_list)  
  
    print(len(off_str_list), len(offset_list))
```

但是, 由于字符串的首字母是容易得知的, 因此我们可以利用首字母增加哈希信息, 例如:
codex -> table_num 都是可以使用的;

一般的哈希表确实极其难找, 此时可以单独建立一个哈希偏移表, 解决冲突问题, 但是哈希偏移表虽然能够建立， <mark style="background: transparent; color: red">会由于哈希偏移表过长导致无法实现</mark>。 

### (3) 空间换时间, 设计优化的哈希搜索算法
**首先我们应当从搜索的角度来考虑, 搜索拼音时，检索的实际上是在拼音表code_table_x中的下标, 因此优化函数只需返回匹配的下标以及数量即可** 

首先, 对于字符串长度仅为1的情况, 只需搜索第一个是否为精确, 否则全部都是模糊匹配。
对于有2个以上的字符串长度情况, <b><mark style="background: transparent; color: blue">首先确定拼音表中是否存在这个值</mark></b>, 这个只需要使用 26 个 32 位数据存储 01 就行了, 而<mark style="background: transparent; color: red">如果存在则将其搜索关系对应到长度和下标的表即可</mark>。

但是如何关联搜索关系对应到下标的表呢? 如果采用数组存储, 则会由于偏移在 0-25 之间, 如果使用26大小的数组存储, 则**由于结构体地址是32位的, 实际存储需要大量的存储空间**; 而我们希望尽可能减小存储空间。

于是, 由于我们已经知道这个字符是存在的, 只需要构建一个哈希函数, 将 0-25 之间**存在的字符部分**投影到一个较小范围(例如7)即可。这样就可以节省存储空间了。 

首先需要提取拼音字符串长度超过2的部分, 类似如下:
```c
['an', 'ba', 'be', 'bi', 'ch', 'cu', 'ca', 'ce',  ... ]
```

为了构建出所需的哈希函数, 我们可以利用 python 寻找合适的表长度, 只需要构建一个 `pinyin_list = ["a","ai","an","ang",.....]`, 采用一个简单哈希函数 `loc2 = (ord(pinyin[1]) - ord('a')) % num  `, 然后采用如下函数遍历检索就可以了:
```python
for num in range(5, 27):  
    l = []  
    m = np.zeros((26, num))  
    match = 1  
    max_index = 0  
    for pinyin in pinyin_list:  
        if (len(pinyin) > 2) and  pinyin[0:2] not in l:  
            l.append(pinyin[0:2])  
            loc1 = (ord(pinyin[0]) - ord('a'))  
            loc2 = (ord(pinyin[1]) - ord('a')) % num  
            if (m[loc1][loc2] !=0):  
                match = 0  
                break  
            m[loc1][loc2] += 1  
            if (loc2 > max_index):  
                max_index = loc2  
    if match:  
        print("find reasonable match at ", num, 'max index is', max_index)
```
得到可用的最小除数为 11, 最大下标为 9; 因此, 处理方法如下:

对于长度超过 2 的部分, 采用 26 个 32 位值存储某一组合是否存在, 如果存在, 则采用 `pinyin[1] - 'a' % 11` 计算其哈希值, <mark style="background: transparent; color: red">获取其在长度为11的子表2中的位置</mark>(对于i, u, v 三个字母, 则不需要子表2 )。 其中子表2的每一项是一个结构体指针, 存储结构体如下所示:
```c
typedef struct {
    const uint8_t num;
    const uint8_t* idx_list;
}__zh_hash_table_node;
```
第一项是针对于两字母可匹配的数量, 第二项是在 zh_code_table.c 的大表中, 用于匹配的下标,

例如对于 `hash_node_an` , 则 num = 2，而 "an","ang" 在数组中下标为 2,3, 则<mark style="background: transparent; color: red">对应的 idx_list 是 3,2;</mark> 

> 需要说明的是, 至于上述 idx_list 为什么设计成3,2而不是2,3, 是考虑到词组的模糊搜索时, 应当尽可能优先匹配长度较大的, 因此搜索先后顺序是这里面进行定义的，从前向后搜索时, 应当尽可能得到更长的全匹配内容, 即先匹配 ang, 再匹配 an, 这样就会减少之后匹配种类量截断对于整体搜索的影响了。

由于需要弄的代码比较多, 手写不太现实, 则直接使用 python 生成, 同时自动去除其中每一项后面的 NULL, 然后分成不同的表: 并使用代码生成对应的存储表, 具体 python 代码如下:
```python
list_length = 11  
  
pinyin_dict = {}  
pinyin_idx_dict = {}  

for pinyin in pinyin_list:  
    if (len(pinyin) >= 2):  
        if pinyin[0:2] not in pinyin_dict:  
            pinyin_dict[pinyin[0:2]] = 1  
            pinyin_idx_dict[pinyin[0:2]] = (ord(pinyin[1]) - ord('a')) % list_length  
        else:  
            pinyin_dict[pinyin[0:2]] += 1  
  
list = [["NULL" for i in range(26)] for j in range(26)]  
for key in pinyin_dict:  
    list[ord(key[0]) - ord('a')][pinyin_idx_dict[key]] = "hash_node_" + key  
  
for i in range(len(list)):  
    # 将数组转换为地址值  
    for j in range(len(list[i])):  
        if list[i][j] != "NULL":  
            list[i][j] = "&" + list[i][j]  
    # 去除后方的所有 NULL 元素  
    while len(list[i]) > 0 and list[i][-1] == "NULL":  
        list[i].pop()  
    print("const __zh_hash_node_t* node_" + chr(ord('a') + (i)) + "[]" + " = ", end="")  
    if list[i] == []:  
        print("NULL;")  
    else:        c_array = "{" + ",".join(["".join(item) for item in list[i]]) + "};"  
        print(c_array)  
  
for key in pinyin_dict:  
    print("const __zh_hash_node_t hash_node_" + key + " = " + "{ " + str(pinyin_dict[key]) +  ", " + "&idx_" + key + "};")

# 生成检查码数组 :
arr = np.zeros((26 ), dtype=int)  
for key in pinyin_dict:  
    arr[ord(key[0]) - ord('a')] |= (1 << (ord(key[1]) - ord('a'))) 
for i in range(len(arr)):  
    print(hex(arr[i]), end=',')
```

同样地, 我们针对于最后的 idx_xx  的确定部分, 仍然也不希望进行自己编写, 就继续写一个 python 脚本来帮我们整理出以对应方法的字符串, 而匹配方法是**首先按照长度排序, 大的放在前面, 相同长度按照字符串比较进行排序, 小的放在前面**。

```python 
list  = ["zhua", "zhuan", "zhuang", "zhun", "zhuai", "zha", "zhai", "zhan", "zhang", "zhao", "zhe", "zhen", "zheng", "zhei", "zhi", "zhou", "zhong", "zhu", "zhuo", "zhui"]

sorted_list = sorted(list, key=lambda x: (-len(x), x))
['zhuang', 'zhang', 'zheng', 'zhong', 'zhuai', 'zhuan', 'zhai', 'zhan', 'zhao', 'zhei', 'zhen', 'zhou', 'zhua', 'zhui', 'zhun', 'zhuo', 'zha', 'zhe', 'zhi', 'zhu']
```

编写下面的代码来直接生成我们所需要的索引代码: 
```python 
l_all = [ l_a, l_b, l_c, l_d, l_e, l_f, l_g, l_h,  l_j, l_k, l_l, l_m, l_n, l_o, l_p, l_q, l_r, l_s, l_t, l_w, l_x, l_y, l_z ]  
for i in range(len(l_all)):  
    # ch = chr(ord('a') + i)  
    # print(ch, " : ", end="")    sorted_list = sorted(l_all[i], key=lambda x: (-len(x), x))  
    pinyin_dict = {}  
  
    # print(sorted_list)  
    for item in sorted_list:  
        if len(item) >= 2 :  
            if (item[0:2] not in pinyin_dict.keys()) :  
                pinyin_dict[item[0:2]] = 1   # 统计每个以双字打头拼音出现情况  
            else:  
                pinyin_dict[item[0:2]] += 1  
    # print(pinyin_dict)  
  
    for key, value in pinyin_dict.items():  
        # 查找其中符合要求的字符  
        index_list = []  
        for item in sorted_list:  
            if item[0:2] == key:  
                index_list.append(l_all[i].index(item))  
        print("const uint8_t idx_" + key + "[]", "= {", end="");  
        for k in range(len(index_list)):  
            if k == len(index_list) - 1:  
                print(index_list[k], end="};\n")  
            else:                print(index_list[k], end=",")
```

这样我们就可以直接哈希表快速查找对应的值了, 

内存消耗量计算: 
用于判定是否存在的 26 个 32 bit 数据:  26 * 4 = 104 
用于存储二字母选取情况所定义的11大小的哈希表 23 个 node, 平均长度 10， 单个元素4byte, 需要 920  byte 存储访问需要的哈希表部分;
二字母节点约为 100 个, 每个为 5byte(1byte 长度, 4byte地址), 共500bytes;
存储二字母下标的对应表约为 450 个(450bytes)
因此多使用空间约为 2kb 左右;

我们采用获取精确和模糊匹配结果的函数, 定义接口:
```c
static uint8_t zh_get_match_idx(const char* str, int16_t* match_idx, const uint8_t* vag_num, uint8_t* vag_idx_arr, int8_t* vag_br)
```
其中 str 为传入函数, match_idx 为精确匹配下标(如果没有则返回-1), vag_num 为最大模糊匹配个数, vag_idx_arr 是大小为 vag_num 的数组, 用于存储模糊匹配结果;
而 vag_br 为实际得到的模糊匹配结果数量。

需要说明的是, `zh_get_match_idx` 函数中不进行分词, 即对于 xian, 仅匹配 xian, xiang, 不需要匹配 xia'n 和 xi'an , 而分词我们在专门的分词函数中进行。 

需要说明的是, zh_get_match_idx 返回的结果中 match_idx 的实际顺序可能并不相同, 但是<mark style="background: transparent; color: red">一般带哈希匹配机制的能够返回更加精确的多词搜索结果</mark>。

### (4) 基于单链表和深度优先搜索获取分词方式的分词算法
由于分词方法可能有多种不同的分词方案, 最常用的就是在尾部插入和头部删除(类似队列), 由于 C 语言没有直接提供队列, 可以采用单链表进行模拟。

主要的步骤包含如下: 
```mermaid
flowchart LR
确定可能的分词方式同时记录每一个位置是模糊分词还是精确分词 --> 分词匹配
```

另外, 我们可以不必每一次进行分词和半匹配, 例如这样长度为 6 的字符串必须遍历匹配整个码表 6 次才能得到结果; <mark style="background: transparent; color: red">仍然只需要遍历一次, 但是, 寻找的是源字符串和目标串前半部分的最大公共子串</mark>长度, 而这个只需要遍历一次就可以全部获得了。 

对于分词方法类的存储, 我们只需要采用头尾指针的双链表结构, 其中尾指针是用于哈希表尾插法的; 在<mark style="background: transparent; color: red">使用哈希表时只需要用尾插法直接插入就行了, 而不使用时需要比较和进行排序才能进行插入</mark> 
```c
/* single link list for store match case */
typedef struct match_case_list {
   uint8_t length;
   __split_method_t* head;
   __split_method_t* tail;
}__split_method_list_t;

typedef struct {
    uint8_t match_size[MAX_WORD_SPLIT_NUMBER];       // precise match size  (1-6) 
    int8_t match_loc[MAX_WORD_SPLIT_NUMBER - 1];    // record the location of split sign location
    __split_method_t* next;
}__split_method_t;
```

> [!caution] 完全匹配分词
> 如果分词结果的每个拼音都能和码表中的拼音分词结果对应, 则称为是 "完全匹配分词", 一般而言, 检索到 "完全匹配词" 的分词方式不用超过两种。

在匹配分词过程中, 为了减少总的搜索次数, 应当使用 `strncmp(min(strlen(str), strlen(str2)))`  类似的方式获取是否有对应的准确匹配或者模糊匹配结果; 
使用一个长度为4的数组 `is_precise_match = 0x7` 存储某个分词方法是否在某一段达到了全匹配; 使用 int8_t 类型的 match_loc 存储某个分词的分词标记位置, 例如1后有分词符, 则 `match_loc[0] = 0;` 我们<b><mark style="background: transparent; color: blue">检查某个分词结果是否为完全匹配词时, 只需检查当match_loc >= 0 部分, 如果is_precise_match全是1, 则这个分词方式为完全匹配分词</mark></b>。 

但是, 对于完全匹配分词情况, 是需要插入前端的, 而非完全匹配情况, 只需挂在尾端。例如匹配 xian 时, xian 为完全匹配 1 字; 然后检查 xia'n (下你), 为非完全匹配, 再检测 xi'an 完全匹配, 则需要使用链表插入的方法, 将 xi'an 插入 xia'n 之前。
<mark style="background: transparent; color: red">需要说明的是, 如果是非完全匹配, 直接采用尾插法即可, 如果完全匹配, 则搜索并插在前面。 </mark>

分词顺序问题: 每次创建一个分词方法的链表节点对象, 分词方法仅记录分词位置(使用长度4-1的int8_t数组) 和某个位置是否为准确分词(使用一个8bit数据);

<mark style="background: transparent; color: red">搜索机制</mark>: 每次寻找最大可能分词量 cmp_num, <mark style="background: transparent; color: red">仅挨个比较一次(节省时间)</mark>, 需要注意的是<mark style="background: transparent; color: red">仅搜索最小长度匹配</mark>, 例如 sha 和 shi 最小长度为 3, 因此不匹配, 不作为结果。 另外考虑分词搜索机制: <mark style="background: transparent; color: red">由于我们希望将码表对应部分遍历一遍, 然后找出所有可能的机制, 从中筛选出可能的, 而同一时刻仅记录一个分词位置信息， 因此搜索机制是深度优先搜索机制(dfs)</mark>, 搜不到则返回-1从前面继续搜索。

另外, 如果某个分词分支的首字母为 i, u,v 的情况, 则仅有这个分支不向方法中进行添加匹配，而并不是删除整个匹配链表返回空的(如果第一个字母是这个, 可以直接返回空);
根据上面的机制, 我们分配的 split_num 数组大小也应该是 4(由于最后一个要记录最后一个分词符的位置)

规定: start_idx = 0 时, 表示分隔符在
![[Excalidraw/1. 汉字显示原理与编码简介 2024-08-27 00.21.35|700]]
每次只需先检查 start_idx 和 strlen 的关系, 即可确定一个字符串是否被匹配完毕, 如果成功匹配完毕, 则在 method 中插入该方法, 否则不插入, 直接放弃此搜寻结果。

最后由于例如匹配的长度为1时, 实际搜索的 depth = 1(因为是从0开始搜索的), 因此最后采用 `memcpy(m->spm, spm,sizeof(uint8_t) * depth);` 来将对应长度的部分放入对应的结果中。然后将其余的设置为0即可。

为了能够搜索一遍就能找出所有的匹配情况, <mark style="background: transparent; color: red">对应的匹配机制不是strncmp, 而是最大公共前缀长度</mark>, 当最大公共前缀长度 > 1 时, 则有2字以上的模糊匹配音。只需要使用如下找最大公共前缀长度即可: 
```c
uint8_t common_prefix_length(const char* str1, const char* str2) {
    const char* start1 = str1;
    const char* start2 = str2;
    while (*str1 && *str2 && *str1 == *str2) {
        str1++; str2++;
    }
    return str1 - start1;
}
```

其三, 因为<mark style="background: transparent; color: red">可能有相同匹配的情况</mark>, 例如 zh'hao 的第一个匹配到 zhi, hao 时返回的分词方法是 2,5; 而匹配到 zhe, hao 时返回的也是 2,5, 实际上是一种分词方式, 且均为模糊匹配。 所以, <mark style="background: transparent; color: red">返回分词方式时, 不用多增加情况(仅需在可能的分词方式中搜索)</mark>, 但是<mark style="background: transparent; color: red">最后进行分词匹配时, 对于模糊部分, 要考虑多个匹配方案。</mark>

另外需要说明的是, <mark style="background: transparent; color: red">同一种分词位置方案下</mark>, 也可能出现不同的匹配情况(例如 hua'le' 的分词法是 3, 5, 精确配位为 11, 但是对于 huan'le 匹配仍然是按照前序匹配的(即每一次仅增加sc), 得到的是模糊匹配结果), 即精确位是 10;

一种考虑是给精确配位部分给予更大的权重, 从高到低进行赋值词语，第一个匹配赋予更大的权重7, 第二个匹配赋予4, 但这样仍然导致对于 zhong'guo'quan 等, zhong'guo'quan 被放弃, 而 zhong'guo'qu'an 被保留; 而我们可能是需要 zhong'guo'quan 的, 因此应当作为一种新的分词方案, 只需要添加在后面即可。

> [!caution] 概念:匹配权重
> 我们按照某个部分是否为 "完全匹配分词" 为分词方案分配 "匹配权重" (m_wt), 在结构体中体现为wt这一项。 
> 在插入时, 按照匹配程度进行权重排序, 链表中存在分词相同且权重序相同的，可以直接丢弃, 但是如果权重不同, 则需要进行插入, <mark style="background: transparent; color: red">具体插入位置按匹配权重进行排序</mark>。
> 如果串长为 4, 且权重为 0, 则可以直接考虑使用尾插法。

匹配权重赋值具体体现为:
```c
for (int i = 0; i < codex->table_length; i++) {
	uint8_t sr = strlen(codex->code_table[i]); /* strlen (result_str) */
	uint8_t sc = common_prefix_length(str + start_idx, codex->code_table[i]);
	if (sc <= 1) continue;  /* for */
	if (sc > 1) {
		res = 0;   /* have at least 1 vague match (1 letter occasion needn't to be considered) */ 
		m_wt = sc == sr ? (m_wt|(1 << (MAX_WORD_LENGTH - 1 - depth))) : m_wt &~(1 << (MAX_WORD_LENGTH - 1 - depth));  /* record precise match location */
		spm[depth] = start_idx + sc;     /* the match part */
		if (!pinyin_dfs(m_list, str, spm, m_wt, depth + 1)) res = 0;  /* If any result is found, set res to 0 */
	}
}
```

其中 MAX_WORD_LENGTH 为4, 即精确匹配词越靠前, 优先级越高, 精确匹配词越多, 优先级越高(在链表位置越靠前); (注意不匹配时一定要取消对应部分)

例如这是一个可能的部分(zhongguoren), 显然, 1101 的 5,7，8,11 (zhong'gu'o'ren) 排在 1001 (zhong'gu o'o'ren)的前面, 这也是我们希望的, 而相同权重的，就会在链表插入中合并为一项分词方法;
![[attachments/Pasted image 20240827122630.png]]

但是, 相应地, 显然 zhong'guo'ren 的权重的是 1110,显然是更大的, 排在模糊匹配之前。我们只需要进行截断就可以规避后期的大量模糊匹配了。
另外, 为了 xian 相对于 xi'an 更加优先匹配, 应当遵循串长最小原则。

> [!NOTE] 决定在匹配链表中位置的三个关键因素
> 1. 匹配串长(字符串的长度, 串长越小, 则越靠前)
> 2. 匹配权重(完美匹配特性情况, 权重越大, 则越靠前)
> 3. 匹配方法(下标顺序, 按下标排列越靠前, 则越靠前)
> 如果上述三个都相同, 则直接进行合并。

分词匹配示例如下图所示:
![[attachments/IMG_20240827_192151.jpg|550]]
可以看出, 确实得到的是非常有效的分词结果, 但是, 可以看出相同的分词结果有好多个, 我们不需要这么多。我们需要的是精确匹配的结果(例如对于y xi yi xi有0111的项, 则显然后面三个项是不需要再模糊匹配的);
因此下一步模糊检索过程需要对如上的结果进行精简。<mark style="background: transparent; color: red">对于分词办法完全相同的组, 由于第一个必定是匹配系数最高的, 可以考虑仅取第一个</mark>,但是这是不全面的, 例如对于 zheng'he'xia'da 部分, <mark style="background: transparent; color: red">我们仍然需要考虑最后一个没有匹配完全的情况(但是前面的就不用考虑了, 因为这属于纠正问题)</mark>, 可能的分词情况还有如 zheng'he'xia'dan, zheng'he'xia'dai 等, 但是实测手机输入法这些情况也是不考虑的，因此也不考虑这些问题。

因此, 对于分词完全相同的情况, <mark style="background: transparent; color: red">只需接受第一组即可, 其余组均不用接受</mark>, 

另外还有一个需要修复的是如下情况:
```c
a man da
```

![[attachments/Pasted image 20240827221437.png]]
需要说明的是, 按照上述排序而言，越靠前显然是越有可能搜索的, 但实际结果是却不一定是连着的。因此<mark style="background: transparent; color: red">对于分词情况相同的部分, 我们需要记载其分词情况, 仅取第一个为实际分词</mark>.
![[attachments/Pasted image 20240828110756.png]]
如果记录前面所有的分词情况, 则需要一个至少 3 * 4 大小的数组, 并且每次轮询比较; 因而<mark style="background: transparent; color: red">最好的方法仍然是哈希办法</mark>。显然分词方案有最多4个位置, 位置之间不重复且不超过20; 同时总长度是一定的;但是很难找到哈希函数;
但是, 如果采用双指针进行搜索, 每次从头到尾进行比较, 如果找到相同的就去掉, 则每次比较次数是一定不超过3的(假设使用分词方法链长为4), 这样就能有效减少比较了。

使用 m1 记录上一个此的位置; 同时, <mark style="background: transparent; color: red">由于是按照长度排序的, 先比较长度, 再比较分词方式</mark>; 如果有重复的, 则可以直接释放节点内存, 而不占用更多空间。
```c
    __split_method_t* m1 = m_list->head, *m2 = m1->next;
    uint8_t num = 1;
    while (m2!=NULL && num < ZH_WORD_MAX_MATCH_TYPES) {
        uint8_t rep = 0;
        for (__split_method_t *mt = m_list->head; mt != m2; mt = mt->next) {
            if (mt->length == m2->length && memcmp(mt->spm,m2->spm, mt->length - 1) == 0) { 
                rep = 1;
                break;
            }
        }
        if (rep) {
            __split_method_t* m = m2;
            m2 = m2->next;
            m1->next = m2;
            free(m);
            m_list->num--;
        }
        else {/* not repeat */
            m1 = m2;
            m2 = m2->next;
            num++;
        }
    }
    /* deposite the element after m1 */
    __split_method_t* m = m1->next;
    m1->next = NULL;
    while (m != NULL) {
        __split_method_t* tmp = m;
        m = m->next;
        free(tmp);
        m_list->num--;
    }
```

然后我们即将所有的方案都提取出来了, 只需要联合 CJSON 搜索词库就行了。




